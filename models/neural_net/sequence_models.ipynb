{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35bc8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision as T\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "root_path = os.path.abspath(os.path.join('../..')) # <- adjust such that root_path always points at the root project dir (i.e. if current file is two folders deep, use '../..'). \n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "import database_server.db_utilities as dbu \n",
    "import pickle as pkl\n",
    "from sklearn.decomposition import PCA\n",
    "from Help_functions import preprocess, game_dict, inputs, club_dict, points_and_co, points_and_co_oppon, data_to_lstm, Sport_pred_2LSTM_1, predict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f98e0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(420)\n",
    "random.seed(420)\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e694093",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"\"\"\n",
    "SELECT ms.*, \n",
    "       m.schedule_date, m.schedule_time, m.schedule_round, m.schedule_day,\n",
    "       w.annual_wages_eur AS annual_wage_team, \n",
    "       w.weekly_wages_eur AS weekly_wages_eur,\n",
    "       w.annual_wages_eur/w.n_players AS annual_wage_player_avg\n",
    "FROM matchstats ms \n",
    "LEFT JOIN matches m ON ms.match_id = m.id\n",
    "LEFT JOIN teamwages w ON ms.team_id = w.team_id\n",
    "AND     ms.season_str = w.season_str\n",
    "ORDER BY m.schedule_date DESC, m.schedule_time DESC; \n",
    "\"\"\"\n",
    "\n",
    "df_allinfo = dbu.select_query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc6fbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_test = preprocess(df_allinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3d3a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_df = new_data_test.data_frame.copy()\n",
    "#scale_df[\"stad_capac\"] = 0\n",
    "for team in scale_df.team_id.unique():\n",
    "    max_attend = max(scale_df[(scale_df.team_id == team) & (scale_df.venue == new_data_test.return_dicts(\"venue\")[\"Home\"])].attendance)\n",
    "    scale_df.loc[(scale_df.team_id == team) & (scale_df.venue == new_data_test.return_dicts(\"venue\")[\"Home\"]),\"stad_capac\"] = scale_df[(scale_df.team_id == team) & (scale_df.venue == new_data_test.return_dicts(\"venue\")[\"Home\"])].attendance.apply(lambda x: x/max_attend)\n",
    "    scale_df.loc[(scale_df.opponent_id == team) & (scale_df.venue == new_data_test.return_dicts(\"venue\")[\"Away\"]),\"stad_capac\"] = scale_df[(scale_df.opponent_id == team) & (scale_df.venue == new_data_test.return_dicts(\"venue\")[\"Away\"])].attendance.apply(lambda x: x/max_attend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2839e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_team = OneHotEncoder()\n",
    "to_ohe_team = scale_df.loc[:, [\"team_id\", \"opponent_id\"]]\n",
    "ohe_team.fit(to_ohe_team)\n",
    "\n",
    "codes = ohe_team.transform(to_ohe_team).toarray()\n",
    "feature_names = ohe_team.get_feature_names(['team_id', 'opponent_id'])\n",
    "\n",
    "scale_df = pd.concat([scale_df, \n",
    "               pd.DataFrame(codes, columns = feature_names).astype(int)], axis=1)\n",
    "##########################################\n",
    "ohe_ligue = OneHotEncoder()\n",
    "to_ohe_ligue = scale_df.loc[:,[\"league_id\"]]\n",
    "ohe_ligue.fit(to_ohe_ligue)\n",
    "\n",
    "codes = ohe_ligue.transform(to_ohe_ligue).toarray()\n",
    "feature_names = ohe_ligue.get_feature_names(['league_id'])\n",
    "\n",
    "scale_df = pd.concat([scale_df, \n",
    "               pd.DataFrame(codes, columns = feature_names).astype(int)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b7eaee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs = club_dict(scale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ae76f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = new_data_test.return_dicts(\"result\")\n",
    "clubs = points_and_co(clubs, result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a134b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs = points_and_co_oppon(clubs, result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19c23f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_train, game_valid = game_dict(scale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a63c57fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['result',\n",
       " 'gf',\n",
       " 'ga',\n",
       " 'goal_diff',\n",
       " 'xg',\n",
       " 'xga',\n",
       " 'shooting_standard_gls',\n",
       " 'shooting_standard_sh',\n",
       " 'shooting_standard_sot',\n",
       " 'shooting_standard_sot_perc',\n",
       " 'shooting_standard_g_per_sh',\n",
       " 'shooting_standard_g_per_sot',\n",
       " 'shooting_standard_dist',\n",
       " 'shooting_standard_fk',\n",
       " 'shooting_standard_pk',\n",
       " 'shooting_standard_pkatt',\n",
       " 'shooting_expected_npxg',\n",
       " 'shooting_expected_npxg_per_sh',\n",
       " 'shooting_expected_g_minus_xg',\n",
       " 'shooting_expected_npg_minus_xg',\n",
       " 'keeper_performance_sota',\n",
       " 'keeper_performance_saves',\n",
       " 'keeper_performance_save_perc',\n",
       " 'keeper_performance_cs',\n",
       " 'keeper_performance_psxg',\n",
       " 'keeper_performance_psxg_plus_minus',\n",
       " 'keeper_penaltykicks_pkatt',\n",
       " 'keeper_penaltykicks_pka',\n",
       " 'keeper_penaltykicks_pksv',\n",
       " 'keeper_penaltykicks_pkm',\n",
       " 'keeper_launched_cmp',\n",
       " 'keeper_launched_att',\n",
       " 'keeper_launched_cmp_perc',\n",
       " 'keeper_passes_att',\n",
       " 'keeper_passes_thr',\n",
       " 'keeper_passes_launch_perc',\n",
       " 'keeper_passes_avglen',\n",
       " 'keeper_goalkicks_att',\n",
       " 'keeper_goalkicks_launch_perc',\n",
       " 'keeper_goalkicks_avglen',\n",
       " 'keeper_crosses_opp',\n",
       " 'keeper_crosses_stp',\n",
       " 'keeper_crosses_stp_perc',\n",
       " 'keeper_sweeper_number_opa',\n",
       " 'keeper_sweeper_avgdist',\n",
       " 'passing_total_cmp',\n",
       " 'passing_total_att',\n",
       " 'passing_total_cmp_perc',\n",
       " 'passing_total_totdist',\n",
       " 'passing_total_prgdist',\n",
       " 'passing_short_cmp',\n",
       " 'passing_short_att',\n",
       " 'passing_short_cmp_perc',\n",
       " 'passing_medium_cmp',\n",
       " 'passing_medium_att',\n",
       " 'passing_medium_cmp_perc',\n",
       " 'passing_long_cmp',\n",
       " 'passing_long_att',\n",
       " 'passing_long_cmp_perc',\n",
       " 'passing_attacking_ast',\n",
       " 'passing_attacking_xag',\n",
       " 'passing_attacking_xa',\n",
       " 'passing_attacking_kp',\n",
       " 'passing_attacking_1_per_3',\n",
       " 'passing_attacking_ppa',\n",
       " 'passing_attacking_crspa',\n",
       " 'passing_attacking_prgp',\n",
       " 'passing_types_passtypes_live',\n",
       " 'passing_types_passtypes_dead',\n",
       " 'passing_types_passtypes_fk',\n",
       " 'passing_types_passtypes_tb',\n",
       " 'misc_aerialduels_won_perc',\n",
       " 'attendance',\n",
       " 'points',\n",
       " 'mean_points',\n",
       " 'weekly_wages_eur',\n",
       " 'season_str',\n",
       " 'league_id',\n",
       " 'venue',\n",
       " 'stad_capac',\n",
       " 'team_id_1',\n",
       " 'team_id_2',\n",
       " 'team_id_3',\n",
       " 'team_id_4',\n",
       " 'team_id_5',\n",
       " 'team_id_6',\n",
       " 'team_id_7',\n",
       " 'team_id_8',\n",
       " 'team_id_9',\n",
       " 'team_id_10',\n",
       " 'team_id_11',\n",
       " 'team_id_12',\n",
       " 'team_id_13',\n",
       " 'team_id_14',\n",
       " 'team_id_15',\n",
       " 'team_id_16',\n",
       " 'team_id_17',\n",
       " 'team_id_18',\n",
       " 'team_id_19',\n",
       " 'team_id_20',\n",
       " 'team_id_21',\n",
       " 'team_id_22',\n",
       " 'team_id_23',\n",
       " 'team_id_24',\n",
       " 'team_id_25',\n",
       " 'team_id_26',\n",
       " 'team_id_27',\n",
       " 'team_id_28',\n",
       " 'team_id_29',\n",
       " 'team_id_30',\n",
       " 'team_id_31',\n",
       " 'team_id_32',\n",
       " 'team_id_33',\n",
       " 'team_id_34',\n",
       " 'team_id_35',\n",
       " 'team_id_36',\n",
       " 'team_id_37',\n",
       " 'team_id_38',\n",
       " 'team_id_39',\n",
       " 'team_id_40',\n",
       " 'team_id_41',\n",
       " 'team_id_42',\n",
       " 'team_id_43',\n",
       " 'team_id_44',\n",
       " 'team_id_45',\n",
       " 'team_id_46',\n",
       " 'team_id_47',\n",
       " 'team_id_48',\n",
       " 'team_id_49',\n",
       " 'team_id_50',\n",
       " 'team_id_51',\n",
       " 'team_id_52',\n",
       " 'team_id_53',\n",
       " 'team_id_54',\n",
       " 'team_id_55',\n",
       " 'team_id_56',\n",
       " 'team_id_57',\n",
       " 'team_id_58',\n",
       " 'team_id_59',\n",
       " 'team_id_60',\n",
       " 'team_id_61',\n",
       " 'team_id_62',\n",
       " 'team_id_63',\n",
       " 'team_id_64',\n",
       " 'team_id_65',\n",
       " 'team_id_66',\n",
       " 'team_id_67',\n",
       " 'team_id_68',\n",
       " 'team_id_69',\n",
       " 'team_id_70',\n",
       " 'team_id_71',\n",
       " 'team_id_72',\n",
       " 'team_id_73',\n",
       " 'team_id_74',\n",
       " 'team_id_75',\n",
       " 'team_id_76',\n",
       " 'team_id_77',\n",
       " 'team_id_78',\n",
       " 'team_id_79',\n",
       " 'team_id_80',\n",
       " 'team_id_81',\n",
       " 'team_id_82',\n",
       " 'team_id_83',\n",
       " 'team_id_84',\n",
       " 'team_id_85',\n",
       " 'team_id_86',\n",
       " 'team_id_87',\n",
       " 'team_id_88',\n",
       " 'team_id_89',\n",
       " 'team_id_90',\n",
       " 'team_id_91',\n",
       " 'team_id_92',\n",
       " 'team_id_93',\n",
       " 'team_id_94',\n",
       " 'team_id_95',\n",
       " 'team_id_96',\n",
       " 'team_id_97',\n",
       " 'team_id_98',\n",
       " 'team_id_99',\n",
       " 'team_id_100',\n",
       " 'team_id_101',\n",
       " 'team_id_102',\n",
       " 'team_id_103',\n",
       " 'team_id_104',\n",
       " 'team_id_105',\n",
       " 'team_id_106',\n",
       " 'team_id_107',\n",
       " 'team_id_108',\n",
       " 'team_id_109',\n",
       " 'team_id_110',\n",
       " 'team_id_111',\n",
       " 'team_id_112',\n",
       " 'team_id_113',\n",
       " 'team_id_114',\n",
       " 'team_id_115',\n",
       " 'team_id_116',\n",
       " 'team_id_117',\n",
       " 'team_id_118',\n",
       " 'team_id_119',\n",
       " 'team_id_120',\n",
       " 'team_id_121',\n",
       " 'team_id_122',\n",
       " 'team_id_123',\n",
       " 'team_id_124',\n",
       " 'team_id_125',\n",
       " 'team_id_127',\n",
       " 'team_id_128',\n",
       " 'team_id_129',\n",
       " 'team_id_130',\n",
       " 'team_id_131',\n",
       " 'team_id_133',\n",
       " 'team_id_134',\n",
       " 'team_id_135',\n",
       " 'team_id_136',\n",
       " 'team_id_137',\n",
       " 'team_id_138',\n",
       " 'team_id_139',\n",
       " 'team_id_140',\n",
       " 'team_id_141',\n",
       " 'team_id_142',\n",
       " 'team_id_143',\n",
       " 'opponent_id_1',\n",
       " 'opponent_id_2',\n",
       " 'opponent_id_3',\n",
       " 'opponent_id_4',\n",
       " 'opponent_id_5',\n",
       " 'opponent_id_6',\n",
       " 'opponent_id_7',\n",
       " 'opponent_id_8',\n",
       " 'opponent_id_9',\n",
       " 'opponent_id_10',\n",
       " 'opponent_id_11',\n",
       " 'opponent_id_12',\n",
       " 'opponent_id_13',\n",
       " 'opponent_id_14',\n",
       " 'opponent_id_15',\n",
       " 'opponent_id_16',\n",
       " 'opponent_id_17',\n",
       " 'opponent_id_18',\n",
       " 'opponent_id_19',\n",
       " 'opponent_id_20',\n",
       " 'opponent_id_21',\n",
       " 'opponent_id_22',\n",
       " 'opponent_id_23',\n",
       " 'opponent_id_24',\n",
       " 'opponent_id_25',\n",
       " 'opponent_id_26',\n",
       " 'opponent_id_27',\n",
       " 'opponent_id_28',\n",
       " 'opponent_id_29',\n",
       " 'opponent_id_30',\n",
       " 'opponent_id_31',\n",
       " 'opponent_id_32',\n",
       " 'opponent_id_33',\n",
       " 'opponent_id_34',\n",
       " 'opponent_id_35',\n",
       " 'opponent_id_36',\n",
       " 'opponent_id_37',\n",
       " 'opponent_id_38',\n",
       " 'opponent_id_39',\n",
       " 'opponent_id_40',\n",
       " 'opponent_id_41',\n",
       " 'opponent_id_42',\n",
       " 'opponent_id_43',\n",
       " 'opponent_id_44',\n",
       " 'opponent_id_45',\n",
       " 'opponent_id_46',\n",
       " 'opponent_id_47',\n",
       " 'opponent_id_48',\n",
       " 'opponent_id_49',\n",
       " 'opponent_id_50',\n",
       " 'opponent_id_51',\n",
       " 'opponent_id_52',\n",
       " 'opponent_id_53',\n",
       " 'opponent_id_54',\n",
       " 'opponent_id_55',\n",
       " 'opponent_id_56',\n",
       " 'opponent_id_57',\n",
       " 'opponent_id_58',\n",
       " 'opponent_id_59',\n",
       " 'opponent_id_60',\n",
       " 'opponent_id_61',\n",
       " 'opponent_id_62',\n",
       " 'opponent_id_63',\n",
       " 'opponent_id_64',\n",
       " 'opponent_id_65',\n",
       " 'opponent_id_66',\n",
       " 'opponent_id_67',\n",
       " 'opponent_id_68',\n",
       " 'opponent_id_69',\n",
       " 'opponent_id_70',\n",
       " 'opponent_id_71',\n",
       " 'opponent_id_72',\n",
       " 'opponent_id_73',\n",
       " 'opponent_id_74',\n",
       " 'opponent_id_75',\n",
       " 'opponent_id_76',\n",
       " 'opponent_id_77',\n",
       " 'opponent_id_78',\n",
       " 'opponent_id_79',\n",
       " 'opponent_id_80',\n",
       " 'opponent_id_81',\n",
       " 'opponent_id_82',\n",
       " 'opponent_id_83',\n",
       " 'opponent_id_84',\n",
       " 'opponent_id_85',\n",
       " 'opponent_id_86',\n",
       " 'opponent_id_87',\n",
       " 'opponent_id_88',\n",
       " 'opponent_id_89',\n",
       " 'opponent_id_90',\n",
       " 'opponent_id_91',\n",
       " 'opponent_id_92',\n",
       " 'opponent_id_93',\n",
       " 'opponent_id_94',\n",
       " 'opponent_id_95',\n",
       " 'opponent_id_96',\n",
       " 'opponent_id_97',\n",
       " 'opponent_id_98',\n",
       " 'opponent_id_99',\n",
       " 'opponent_id_100',\n",
       " 'opponent_id_101',\n",
       " 'opponent_id_102',\n",
       " 'opponent_id_103',\n",
       " 'opponent_id_104',\n",
       " 'opponent_id_105',\n",
       " 'opponent_id_106',\n",
       " 'opponent_id_107',\n",
       " 'opponent_id_108',\n",
       " 'opponent_id_109',\n",
       " 'opponent_id_110',\n",
       " 'opponent_id_111',\n",
       " 'opponent_id_112',\n",
       " 'opponent_id_113',\n",
       " 'opponent_id_114',\n",
       " 'opponent_id_115',\n",
       " 'opponent_id_116',\n",
       " 'opponent_id_117',\n",
       " 'opponent_id_118',\n",
       " 'opponent_id_119',\n",
       " 'opponent_id_120',\n",
       " 'opponent_id_121',\n",
       " 'opponent_id_122',\n",
       " 'opponent_id_123',\n",
       " 'opponent_id_124',\n",
       " 'opponent_id_125',\n",
       " 'opponent_id_127',\n",
       " 'opponent_id_128',\n",
       " 'opponent_id_129',\n",
       " 'opponent_id_130',\n",
       " 'opponent_id_131',\n",
       " 'opponent_id_133',\n",
       " 'opponent_id_134',\n",
       " 'opponent_id_135',\n",
       " 'opponent_id_136',\n",
       " 'opponent_id_137',\n",
       " 'opponent_id_138',\n",
       " 'opponent_id_139',\n",
       " 'opponent_id_140',\n",
       " 'opponent_id_141',\n",
       " 'opponent_id_142',\n",
       " 'opponent_id_143',\n",
       " 'league_id_0',\n",
       " 'league_id_1',\n",
       " 'league_id_2',\n",
       " 'league_id_3',\n",
       " 'league_id_4',\n",
       " 'last_results',\n",
       " 'oppon_points',\n",
       " 'oppon_mean_points',\n",
       " 'schedule_round',\n",
       " 'captain',\n",
       " 'formation',\n",
       " 'referee',\n",
       " 'match_id',\n",
       " 'schedule_date',\n",
       " 'schedule_time',\n",
       " 'schedule_day',\n",
       " 'annual_wage_team',\n",
       " 'annual_wage_player_avg']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abcdefg = list(scale_df.columns)\n",
    "abc = abcdefg[:abcdefg.index(\"annual_wage_player_avg\")+1]\n",
    "defg = abcdefg[abcdefg.index(\"annual_wage_player_avg\")+1:]\n",
    "\n",
    "rearange_list = ['result', 'gf', 'ga', 'goal_diff', 'xg', 'xga', 'shooting_standard_gls',\n",
    "       'shooting_standard_sh', 'shooting_standard_sot',\n",
    "       'shooting_standard_sot_perc', 'shooting_standard_g_per_sh',\n",
    "       'shooting_standard_g_per_sot', 'shooting_standard_dist',\n",
    "       'shooting_standard_fk', 'shooting_standard_pk',\n",
    "       'shooting_standard_pkatt', 'shooting_expected_npxg',\n",
    "       'shooting_expected_npxg_per_sh', 'shooting_expected_g_minus_xg',\n",
    "       'shooting_expected_npg_minus_xg', 'keeper_performance_sota',\n",
    "       'keeper_performance_saves', 'keeper_performance_save_perc',\n",
    "       'keeper_performance_cs', 'keeper_performance_psxg',\n",
    "       'keeper_performance_psxg_plus_minus', 'keeper_penaltykicks_pkatt',\n",
    "       'keeper_penaltykicks_pka', 'keeper_penaltykicks_pksv',\n",
    "       'keeper_penaltykicks_pkm', 'keeper_launched_cmp', 'keeper_launched_att',\n",
    "       'keeper_launched_cmp_perc', 'keeper_passes_att', 'keeper_passes_thr',\n",
    "       'keeper_passes_launch_perc', 'keeper_passes_avglen',\n",
    "       'keeper_goalkicks_att', 'keeper_goalkicks_launch_perc',\n",
    "       'keeper_goalkicks_avglen', 'keeper_crosses_opp', 'keeper_crosses_stp',\n",
    "       'keeper_crosses_stp_perc', 'keeper_sweeper_number_opa',\n",
    "       'keeper_sweeper_avgdist', 'passing_total_cmp', 'passing_total_att',\n",
    "       'passing_total_cmp_perc', 'passing_total_totdist',\n",
    "       'passing_total_prgdist', 'passing_short_cmp', 'passing_short_att',\n",
    "       'passing_short_cmp_perc', 'passing_medium_cmp', 'passing_medium_att',\n",
    "       'passing_medium_cmp_perc', 'passing_long_cmp', 'passing_long_att',\n",
    "       'passing_long_cmp_perc', 'passing_attacking_ast',\n",
    "       'passing_attacking_xag', 'passing_attacking_xa', 'passing_attacking_kp',\n",
    "       'passing_attacking_1_per_3', 'passing_attacking_ppa',\n",
    "       'passing_attacking_crspa', 'passing_attacking_prgp',\n",
    "       'passing_types_passtypes_live', 'passing_types_passtypes_dead',\n",
    "       'passing_types_passtypes_fk', 'passing_types_passtypes_tb',\n",
    "       'misc_aerialduels_won_perc','attendance', 'points', 'mean_points',\n",
    "       'weekly_wages_eur', 'season_str',  'league_id', 'venue', 'team_id',\n",
    "       'opponent_id', 'last_results', 'oppon_points', 'oppon_mean_points', 'schedule_round',\n",
    "        'captain', 'formation', 'referee',  'match_id', 'schedule_date', 'schedule_time',\n",
    "        'schedule_day', 'annual_wage_team', 'annual_wage_player_avg',]\n",
    "\n",
    "rearange_list = list(itertools.chain.from_iterable(defg if item == \"team_id\" else [item] for item in rearange_list))\n",
    "\n",
    "del rearange_list[rearange_list.index(\"opponent_id\")]\n",
    "rearange_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78713aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "train_inputs = inputs(game_train, clubs, rearange_list, scale_df)\n",
    "valid_inputs = inputs(game_valid, clubs, rearange_list, scale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd5c2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_loader = data_to_lstm(train_inputs)\n",
    "valid_for_loader = data_to_lstm(valid_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fab8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_for_loader, batch_size = 32, drop_last = True)\n",
    "test_loader = torch.utils.data.DataLoader(valid_for_loader, batch_size = 32, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "315ccecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss accuracy training: 36.95652173913044%\n",
      "Epoch: 0\n",
      "Loss 40.608262330293655 accuracy 42.49320652173913%\n",
      "Epoch: 1\n",
      "Loss 40.53224788606167 accuracy 42.08559782608695%\n",
      "Epoch: 2\n",
      "Loss 40.07095202803612 accuracy 42.56114130434783%\n",
      "Epoch: 3\n",
      "Loss 40.33928169310093 accuracy 42.59510869565217%\n",
      "Epoch: 4\n",
      "Loss 40.06759561598301 accuracy 42.255434782608695%\n",
      "Epoch: 5\n",
      "Loss 40.065742045640945 accuracy 42.79891304347826%\n",
      "Epoch: 6\n",
      "Loss 40.19216784834862 accuracy 42.900815217391305%\n",
      "Epoch: 7\n",
      "Loss 39.845893770456314 accuracy 42.79891304347826%\n",
      "Epoch: 8\n",
      "Loss 39.964572086930275 accuracy 42.66304347826087%\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     pred \u001b[38;5;241m=\u001b[39m net(input2)\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred\u001b[38;5;241m.\u001b[39mfloat(), result2\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     46\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "len_input = train_inputs[0][0].shape[1]\n",
    "lr = 2e-5\n",
    "#wdc = [0.2, 0.15, 0.1, 0.05, 0]\n",
    "#lrs = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "#for lr in lrs:\n",
    "#for wd in wdc:\n",
    "net = Sport_pred_2LSTM_1(len_input, len_input, 3, 2)\n",
    "#print(f\"\\n\\nLR: {lr}, WD: {wd}\")\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)#, weight_decay = 0.01)\n",
    "\n",
    "net.eval()\n",
    "accur = 0\n",
    "for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "    pred = net(input1)\n",
    "    pred = torch.argmax(pred[:,-1,:], dim = 1)\n",
    "    result = result1[:,-1,:]\n",
    "    result = torch.argmax(result, dim = 1)\n",
    "    accur += pred.eq(result).sum().item()\n",
    "\n",
    "    pred = net(input2)\n",
    "    pred = torch.argmax(pred[:,-1,:], dim = 1)\n",
    "    result2 = result2[:,-1,:]\n",
    "    result2 = torch.argmax(result2, dim = 1)\n",
    "    accur += pred.eq(result2).sum().item()\n",
    "print(f\"Loss accuracy training: {100 * accur /((step + 1) * 64)}%\")\n",
    "\n",
    "for epoch in range(15):\n",
    "    losses_val = []\n",
    "    accuracies = []\n",
    "\n",
    "    net.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        pred = net(input1)\n",
    "        loss = criterion(pred.float(), result1.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        net.zero_grad()\n",
    "        pred = net(input2)\n",
    "        loss = criterion(pred.float(), result2.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    accur = 0\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "        pred = net(input1)\n",
    "        result1 = result1[:,-1,:]\n",
    "        pred = pred[:,-1,:]\n",
    "        loss += criterion(pred, result1).item()\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        result1 = torch.argmax(result1, dim = 1)\n",
    "        accur += pred.eq(result1).sum().item()\n",
    "\n",
    "        pred = net(input2)\n",
    "        result2 = result2[:,-1,:]\n",
    "        pred = pred[:,-1,:]\n",
    "        loss += criterion(pred, result2).item()\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        result2  = torch.argmax(result2, dim = 1)\n",
    "        accur += pred.eq(result2).sum().item()\n",
    "    losses_val.append(loss)\n",
    "    accuracy = 100 * (accur /((step + 1) * 64))\n",
    "    os.makedirs(os.path.dirname(f\"./models/sequence_model/LSTM/{lr}/accur_{round(accuracy, 2)}\"), exist_ok = True)\n",
    "    torch.save(net.state_dict(), f\"./models/sequence_model/LSTM/{lr}/accur_{round(accuracy, 2)}\")\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Loss {loss} accuracy {accuracy}%\") \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_input = train_inputs[0][0].shape[1]\n",
    "lr = 2e-5\n",
    "#wdc = [0.2, 0.15, 0.1, 0.05, 0]\n",
    "#lrs = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "#for lr in lrs:\n",
    "#for wd in wdc:\n",
    "net = Sport_pred_2LSTM_1(len_input, len_input, 3, 2)\n",
    "#print(f\"\\n\\nLR: {lr}, WD: {wd}\")\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = lr)#, weight_decay = 0.01)\n",
    "\n",
    "net.eval()\n",
    "accur = 0\n",
    "for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "    pred = net(input1)\n",
    "    result = result1[:,-1,:]\n",
    "    result = torch.argmax(result, dim = 1)\n",
    "    pred2 = net(input2)\n",
    "    predicted = predict(pred1, pred2, result_dict)\n",
    "    accur += predicted.eq(result).sum().item()\n",
    "print(f\"Loss accuracy training: {100 * accur /((step + 1) * 32)}%\")\n",
    "\n",
    "for epoch in range(3):\n",
    "    losses_val = []\n",
    "    accuracies = []\n",
    "\n",
    "    net.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        pred = net(input1)\n",
    "        loss = criterion(pred.float(), result1.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        net.zero_grad()\n",
    "        pred = net(input2)\n",
    "        loss = criterion(pred.float(), result2.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    accur = 0\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "        pred1 = net(input1)\n",
    "        pred2 = net(input2)\n",
    "        result  = torch.argmax(result1, dim = 1)\n",
    "        predicted = torch.nn.functional.one_hot(predict(pred1, pred2, result_dict), num_classes = 3)\n",
    "        accur += predicted.eq(result).sum().item()\n",
    "    losses_val.append(loss)\n",
    "    accuracy = 100 * (accur /((step + 1) * 32))\n",
    "    os.makedirs(os.path.dirname(f\"./models/sequence_model/LSTM/{lr}/accur_{round(accuracy, 2)}\"), exist_ok = True)\n",
    "    torch.save(net.state_dict(), f\"./models/sequence_model/LSTM/{lr}/accur_{round(accuracy, 2)}\")\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Loss {loss} accuracy {accuracy}%\") \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb83bedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Home': 0, 'Away': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_test.return_dicts(\"venue\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2dde1633",
   "metadata": {},
   "source": [
    "# # # # # # # # # # # # # # # # # # # # ## # # # ## # # # # # # # ## # # # ## # #. ## # # # # # ## # # # # #  ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b95431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b7c15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/paul/Documents/Uni/SoSe23/dsp2/DS-Project/data/scraped/cleaned/match/data_match_clean.csv\", delimiter = \";\", index_col = 1)\n",
    "data = data.drop([\"Unnamed: 0\"], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess():\n",
    "    \n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "        \n",
    "        self.data_frame = self.data_frame.loc[:,~self.data_frame.columns.duplicated()].copy()\n",
    "        \n",
    "        self.venue_dict = {}\n",
    "        i = 0\n",
    "        for where in self.data_frame.venue.unique():\n",
    "            self.venue_dict[where] = i\n",
    "            i += 1\n",
    "        \n",
    "        self.day_dict = {}\n",
    "        i = 0\n",
    "        for day in self.data_frame.schedule_day.unique():\n",
    "            self.day_dict[day] = i\n",
    "            i += 1\n",
    "        \n",
    "        self.result = {}\n",
    "        i = 0\n",
    "        for res in self.data_frame.result.unique():\n",
    "            self.result[res] = i\n",
    "            i += 1\n",
    "        #print(self.result)\n",
    "        \n",
    "        self.capt = {}\n",
    "        i = 0\n",
    "        for cap in set(list(self.data_frame.captain) + list(self.data_frame.captain)): #change\n",
    "            self.capt[cap] = i\n",
    "            i += 1\n",
    "            \n",
    "        self.formation = {}\n",
    "        i = 0\n",
    "        for form in self.data_frame.formation.unique(): #changed\n",
    "            self.formation[form] = i\n",
    "            i += 1\n",
    "            \n",
    "        self.referee = {}\n",
    "        i = 0\n",
    "        for ref in self.data_frame.referee.unique():\n",
    "            self.referee[ref] = i\n",
    "            i += 1\n",
    "        \n",
    "#        self.teams = {}\n",
    " #       i = 0\n",
    "  #      for team in self.data_frame.team_id.unique():#set(list(self.data_frame.team_id) + list(self.data_frame.team_id)):\n",
    "   #         self.teams[team] = i\n",
    "    #        i += 1\n",
    "        #print(self.teams)\n",
    "        \n",
    "        self.season = {}\n",
    "        i = 0\n",
    "        for seas in self.data_frame.season_str.unique():\n",
    "            self.season[seas] = i\n",
    "            i += 1\n",
    "        \n",
    "        self.time = {}\n",
    "        i = 0\n",
    "        for tim in self.data_frame.schedule_time.unique():\n",
    "            self.time[tim] = i\n",
    "            i += 1\n",
    "            \n",
    "        self.ligue = {}\n",
    "        i = 0\n",
    "        for lig in self.data_frame.league_id.unique():\n",
    "            self.ligue[lig] = i\n",
    "            i += 1\n",
    "            \n",
    "#        self.match = {}\n",
    " #       i = 0\n",
    "  #      for mat in self.data_frame.match_id.unique():\n",
    "   #         self.match[mat] = i\n",
    "    #        i += 1\n",
    "        \n",
    "        i = 0\n",
    "        self.matchweek = {}\n",
    "        self.data_frame.drop(self.data_frame[self.data_frame.schedule_round == \"Relegation tie-breaker\"].index, inplace = True)\n",
    "        self.data_frame.reset_index(drop = True, inplace = True)\n",
    "        for week in self.data_frame.schedule_round.unique():\n",
    "            self.matchweek[week] = int(week.split()[1])\n",
    "            i += 1\n",
    "        \n",
    "        #self.data_frame.drop(columns = \"fbref_id\", inplace = True)\n",
    "        \n",
    "        \n",
    "        self.data_frame.schedule_day.replace(self.day_dict, inplace = True)\n",
    "        self.data_frame.result.replace(self.result, inplace = True) #change\n",
    "        #self.data_frame.result_away.replace(self.result, inplace = True)\n",
    "        self.data_frame.captain.replace(self.capt, inplace = True)\n",
    "        #self.data_frame.captain_away.replace(self.capt, inplace = True)\n",
    "        self.data_frame.formation.replace(self.formation, inplace = True)\n",
    "        #self.data_frame.formation_away.replace(self.formation, inplace = True)\n",
    "        self.data_frame.referee.replace(self.referee, inplace = True)\n",
    "        #self.data_frame.team_id.replace(self.teams, inplace = True)\n",
    "        #self.data_frame.team_id.replace(self.teams, inplace = True)\n",
    "        self.data_frame.season_str.replace(self.season, inplace = True)\n",
    "        self.data_frame.schedule_time.replace(self.time, inplace = True)\n",
    "        self.data_frame.league_id.replace(self.ligue, inplace = True)\n",
    "        #self.data_frame.match_id.replace(self.match, inplace = True)\n",
    "        self.data_frame.schedule_round.replace(self.matchweek, inplace = True)\n",
    "        self.data_frame.venue.replace(self.venue_dict, inplace = True)\n",
    "        \n",
    "        self.features = ['shooting_standard_gls', 'shooting_standard_sh', 'shooting_standard_sot', \n",
    "                         'shooting_standard_sot_perc', 'shooting_standard_g_per_sh', 'shooting_standard_g_per_sot',\n",
    "                         'shooting_standard_dist', 'shooting_standard_fk', 'shooting_standard_pk', \n",
    "                         'shooting_standard_pkatt', 'shooting_expected_npxg', 'shooting_expected_npxg_per_sh',\n",
    "                         'shooting_expected_g_minus_xg', 'shooting_expected_npg_minus_xg', 'keeper_performance_sota',\n",
    "                         'keeper_performance_saves', 'keeper_performance_save_perc', 'keeper_performance_cs',\n",
    "                         'keeper_performance_psxg', 'keeper_performance_psxg_plus_minus', 'keeper_penaltykicks_pkatt', \n",
    "                         'keeper_penaltykicks_pksv', 'keeper_penaltykicks_pkm', 'keeper_launched_cmp', 'keeper_launched_att',\n",
    "                         'keeper_launched_cmp_perc', 'keeper_passes_att', 'keeper_passes_thr', 'keeper_passes_launch_perc',\n",
    "                         'keeper_passes_avglen', 'keeper_goalkicks_att', 'keeper_goalkicks_launch_perc', \n",
    "                         'keeper_goalkicks_avglen', 'keeper_crosses_opp', 'keeper_crosses_stp', 'keeper_crosses_stp_perc',\n",
    "                         'keeper_sweeper_number_opa', 'keeper_sweeper_avgdist', 'passing_total_cmp', 'passing_total_att',\n",
    "                         'passing_total_cmp_perc', 'passing_total_totdist', 'passing_total_prgdist', 'passing_short_cmp',\n",
    "                         'passing_short_att', 'passing_short_cmp_perc', 'passing_medium_cmp', 'passing_medium_att',\n",
    "                         'passing_medium_cmp_perc', 'passing_long_cmp', 'passing_long_att', 'passing_long_cmp_perc',\n",
    "                         'passing_attacking_ast', 'passing_attacking_xag', 'passing_attacking_xa', 'passing_attacking_kp',\n",
    "                         'passing_attacking_1_per_3', 'passing_attacking_ppa', 'passing_attacking_crspa', 'passing_attacking_prgp',\n",
    "                         'passing_types_passtypes_live', 'passing_types_passtypes_dead', 'passing_types_passtypes_fk',\n",
    "                         'passing_types_passtypes_tb', 'passing_types_passtypes_sw', 'passing_types_passtypes_crs',\n",
    "                         'passing_types_passtypes_ti', 'passing_types_passtypes_ck', 'passing_types_cornerkicks_in',\n",
    "                         'passing_types_cornerkicks_out', 'passing_types_cornerkicks_str', 'passing_types_outcomes_off',\n",
    "                         'passing_types_outcomes_blocks', 'gca_scatypes_sca', 'gca_scatypes_passlive', 'gca_scatypes_passdead',\n",
    "                         'gca_scatypes_to', 'gca_scatypes_sh', 'gca_scatypes_fld', 'gca_scatypes_def', 'gca_gcatypes_gca', \n",
    "                         'gca_gcatypes_passlive', 'gca_gcatypes_passdead', 'gca_gcatypes_to', 'gca_gcatypes_sh',\n",
    "                         'gca_gcatypes_fld',  'gca_gcatypes_def', 'defense_tackles_tkl', 'defense_tackles_tklw',\n",
    "                         'defense_tackles_def3rd', 'defense_tackles_mid3rd', 'defense_tackles_att3rd', 'defense_challenges_tkl',\n",
    "                         'defense_challenges_att', 'defense_challenges_tkl_perc', 'defense_challenges_lost', 'defense_blocks_blocks',\n",
    "                         'defense_blocks_sh', 'defense_blocks_pass', 'defense_general_int', 'defense_general_tkl_plus_int',\n",
    "                         'defense_general_clr', 'defense_general_err', 'possession_general_poss', 'possession_touches_touches',\n",
    "                         'possession_touches_defpen', 'possession_touches_def3rd', 'possession_touches_mid3rd',\n",
    "                         'possession_touches_att3rd', 'possession_touches_attpen', 'possession_touches_live', 'possession_takeons_att',\n",
    "                         'possession_takeons_succ', 'possession_takeons_succ_perc', 'possession_takeons_tkld', 'possession_takeons_tkld_perc',\n",
    "                         'possession_carries_carries', 'possession_carries_totdist', 'possession_carries_prgdist',\n",
    "                         'possession_carries_prgc', 'possession_carries_1_per_3', 'possession_carries_cpa', 'possession_carries_mis',\n",
    "                         'possession_carries_dis', 'possession_receiving_rec', 'possession_receiving_prgr',\n",
    "                         'misc_performance_crdy', 'misc_performance_crdr', 'misc_performance_2crdy', 'misc_performance_fls',\n",
    "                         'misc_performance_fld', 'misc_performance_off', 'misc_performance_og', 'misc_performance_recov',\n",
    "                         'misc_aerialduels_won', 'misc_aerialduels_lost', 'misc_aerialduels_won_perc', 'keeper_penaltykicks_pka',\n",
    "                         'attendance']\n",
    "        \n",
    "        \n",
    "        #self.data_frame = data_frame\n",
    "    \n",
    "    def data_frame(self):\n",
    "        return self.data_frame\n",
    "        \n",
    "    def dataset_team(self, team_id):\n",
    "        # split dataset \n",
    "        random_team1 = self.data_frame[self.data_frame.fbref_home_id == team_id].loc[:, self.data_frame.columns.str.endswith('home')]\n",
    "        random_team2 = self.data_frame[self.data_frame.fbref_away_id == team_id].loc[:, self.data_frame.columns.str.endswith('away')]\n",
    "        random_team3 = self.data_frame[(self.data_frame.fbref_home_id == team_id) |\n",
    "                        (self.data_frame.fbref_away_id == team_id)].loc[:,['schedule_time', 'attendance',\n",
    "                                                                           'referee', 'fbref_season',\n",
    "                                                                           'fbref_league_id', 'fbref_home_id',\n",
    "                                                                           'fbref_away_id', 'fbref_match_id']]\n",
    "\n",
    "        #random_team2.rename({\"gf_away\": \"gf_home\", \"gf_home\": \"gf_away\"}, axis = 1, inplace = True)\n",
    "        # preprocess chunks\n",
    "        self.concat_preprocess(random_team1, \"home\")\n",
    "        self.concat_preprocess(random_team2, \"away\")\n",
    "        random_team = pd.concat([random_team1, random_team2])\n",
    "\n",
    "        # dummy to account for home and away\n",
    "        random_team3[\"home\"] = np.where(random_team3.fbref_home_id == team_id, 1, 0)\n",
    "\n",
    "        # rename columns to change them\n",
    "        random_team3_away = random_team3.loc[random_team3.fbref_away_id == team_id].copy()\n",
    "        random_team3_away.rename({\"fbref_home_id\": \"fbref_away_id\",\n",
    "                                  \"fbref_away_id\": \"fbref_home_id\"}, axis = 1, inplace = True)\n",
    "        random_team3_home = random_team3[random_team3.fbref_home_id == team_id]\n",
    "\n",
    "        # concat after changing column names and rename \n",
    "        random_team3 = pd.concat([random_team3_home, random_team3_away])\n",
    "        random_team3.rename({\"fbref_home_id\": \"fbref_own\", \n",
    "                             \"fbref_away_id\": \"fbref_oppon\",\n",
    "                             \"gf_home\": \"gf_own\", \n",
    "                             \"gf_away\": \"gf_own\"}, axis = 1, inplace = True)\n",
    "\n",
    "        self.end_team = random_team.merge(random_team3, \"left\", on = [\"schedule_date\"], sort = True).copy()\n",
    "\n",
    "        return self.end_team\n",
    "\n",
    "    def concat_preprocess(self, df, where):\n",
    "        colname_dict = {}\n",
    "        result_dict = {self.result[\"W\"]: \"L\", self.result[\"D\"]: \"D\", self.result[\"L\"]: \"W\"}\n",
    "        for colname in self.data_frame.loc[:, self.data_frame.columns.str.endswith(where)]:\n",
    "            colname_dict[colname]  = colname[:-5]\n",
    "    \n",
    "        return df.rename(colname_dict, axis = 1, inplace = True)\n",
    "    \n",
    "    def return_dicts(self, dict_name):\n",
    "        dicts = {\"day\": self.day_dict,\n",
    "                 \"result\": self.result,\n",
    "                 \"captain\": self.capt,\n",
    "                 \"formation\": self.formation,\n",
    "                 \"referee\": self.referee,\n",
    "                 \"teams\": self.teams,\n",
    "                 \"season\": self.season}\n",
    "        return dicts[dict_name]\n",
    "        if dict_name == \"day\":\n",
    "            return self.day_dict\n",
    "        elif dict_name == \"result\":\n",
    "            return self.result\n",
    "        elif dict_name == \"captain\":\n",
    "            return self.capt\n",
    "        elif dict_name == \"formation\":\n",
    "            return self.formation\n",
    "        elif dict_name == \"referee\":\n",
    "            return self.referee\n",
    "        elif dict_name == \"teams\":\n",
    "            return self.teams\n",
    "        elif dict_name == \"season\":\n",
    "            return self.season\n",
    "        \n",
    "    def data_for_team1(self, date, team_1, window_size = 10, discount = 0.5):\n",
    "        \n",
    "        object = StandardScaler()\n",
    "        dataset1 = self.dataset_team(team_1)\n",
    "        cols_for_movavg = dataset1.columns.isin(self.features)\n",
    "        dataset1.loc[:,cols_for_movavg] = self.moving_average(dataset1.loc[:,cols_for_movavg], window_size, discount)\n",
    "        dataset1 = dataset1.add_suffix('_1')\n",
    "        dataset1 = dataset1.loc[date]\n",
    "        \n",
    "        dataset2 = self.dataset_team(dataset1.fbref_oppon_1)\n",
    "        dataset2.loc[:,cols_for_movavg] = self.moving_average(dataset2.loc[:,cols_for_movavg], window_size, discount)\n",
    "        dataset2 = dataset2.add_suffix('_2')\n",
    "        dataset2 = dataset2.loc[date]\n",
    "        \n",
    "        cols_to_drop = ['gf_1', 'xg_1', 'schedule_time_1', 'referee_1', 'fbref_season_1', 'fbref_league_id_1', \n",
    "                'fbref_own_1', 'fbref_oppon_1', 'fbref_match_id_1','gf_2', 'xg_2', \n",
    "                'fbref_own_2', 'fbref_oppon_2', 'fbref_match_id_2']\n",
    "        \n",
    "        to_return = pd.concat([dataset1, dataset2])#.loc[date]\n",
    "        to_return = to_return[~to_return.index.isin(cols_to_drop)]\n",
    "        \n",
    "        X = to_return[~to_return.index.isin([\"result_1\", \"result_2\"])]\n",
    "        y = to_return[to_return.index.isin([\"result_1\"])]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def moving_average(self, df, window_size = 10, discount = 0.5, date = None):\n",
    "        \"compute the moving average with recency bias\"\n",
    "        weights = np.array([discount**x for x in reversed(range(window_size))]) # discount y_t-windowsize with discount**windowsize\n",
    "        weights = np.append(weights , 0)\n",
    "        sum_weights = np.sum(weights)\n",
    "        mov_avg = df.rolling(window = window_size + 1).apply(lambda x: np.sum(weights*x) / sum_weights, raw=False) # compute moving average of window_size \n",
    "        return mov_avg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"\"\"\n",
    "SELECT ms.*, \n",
    "       m.schedule_date, m.schedule_time, m.schedule_round, m.schedule_day,\n",
    "       w.annual_wages_eur AS annual_wage_team, \n",
    "       w.weekly_wages_eur AS weekly_wages_eur,\n",
    "       w.annual_wages_eur/w.n_players AS annual_wage_player_avg\n",
    "FROM matchstats ms \n",
    "LEFT JOIN matches m ON ms.match_id = m.id\n",
    "LEFT JOIN teamwages w ON ms.team_id = w.team_id\n",
    "AND     ms.season_str = w.season_str\n",
    "ORDER BY m.schedule_date DESC, m.schedule_time DESC; \n",
    "\"\"\"\n",
    "\n",
    "df_allinfo = dbu.select_query(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fc3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_test = preprocess(df_allinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b07c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale_df = new_data_test.data_frame.copy()\n",
    "\n",
    "liste = ['schedule_time', 'schedule_round', 'schedule_day', 'result', 'gf', 'ga', 'xg', 'xga', 'formation', \n",
    "         'referee', 'season_str', 'league_id', 'team_id', 'opponent_id', 'match_id', 'id', 'fbref_id', \n",
    "         'home_team_id', 'away_team_id', 'schedule_date', 'venue', 'captain',]\n",
    "\n",
    "cols_to_scale = list(set(list(scale_df.columns)).difference(liste))\n",
    "object = StandardScaler()\n",
    "\n",
    "scale_df.loc[:,cols_to_scale] = object.fit_transform(scale_df.loc[:,cols_to_scale])\n",
    "scale_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b37e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_dict(scale_df):\n",
    "    games_train = {}\n",
    "    for seas in scale_df.season_str.unique()[:-1]:\n",
    "        for week in sorted(scale_df.schedule_round.unique()):\n",
    "            if week not in games_train.keys():\n",
    "                games_train[week] = []\n",
    "            df = scale_df[(scale_df.season_str == seas) & (scale_df.schedule_round == week)]\n",
    "            df1 = df[df.venue == 0]\n",
    "            df2 = df[df.venue == 1]\n",
    "            df = df1.merge(df2, on = \"match_id\")\n",
    "            print(df.columns)\n",
    "            break\n",
    "            for game in df.match_id:\n",
    "                games_train[week].append(game)\n",
    "        break\n",
    "    \n",
    "    games_test = {}\n",
    "    seas = scale_df.season_str.unique()[-1]\n",
    "    for week in sorted(scale_df.schedule_round.unique()):\n",
    "        if week not in games_test.keys():\n",
    "            games_test[week] = []\n",
    "        df = scale_df[(scale_df.season_str == seas) & (scale_df.schedule_round == week)]\n",
    "        df1 = df[df.venue == 0]\n",
    "        df2 = df[df.venue == 1]\n",
    "        df = df1.merge(df2, on = \"match_id\")\n",
    "        for game in df.match_id:\n",
    "            games_test[week].append(game)\n",
    "                \n",
    "    return games_train, games_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06715cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_train, game_valid = game_dict(scale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed6371",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scale_df.loc[:,\"shooting_standard_gls\":'misc_aerialduels_won_perc'].fillna(0)\n",
    "\n",
    "pca = PCA().fit(x)\n",
    "a = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "i = np.where(np.isclose(a, 0.95, atol=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c58399",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_matchstat = PCA(n_components = 0.95)\n",
    "pcs_matchstat = pca_matchstat.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908a38c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "principal_ms_df = pd.DataFrame(data = pcs_matchstat, columns = [f\"feature_{p}\" for p in range(int(i[0]))])\n",
    "principal_ms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_overwrite = list(scale_df.loc[:,\"shooting_standard_gls\":'misc_aerialduels_won_perc'].columns)\n",
    "scale_df_pca = scale_df\n",
    "\n",
    "scale_df_pca.drop(labels = columns_to_overwrite, axis = \"columns\", inplace = True)\n",
    "scale_df_pca = original_df[columns_to_overwrite] = other_data_frame[columns_to_overwrite]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, save_name):\n",
    "        \"\"\"\n",
    "        Saves obj to a pickle file in directory path.\n",
    "        \"\"\" \n",
    "        with open(os.path.join(f\"{save_name}.pkl\"), 'wb') as f:\n",
    "            pkl.dump(obj, f)\n",
    "\n",
    "def load_data_prep_object(obj_name):\n",
    "    \"\"\"\n",
    "    Loads and returns the specified data prep object from the data_prep_objects_path.\n",
    "    \"\"\"\n",
    "        # check if file exists\n",
    "    if not os.path.isfile(os.path.join(f\"{obj_name}.pkl\")):\n",
    "        raise ValueError(f\"Data prep object file '{obj_name}.pkl' does not exist.\")\n",
    "    else:\n",
    "        with open(os.path.join(f\"{obj_name}.pkl\"), 'rb') as f:\n",
    "            obj = pkl.load(f)\n",
    "            return obj\n",
    "            \n",
    "def do_pca(df, perc_var, col_name1 = \"shooting_standard_gls\", col_name2 = 'misc_aerialduels_won_perc'):\n",
    "    try:\n",
    "        pcs_matchstat = load_data_prep_object(\"./pca_matchstats\")\n",
    "    except:\n",
    "        x = df.loc[:,col_name1:col_name2].fillna(0)\n",
    "        pca_matchstat = PCA(n_components = perc_var)\n",
    "        pcs_matchstat = pca_matchstat.fit_transform(x)\n",
    "        save_object(pcs_matchstat, \"pca_matchstats\")\n",
    "    principal_ms_df = pd.DataFrame(data = pcs_matchstat, columns = [f\"feature_{p}\" for p in range(pcs_matchstat.shape[1])])\n",
    "    num_pcs = principal_ms_df.shape[1]\n",
    "    print(num_pcs)\n",
    "    columns_to_overwrite = list(df.loc[:, col_name1:col_name2].columns)\n",
    "    df = df.drop(labels = columns_to_overwrite, axis = \"columns\")\n",
    "    new_cols = columns_to_overwrite[:num_pcs-1] + columns_to_overwrite[-1:]\n",
    "    print(len(new_cols))\n",
    "    df.loc[:, new_cols] = principal_ms_df.values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114acac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_df_pca = do_pca(scale_df, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def club_dict(scale_df):\n",
    "    clubs = {}\n",
    "    for club in list(scale_df.team_id.unique()):\n",
    "        club_df = scale_df[scale_df.team_id == club]\n",
    "        club_df.sort_values(by = ['season_str', 'schedule_round'], inplace = True)\n",
    "        club_df.reset_index(drop = True, inplace = True)\n",
    "        clubs[club] = club_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_and_co(clubs, resutlt_dict):\n",
    "    result_dict = new_data_test.return_dicts(\"result\")\n",
    "    point_dict = {result_dict['W']: 3, result_dict['L']: 0, result_dict['D']: 1}\n",
    "    for club in clubs:\n",
    "        #print(clubs[club])\n",
    "        new_column_points = []\n",
    "        new_column_meanpoints = []\n",
    "        last_year_game_res = {}\n",
    "        last_year_game = []\n",
    "        goal_diff_seasons = []\n",
    "        z = 0\n",
    "        for season in clubs[club].groupby(\"season_str\"):\n",
    "            season = season[1]\n",
    "            new_col_seas_p = [0] * len(season)\n",
    "            new_col_seas_mp = [0] * len(season)\n",
    "            new_col_last_year_game = [0] * len(season)\n",
    "            index = 0\n",
    "            #print(season)\n",
    "            last_year_game_res[z] = {}\n",
    "            for _, matchday in season.iterrows():\n",
    "                goal_diff_seasons.append(int(matchday.gf) - int(matchday.ga))\n",
    "                if index > 0:\n",
    "                    new_col_seas_p[index] = new_col_seas_p[index - 1] + point_dict[matchday.result]\n",
    "                    new_col_seas_mp[index] = (new_col_seas_mp[index - 1] + point_dict[matchday.result])/2\n",
    "                else:\n",
    "                    new_col_seas_p[index] = point_dict[matchday.result]\n",
    "                    new_col_seas_mp[index] = point_dict[matchday.result]\n",
    "\n",
    "                if matchday.opponent_id not in last_year_game_res[z].keys():\n",
    "                    #print(\"club: \", club, \"\\n z: \", z, \"\\n oppom: \", matchday.opponent_id)\n",
    "                    last_year_game_res[z][matchday.opponent_id] = []\n",
    "                    last_year_game_res[z][matchday.opponent_id].append(matchday.result)\n",
    "                else:\n",
    "                    last_year_game_res[z][matchday.opponent_id].append(matchday.result)\n",
    "\n",
    "                if z > 0:\n",
    "                    if (index > len(season)/2) & (matchday.opponent_id in last_year_game_res[z - 1].keys()):\n",
    "                        new_col_last_year_game[index] = np.mean(last_year_game_res[z - 1][matchday.opponent_id] + last_year_game_res[z][matchday.opponent_id])\n",
    "                    elif (index <= len(season)/2) & (matchday.opponent_id in last_year_game_res[z - 1].keys()):\n",
    "                        new_col_last_year_game[index] = np.mean(last_year_game_res[z - 1][matchday.opponent_id])\n",
    "                    else:\n",
    "                        pass\n",
    "                index += 1\n",
    "            new_column_points.append(new_col_seas_p)\n",
    "            new_column_meanpoints.append(new_col_seas_mp)\n",
    "            #print(z, sum(new_col_last_year_game))\n",
    "            last_year_game.append(new_col_last_year_game)\n",
    "            z += 1\n",
    "        new_column_points = list(itertools.chain.from_iterable(new_column_points))\n",
    "        new_column_meanpoints = list(itertools.chain.from_iterable(new_column_meanpoints))\n",
    "        last_year_game = list(itertools.chain.from_iterable(last_year_game))\n",
    "        clubs[club][\"goal_diff\"] = goal_diff_seasons\n",
    "        clubs[club][\"points\"] = new_column_points\n",
    "        clubs[club][\"mean_points\"] = new_column_meanpoints\n",
    "        clubs[club][\"last_results\"] = last_year_game\n",
    "        clubs[club].loc[:,['points', 'mean_points', \"last_results\"]] = object.fit_transform(clubs[club].loc[:,['points', 'mean_points', \"last_results\"]])\n",
    "    return clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc620d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = new_data_test.return_dicts(\"result\")\n",
    "def points_and_co_oppon(clubs, result_dict):\n",
    "    point_dict = {result_dict['W']: 3, result_dict['L']: 0, result_dict['D']: 1}\n",
    "    for club in clubs:\n",
    "        #print(clubs[club])\n",
    "        oppon_points = []\n",
    "        oppon_meanpoints = []\n",
    "        for season in clubs[club].groupby(\"season_str\"):\n",
    "            season = season[1]\n",
    "            new_col_seas_oppon_p = [0] * len(season)\n",
    "            new_col_seas_oppon_mp = [0] * len(season)\n",
    "            index = 0\n",
    "            #print(season)\n",
    "            for _, matchday in season.iterrows():\n",
    "                if len(clubs[matchday.opponent_id].loc[:clubs[matchday.opponent_id][clubs[matchday.opponent_id].match_id == matchday.match_id].index[0],]) > 1:\n",
    "                    new_col_seas_oppon_p[index] = clubs[matchday.opponent_id].loc[:clubs[matchday.opponent_id][clubs[matchday.opponent_id].match_id == matchday.match_id].index[0],].iloc[-2,:].points\n",
    "                    new_col_seas_oppon_mp[index] = clubs[matchday.opponent_id].loc[:clubs[matchday.opponent_id][clubs[matchday.opponent_id].match_id == matchday.match_id].index[0],].iloc[-2,:].mean_points\n",
    "                    if clubs[matchday.opponent_id].loc[:clubs[matchday.opponent_id][clubs[matchday.opponent_id].match_id == matchday.match_id].index[0],].iloc[-2,:].schedule_date == matchday.schedule_date:\n",
    "                        print(\"Alarm\")\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                index += 1\n",
    "            oppon_points.append(new_col_seas_oppon_p)\n",
    "            oppon_meanpoints.append(new_col_seas_oppon_mp)\n",
    "        oppon_points = list(itertools.chain.from_iterable(oppon_points))\n",
    "        oppon_meanpoints = list(itertools.chain.from_iterable(oppon_meanpoints))\n",
    "        clubs[club][\"oppon_points\"] = oppon_points\n",
    "        clubs[club][\"oppon_mean_points\"] = oppon_meanpoints\n",
    "        \n",
    "    return clubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145dc762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clubs[matchday.opponent_id].loc[:clubs[matchday.opponent_id][clubs[matchday.opponent_id].match_id == matchday.match_id].index[0],].iloc[-2,:].schedule_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c879d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchday.schedule_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def game_dict(scale_df):\n",
    "    games_train = {}\n",
    "    for seas in scale_df.season_str.unique()[:-1]:\n",
    "        for week in sorted(scale_df.schedule_round.unique()):\n",
    "            if week not in games_train.keys():\n",
    "                games_train[week] = []\n",
    "            df = scale_df[(scale_df.season_str == seas) & (scale_df.schedule_round == week)]\n",
    "            for game in df.match_id:\n",
    "                games_train[week].append(game)\n",
    "\n",
    "    games_test = {}\n",
    "    seas = scale_df.season_str.unique()[-1]\n",
    "    for week in sorted(scale_df.schedule_round.unique()):\n",
    "        if week not in games_test.keys():\n",
    "            games_test[week] = []\n",
    "        df = scale_df[(scale_df.season_str == seas) & (scale_df.schedule_round == week)]\n",
    "        for game in df.match_id:\n",
    "            games_test[week].append(game)\n",
    "                \n",
    "    return games_train, games_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_train, game_valid = game_dict(scale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f921b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(clubs[12].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d28d5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earrange_list = [ 'result',\n",
    "     'gf',\n",
    "     'ga',\n",
    "     'goal_diff',\n",
    "     'xg',\n",
    "     'xga',\n",
    "     'shooting_standard_gls',\n",
    "     'shooting_standard_sh',\n",
    "     'shooting_standard_sot',\n",
    "     'shooting_standard_sot_perc',\n",
    "     'shooting_standard_g_per_sh',\n",
    "     'shooting_standard_g_per_sot',\n",
    "     'shooting_standard_dist',\n",
    "     'shooting_standard_fk',\n",
    "     'shooting_standard_pk',\n",
    "     'shooting_standard_pkatt',\n",
    "     'shooting_expected_npxg',\n",
    "     'shooting_expected_npxg_per_sh',\n",
    "     'shooting_expected_g_minus_xg',\n",
    "     'shooting_expected_npg_minus_xg',\n",
    "     'keeper_performance_sota',\n",
    "     'keeper_performance_saves',\n",
    "     'keeper_performance_save_perc',\n",
    "     'keeper_performance_cs',\n",
    "     'keeper_performance_psxg',\n",
    "     'keeper_performance_psxg_plus_minus',\n",
    "     'keeper_penaltykicks_pkatt',\n",
    "     'keeper_penaltykicks_pka',\n",
    "     'keeper_penaltykicks_pksv',\n",
    "     'keeper_penaltykicks_pkm',\n",
    "     'keeper_launched_cmp',\n",
    "     'keeper_launched_att',\n",
    "     'keeper_launched_cmp_perc',\n",
    "     'keeper_passes_att',\n",
    "     'keeper_passes_thr',\n",
    "     'keeper_passes_launch_perc',\n",
    "     'keeper_passes_avglen',\n",
    "     'keeper_goalkicks_att',\n",
    "     'keeper_goalkicks_launch_perc',\n",
    "     'keeper_goalkicks_avglen',\n",
    "     'keeper_crosses_opp',\n",
    "     'keeper_crosses_stp',\n",
    "     'keeper_crosses_stp_perc',\n",
    "     'keeper_sweeper_number_opa',\n",
    "     'keeper_sweeper_avgdist',\n",
    "     'passing_total_cmp',\n",
    "     'passing_total_att',\n",
    "     'passing_total_cmp_perc',\n",
    "     'passing_total_totdist',\n",
    "     'passing_total_prgdist',\n",
    "     'passing_short_cmp',\n",
    "     'passing_short_att',\n",
    "     'passing_short_cmp_perc',\n",
    "     'passing_medium_cmp',\n",
    "     'passing_medium_att',\n",
    "     'passing_medium_cmp_perc',\n",
    "     'passing_long_cmp',\n",
    "     'passing_long_att',\n",
    "     'passing_long_cmp_perc',\n",
    "     'passing_attacking_ast',\n",
    "     'passing_attacking_xag',\n",
    "     'passing_attacking_xa',\n",
    "     'passing_attacking_kp',\n",
    "     'passing_attacking_1_per_3',\n",
    "     'passing_attacking_ppa',\n",
    "     'passing_attacking_crspa',\n",
    "     'passing_attacking_prgp',\n",
    "     'passing_types_passtypes_live',\n",
    "     'passing_types_passtypes_dead',\n",
    "     'passing_types_passtypes_fk',\n",
    "     'passing_types_passtypes_tb',\n",
    "     'passing_types_passtypes_sw',\n",
    "     'passing_types_passtypes_crs',\n",
    "     'passing_types_passtypes_ti',\n",
    "     'passing_types_passtypes_ck',\n",
    "     'passing_types_cornerkicks_in',\n",
    "     'passing_types_cornerkicks_out',\n",
    "     'passing_types_cornerkicks_str',\n",
    "     'passing_types_outcomes_off',\n",
    "     'passing_types_outcomes_blocks',\n",
    "     'gca_scatypes_sca',\n",
    "     'gca_scatypes_passlive',\n",
    "     'gca_scatypes_passdead',\n",
    "     'gca_scatypes_to',\n",
    "     'gca_scatypes_sh',\n",
    "     'gca_scatypes_fld',\n",
    "     'gca_scatypes_def',\n",
    "     'gca_gcatypes_gca',\n",
    "     'gca_gcatypes_passlive',\n",
    "     'gca_gcatypes_passdead',\n",
    "     'gca_gcatypes_to',\n",
    "     'gca_gcatypes_sh',\n",
    "     'gca_gcatypes_fld',\n",
    "     'gca_gcatypes_def',\n",
    "     'defense_tackles_tkl',\n",
    "     'defense_tackles_tklw',\n",
    "     'defense_tackles_def3rd',\n",
    "     'defense_tackles_mid3rd',\n",
    "     'defense_tackles_att3rd',\n",
    "     'defense_challenges_tkl',\n",
    "     'defense_challenges_att',\n",
    "     'defense_challenges_tkl_perc',\n",
    "     'defense_challenges_lost',\n",
    "     'defense_blocks_blocks',\n",
    "     'defense_blocks_sh',\n",
    "     'defense_blocks_pass',\n",
    "     'defense_general_int',\n",
    "     'defense_general_tkl_plus_int',\n",
    "     'defense_general_clr',\n",
    "     'defense_general_err',\n",
    "     'possession_general_poss',\n",
    "     'possession_touches_touches',\n",
    "     'possession_touches_defpen',\n",
    "     'possession_touches_def3rd',\n",
    "     'possession_touches_mid3rd',\n",
    "     'possession_touches_att3rd',\n",
    "     'possession_touches_attpen',\n",
    "     'possession_touches_live',\n",
    "     'possession_takeons_att',\n",
    "     'possession_takeons_succ',\n",
    "     'possession_takeons_succ_perc',\n",
    "     'possession_takeons_tkld',\n",
    "     'possession_takeons_tkld_perc',\n",
    "     'possession_carries_carries',\n",
    "     'possession_carries_totdist',\n",
    "     'possession_carries_prgdist',\n",
    "     'possession_carries_prgc',\n",
    "     'possession_carries_1_per_3',\n",
    "     'possession_carries_cpa',\n",
    "     'possession_carries_mis',\n",
    "     'possession_carries_dis',\n",
    "     'possession_receiving_rec',\n",
    "     'possession_receiving_prgr',\n",
    "     'misc_performance_crdy',\n",
    "     'misc_performance_crdr',\n",
    "     'misc_performance_2crdy',\n",
    "     'misc_performance_fls',\n",
    "     'misc_performance_fld',\n",
    "     'misc_performance_off',\n",
    "     'misc_performance_og',\n",
    "     'misc_performance_recov',\n",
    "     'misc_aerialduels_won',\n",
    "     'misc_aerialduels_lost',\n",
    "     'misc_aerialduels_won_perc',\n",
    "     'attendance',\n",
    "     'points',\n",
    "     'mean_points',\n",
    "     'weekly_wages_eur',\n",
    "     'season_str',\n",
    "     'league_id',\n",
    "     'venue',\n",
    "     'team_id',\n",
    "     'opponent_id',\n",
    "     \"last_results\",\n",
    "     'oppon_points',\n",
    "     'oppon_mean_points',\n",
    "     'schedule_round',\n",
    "     'formation',\n",
    "     'captain',\n",
    "     'referee',\n",
    "     'schedule_day',\n",
    "     'match_id',\n",
    "     'schedule_date',\n",
    "     'schedule_time',\n",
    "     ]\n",
    "def inputs(games, clubs, rearrange_list):\n",
    "\n",
    "    lstm_inputs = [[], [], [], [], []] # [[input1], [input2], [result1], [gf1], [ga1], [g_diff1], [result2], [gf2], [ga2], [g_diff2], [features_nxt_game1], [features_nxt_game2]]\n",
    "    for i in range(7,len(games)): # not plus 1 as last matchday should yield no result, investigate!\n",
    "        #lstm_inputs[i] = [[], [], []]\n",
    "        print(i)\n",
    "        for game in games[i]:\n",
    "                # get ids of participating teams\n",
    "            team1 = scale_df[scale_df.match_id == game].iloc[0].team_id\n",
    "            team2 = scale_df[scale_df.match_id == game].iloc[1].team_id\n",
    "\n",
    "            df_team1 = clubs[team1]\n",
    "            df_team1 = df_team1[rearrange_list]\n",
    "\n",
    "            season = df_team1[df_team1.match_id == game].season_str.values[0]  # to only take history of current season into account\n",
    "            df_team1_past = df_team1.loc[df_team1[df_team1.season_str == season].iloc[0].name:df_team1[df_team1.match_id == game].index.values[0],:]  # df with all predecessing games in this season\n",
    "\n",
    "            df_team2 = clubs[team2]#  get teams df\n",
    "            df_team2 = df_team2[rearrange_list]\n",
    "\n",
    "            df_team2_past = df_team2.loc[df_team2[df_team2.season_str == season].iloc[0].name:df_team2[df_team2.match_id == game].index.values[0],:]  # df with all predecessing games in this season\n",
    "\n",
    "                # create zeros np array to store date\n",
    "            np_team1 = np.zeros([len(games), df_team1_past.loc[:,\"xg\":\"schedule_round\"].shape[1]])\n",
    "            np_team2 = np.zeros([len(games), df_team2_past.loc[:,\"xg\":\"schedule_round\"].shape[1]])\n",
    "\n",
    "         #   print(\"np_empty\", np_team1.shape)\n",
    "          #  print(\"np\", np_team1[- len(df_team1_past):, :-df_team1_past.loc[:, \"weekly_wages_eur\":\"schedule_round\"].shape[1]].shape)\n",
    "           # print(\"df\", df_team1_past.loc[:, \"weekly_wages_eur\":\"schedule_round\"].shape)\n",
    "\n",
    "                # insert data into dataframe (back to front) such that all input into the lstm has the same length (padding)\n",
    "            np_team1[- len(df_team1_past):, :-df_team1.loc[:, \"weekly_wages_eur\":\"schedule_round\"].shape[1]] = df_team1_past.loc[:,\"xg\": \"mean_points\"]\n",
    "            np_team2[- len(df_team2_past):, :-df_team1.loc[:, \"weekly_wages_eur\":\"schedule_round\"].shape[1]] = df_team2_past.loc[:,\"xg\": \"mean_points\"]\n",
    "\n",
    "                 # tensor for pytorch lstm \n",
    "                #np_team1 = torch.from_numpy(np_team1)\n",
    "                #np_team2 = torch.from_numpy(np_team2)\n",
    "\n",
    "                # if last game, no do not add as no future results to predict\n",
    "            if df_team1.iloc[-1].name == df_team1_past.iloc[-1].name:\n",
    "                pass\n",
    "\n",
    "            elif df_team2.iloc[-1].name == df_team2_past.iloc[-1].name:\n",
    "                pass\n",
    "\n",
    "                # if not last game, add to input dict together with result\n",
    "            else:\n",
    "                np_team1[- len(df_team1_past):, -df_team1.loc[:, \"weekly_wages_eur\":\"schedule_round\"].shape[1]:] = df_team1.loc[df_team1[df_team1.season_str == season].iloc[1].name:df_team1.loc[df_team1[df_team1.match_id == game].index.values[0]:,:].iloc[1].name,\"weekly_wages_eur\":\"schedule_round\"]  # df with all predecessing games in this season\n",
    "                np_team2[- len(df_team2_past):, -df_team1.loc[:, \"weekly_wages_eur\":\"schedule_round\"].shape[1]:] = df_team2.loc[df_team2[df_team2.season_str == season].iloc[1].name:df_team2.loc[df_team2[df_team2.match_id == game].index.values[0]:,:].iloc[1].name,\"weekly_wages_eur\":\"schedule_round\"]  # df with all predecessing games in this season\n",
    "                \n",
    "                results1 = np.zeros((len(games), 4))\n",
    "                results2 = np.zeros((len(games), 4))\n",
    "                #print(result1)\n",
    "                res1 = len(df_team1.loc[df_team1[df_team1.season_str == season].iloc[1].name:df_team1.loc[df_team1[df_team1.match_id == game].index.values[0]:,:].iloc[1].name,\"result\"])\n",
    "                res2 = len(df_team2.loc[df_team2[df_team2.season_str == season].iloc[1].name:df_team2.loc[df_team2[df_team2.match_id == game].index.values[0]:,:].iloc[1].name,\"result\"])\n",
    "                results1[-res1:, 0] = df_team1.loc[df_team1[df_team1.season_str == season].iloc[1].name:df_team1.loc[df_team1[df_team1.match_id == game].index.values[0]:,:].iloc[1].name,\"result\"]\n",
    "                results1[-res1:, 1] = df_team1.loc[df_team1[df_team1.season_str == season].iloc[1].name:df_team1.loc[df_team1[df_team1.match_id == game].index.values[0]:,:].iloc[1].name,\"gf\"]\n",
    "                results1[-res1:, 2] = df_team1.loc[df_team1[df_team1.season_str == season].iloc[1].name:df_team1.loc[df_team1[df_team1.match_id == game].index.values[0]:,:].iloc[1].name,\"ga\"]\n",
    "                results1[-res1:, 3] = df_team1.loc[df_team1[df_team1.season_str == season].iloc[1].name:df_team1.loc[df_team1[df_team1.match_id == game].index.values[0]:,:].iloc[1].name,\"goal_diff\"]\n",
    "                results2[-res2:, 0] = df_team2.loc[df_team2[df_team2.season_str == season].iloc[1].name:df_team2.loc[df_team2[df_team2.match_id == game].index.values[0]:,:].iloc[1].name,\"result\"]\n",
    "                results2[-res2:, 1] = df_team2.loc[df_team2[df_team2.season_str == season].iloc[1].name:df_team2.loc[df_team2[df_team2.match_id == game].index.values[0]:,:].iloc[1].name,\"gf\"]\n",
    "                results2[-res2:, 2] = df_team2.loc[df_team2[df_team2.season_str == season].iloc[1].name:df_team2.loc[df_team2[df_team2.match_id == game].index.values[0]:,:].iloc[1].name,\"ga\"]\n",
    "                results2[-res2:, 3] = df_team2.loc[df_team2[df_team2.season_str == season].iloc[1].name:df_team2.loc[df_team2[df_team2.match_id == game].index.values[0]:,:].iloc[1].name,\"goal_diff\"]\n",
    "                #fut_feature1 = np.array(df_team1.loc[df_team1[df_team1.match_id == game].index.values[0]:,:].iloc[1,2:].values)\n",
    "                #fut_feature2 = np.array(df_team2.loc[df_team2[df_team2.match_id == game].index.values[0]:,:].iloc[1,2:].values)\n",
    "                #print(result1)\n",
    "                result1 = torch.nn.functional.one_hot(torch.tensor(results1[:,0]).long(), num_classes = 3)\n",
    "                result2 = torch.nn.functional.one_hot(torch.tensor(results2[:,0]).long(), num_classes = 3)\n",
    "                lstm_inputs[0].append(torch.tensor(np_team1))\n",
    "                lstm_inputs[1].append(torch.tensor(np_team2))\n",
    "                lstm_inputs[2].append(result1)\n",
    "                lstm_inputs[3].append(result2)\n",
    "                #lstm_inputs[4].append(fut_feature1)\n",
    "                #lstm_inputs[5].append(fut_feature2)\n",
    "    return lstm_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7119e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inputs = inputs(game_train, clubs)\n",
    "valid_inputs = inputs(game_valid, clubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd9c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_to_lstm():\n",
    "\n",
    "    def __init__(self, mylist):\n",
    "        self.output = []\n",
    "        self.team_home = mylist[0]\n",
    "        self.team_away = mylist[1]\n",
    "        self.result1 = mylist[2]\n",
    "        self.result2 = mylist[3]\n",
    "        #self.team_home_next = mylist[4]\n",
    "        #self.team_away_next = mylist[5]\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.team_home)):\n",
    "            self.output.append(((self.team_home[i], self.team_away[i]), (self.result1[i], self.result2[i])))#, (self.team_home_next[i], self.team_away_next[i])))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.output)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.output[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1527e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_for_loader = data_to_lstm(train_inputs)\n",
    "valid_for_loader = data_to_lstm(valid_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815db868",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_for_loader, batch_size = 32, drop_last = True)\n",
    "test_loader = torch.utils.data.DataLoader(valid_for_loader, batch_size = 32, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0dffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_2LSTM_1(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes, num_layers = 1):\n",
    "        super(Sport_pred_2LSTM_1, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = num_layers # number of LSTM layers (stacked)\n",
    "        \n",
    "        # two separate lstms to account for every teams history\n",
    "        self.l_lstm1 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "        \n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        #self.l_linear2 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        \n",
    "        \n",
    "        self.soft = torch.nn.Softmax()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigm = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        \n",
    "        \n",
    "        # initialize values for two lstms\n",
    "        #h01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #c01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        \n",
    "        \n",
    "        # run data through lstm and yield output\n",
    "        lstm_out1,(ht1, ct1) = self.l_lstm1(x)#,(h01, c01))\n",
    "        ht1 = ht1.squeeze()#[-1,:]\n",
    "        ct1 = ct1.squeeze()#[-1,:]\n",
    "        #print(lstm_out1.shape)\n",
    "        out = lstm_out1#[:,-1,:]#.contiguous().view(-1, self.n_hidden)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        # run lstm output through nn to predict outcome\n",
    "        result = self.l_linear1(out)\n",
    "        #result = self.l_linear2(result)\n",
    "        #result = self.soft(result)\n",
    "        #result = self.sigm(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66653f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_2LSTM_2(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_2LSTM_2, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        \n",
    "        # two separate lstms to account for every teams history\n",
    "        self.l_lstm1 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "        \n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        \n",
    "        \n",
    "        self.soft = torch.nn.Softmax()\n",
    "        #self.relu = torch.nn.SiLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        \n",
    "        \n",
    "        # initialize values for two lstms\n",
    "        #h01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #c01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        \n",
    "        \n",
    "        # run data through lstm and yield output\n",
    "        lstm_out1,(ht1, ct1) = self.l_lstm1(x)#,(h01, c01))\n",
    "        \n",
    "        out = lstm_out1#.contiguous().view(-1, self.n_hidden)\n",
    "\n",
    "        result = self.l_linear1(out)\n",
    "        #result = self.soft(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6040277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sport_pred_2LSTM(lstm1, lstm2, train_loader, test_loader, learn_rate, epochs):\n",
    "    criterion1 = torch.nn.MSELoss()\n",
    "    criterion2 = torch.nn.MSELoss()\n",
    "    optimizer1 = torch.optim.SGD(lstm1.parameters(), lr = learn_rate, weight_decay = 0.01)\n",
    "    optimizer2 = torch.optim.SGD(lstm2.parameters(), lr = learn_rate, weight_decay = 0.01)\n",
    "    \n",
    "    lstm1.eval()\n",
    "    lstm2.eval()\n",
    "    accur1 = 0\n",
    "    accur2 = 0\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(test_loader): #, (next_feat1, next_feat2)\n",
    "        pred1 = lstm1(input1)\n",
    "        pred2 = lstm2(input2)\n",
    "        #print(pred.shape)\n",
    "        pred1 = torch.argmax(pred1, dim = 1)\n",
    "        pred2 = torch.argmax(pred2, dim = 1)\n",
    "        #print(pred.shape)\n",
    "        #result = torch.nn.functional.one_hot(result.to(torch.int64), num_classes = 3)\n",
    "        result1 = torch.argmax(result1, dim = 1)\n",
    "        result2 = torch.argmax(result2, dim = 1)\n",
    "        accur1 += pred1.eq(result1).sum().item()\n",
    "        accur2 += pred2.eq(result2).sum().item()\n",
    "    print(f\"Loss accuracy training 1: {100 * accur1 /((step + 1) * 32)}% \\n Loss accuracy training 2: {100 * accur2 /((step + 1) * 32)}%\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses_val1 = []\n",
    "        losses_val2 = []\n",
    "        accuracies1 = []\n",
    "        accuracies2 = []\n",
    "        \n",
    "        lstm1.train()\n",
    "        lstm2.train()\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        for step, ((input1, input2), (result1, result2)) in enumerate(train_loader): #, (next_feat1, next_feat2)\n",
    "            lstm1.zero_grad()\n",
    "            lstm2.zero_grad()\n",
    "            pred1 = lstm1(input1)\n",
    "            pred2 = lstm2(input2)\n",
    "            #result1 = torch.nn.functional.one_hot(result1.to(torch.int64), num_classes = 3).to(torch.float32)\n",
    "            #result2 = torch.nn.functional.one_hot(result2.to(torch.int64), num_classes = 3).to(torch.float32)\n",
    "            #print(f\"pred {pred.dtype}, result {result.dtype}\")\n",
    "            loss1 = criterion1(pred1.to(torch.float32), result1.to(torch.float32))\n",
    "            loss2 = criterion2(pred2.to(torch.float32), result2.to(torch.float32))\n",
    "            loss1.backward()\n",
    "            loss2.backward()\n",
    "            optimizer1.step()\n",
    "            optimizer2.step()\n",
    "        \n",
    "        lstm1.eval()\n",
    "        lstm2.eval()\n",
    "        loss1 = 0\n",
    "        loss2 = 0\n",
    "        accur1 = 0\n",
    "        accur2 = 0\n",
    "        for step, ((input1, input2), (result1, result2)) in enumerate(test_loader): #, (next_feat1, next_feat2)\n",
    "            pred1 = lstm1(input1)\n",
    "            pred2 = lstm1(input2)\n",
    "            #result_oh1 = result1.to(torch.int64)\n",
    "            #result_oh2 = result2.to(torch.int64)\n",
    "            #print(result.shape)\n",
    "            #result_oh1 = torch.nn.functional.one_hot(result_oh1, num_classes = 3)\n",
    "            #result_oh2 = torch.nn.functional.one_hot(result_oh2, num_classes = 3)\n",
    "            #print(result.shape)\n",
    "            #print(pred.shape)\n",
    "            loss1 += criterion1(pred1, result1.to(torch.float32)).item()\n",
    "            loss2 += criterion2(pred2, result2.to(torch.float32)).item()\n",
    "            result1 = torch.argmax(result1, dim = 1)[-1]\n",
    "            result2 = torch.argmax(result2, dim = 1)[-1]\n",
    "            pred1 = torch.argmax(pred1, dim = 1)[-1]\n",
    "            pred2 = torch.argmax(pred2, dim = 1)[-1]\n",
    "            #print(pred.shape)\n",
    "            #print(result1.shape)\n",
    "            accur1 += pred1.eq(result1).sum().item()\n",
    "            accur2 += pred2.eq(result2).sum().item()\n",
    "        losses_val1.append(loss1)\n",
    "        losses_val2.append(loss2)\n",
    "        accuracy1 = 100 * (accur1 /((step + 1) * 32))\n",
    "        accuracy2 = 100 * (accur2 /((step + 1) * 32))\n",
    "        accuracies1.append(accuracy1)\n",
    "        accuracies2.append(accuracy2)\n",
    "        print(f\"Loss 1 {loss1} accuracy 1 {accuracy1}% \\n Loss 2 {loss2} accuracy 2 {accuracy2}% \") \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3793896",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#wdc = [0.2, 0.15, 0.1, 0.05, 0]\n",
    "#lrs = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "#for lr in lrs:\n",
    "#for wd in wdc:\n",
    "net = Sport_pred_2LSTM_1(153, 153, 3, 2)\n",
    "print(f\"\\n\\nLR: {lr}, WD: {wd}\")\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 1e-5, weight_decay = 0.01)\n",
    "\n",
    "net.eval()\n",
    "accur = 0\n",
    "for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "    pred = net(input1)\n",
    "    #print(pred.shape)\n",
    "    #print(result1.shape)\n",
    "    pred = torch.argmax(pred[:,-1,:], dim = 1)\n",
    "    result = result1[:,-1,:]\n",
    "    result = torch.argmax(result, dim = 1)\n",
    "    accur += pred.eq(result).sum().item()\n",
    "\n",
    "    pred = net(input2)\n",
    "    pred = torch.argmax(pred[:,-1,:], dim = 1)\n",
    "    result2 = result2[:,-1,:]\n",
    "    result2 = torch.argmax(result2, dim = 1)\n",
    "    accur += pred.eq(result2).sum().item()\n",
    "print(f\"Loss accuracy training: {100 * accur /((step + 1) * 64)}%\")\n",
    "\n",
    "for epoch in range(25):\n",
    "    losses_val = []\n",
    "    accuracies = []\n",
    "\n",
    "    net.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        pred = net(input1)\n",
    "        #result = torch.nn.functional.one_hot(result.to(torch.int64), num_classes = 3).to(torch.float32)\n",
    "        #print(f\"pred {pred.dtype}, result {result.dtype}\")\n",
    "        #result1 = result1[:,-1,:]\n",
    "        loss = criterion(pred.float(), result1.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        net.zero_grad()\n",
    "        pred = net(input2)\n",
    "        #result2 = result2[:,-1,:]\n",
    "        #result = torch.nn.functional.one_hot(result.to(torch.int64), num_classes = 3).to(torch.float32)\n",
    "        #print(f\"pred {pred.dtype}, result {result.dtype}\")\n",
    "        loss = criterion(pred.float(), result2.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    accur = 0\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "        pred = net(input1)\n",
    "        #print(pred)\n",
    "        #print(pred.shape)\n",
    "        result1 = result1[:,-1,:]\n",
    "        pred = pred[:,-1,:]\n",
    "        loss += criterion(pred, result1).item()\n",
    "        #\n",
    "        #print(pred)\n",
    "        #print(pred.shape)\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        #print(pred)\n",
    "        #print(pred.shape)\n",
    "\n",
    "        result1 = torch.argmax(result1, dim = 1)\n",
    "        accur += pred.eq(result1).sum().item()\n",
    "       # if epoch == 9:\n",
    "        #    print(\"pred\", pred)\n",
    "         #   print(\"result\", result1)\n",
    "        pred = net(input2)\n",
    "        result2 = result2[:,-1,:]\n",
    "        pred = pred[:,-1,:]\n",
    "        loss += criterion(pred, result2).item()\n",
    "        #pred = pred[:,-1,:]\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        result2  = torch.argmax(result2, dim = 1)\n",
    "        accur += pred.eq(result2).sum().item()\n",
    "    losses_val.append(loss)\n",
    "    accuracy = 100 * (accur /((step + 1) * 64))\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Loss {loss} accuracy {accuracy}%\") \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164d41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8c405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb7d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0ca99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class res_to_res(torch.nn.Module):\n",
    "    def __init__(self, n_features, num_layers):\n",
    "        super(res_to_res, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.fcs = []\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "        self.soft = torch.nn.Softmax()\n",
    "        \n",
    "        for i in range(self.num_layers):               # build hidden layers \n",
    "            input_size = n_features if i == 0 else 10\n",
    "            fc = torch.nn.Linear(input_size, 10)\n",
    "            setattr(self, 'fc%i' % i, fc)       # set layer to the Module\n",
    "            self._set_init(fc)                  # parameters initialization\n",
    "            self.fcs.append(fc)\n",
    "\n",
    "        self.predict = torch.nn.Linear(10, 3)         # output layer\n",
    "        self._set_init(self.predict)            # parameters initialization\n",
    "\n",
    "    def _set_init(self, layer):\n",
    "        torch.nn.init.normal_(layer.weight, mean=0., std=.1)\n",
    "        #torch.nn.init.constant_(layer.bias, self.bias_init)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.fcs[i](x)\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        out = self.predict(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5315a41a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net2 = res_to_res(6,3)\n",
    "\n",
    "#net = Sport_pred_2LSTM_1(151, 151, 3)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net2.parameters(), lr = 1e-3)\n",
    "    \n",
    "net.eval()\n",
    "net2.eval()\n",
    "accur = 0\n",
    "for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "    pred1 = net(input1)\n",
    "    pred2 = net(input2)\n",
    "    pred = torch.concat([pred1, pred2], dim = 2)[:,-1,:]\n",
    "    pred = net2(pred)\n",
    "    result = result1[:,-1,:]\n",
    "    pred = torch.argmax(pred, dim = 1)\n",
    "    result = torch.argmax(result, dim = 1)\n",
    "    accur += pred.eq(result).sum().item()\n",
    "    \n",
    "print(f\"Loss accuracy training: {100 * accur /((step + 1) * 32)}%\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses_val = []\n",
    "    accuracies = []\n",
    "\n",
    "    net2.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(train_loader):\n",
    "        net2.zero_grad()\n",
    "        pred1 = net(input1)\n",
    "        pred2 = net(input2)\n",
    "        pred = torch.concat([pred1, pred2], dim = 2)[:,-1,:]\n",
    "        pred = net2(pred)\n",
    "        result1 = result1[:,-1,:]\n",
    "        loss = criterion(pred.float(), result1.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    net2.eval()\n",
    "    loss = 0\n",
    "    accur = 0\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "            pred1 = net(input1)\n",
    "            pred2 = net(input2)\n",
    "            pred = torch.concat([pred1, pred2], dim = 2)[:,-1,:]\n",
    "            pred = net2(pred)\n",
    "            result = result1[:,-1,:]\n",
    "            loss += criterion(pred, result).item()\n",
    "            pred = torch.argmax(pred, dim = 1)\n",
    "            result = torch.argmax(result, dim = 1)\n",
    "            accur += pred.eq(result).sum().item()\n",
    "    losses_val.append(loss)\n",
    "    accuracy = 100 * (accur /((step + 1) * 32))\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Loss {loss} accuracy {accuracy}%\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b479a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.concat([pred1, pred2], dim = 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68b420e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_2LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_2LSTM, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        \n",
    "        # two separate lstms to account for every teams history\n",
    "        self.l_lstm1 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "        \n",
    "        self.l_lstm2 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear2 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear3 = torch.nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "        self.l_linear4 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        \n",
    "        \n",
    "        self.soft = torch.nn.Softmax()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        y = y.to(torch.float32)\n",
    "        y = torch.nan_to_num(y, nan = 0.0)    \n",
    "        \n",
    "        \n",
    "        # initialize values for two lstms\n",
    "        h01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        h02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        c01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        c02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        \n",
    "        \n",
    "        # run data through lstm and yield output\n",
    "        lstm_out1,(ht1, ct1) = self.l_lstm1(x,(h01, c01))\n",
    "        lstm_out2,(ht2, ct2) = self.l_lstm2(x,(h02, c02))\n",
    "        ht1 = ht1.squeeze()#[-1,:]\n",
    "        ht2 = ht2.squeeze()#[-1,:]\n",
    "        ct1 = ct1.squeeze()#[-1,:]\n",
    "        ct2 = ct2.squeeze()#[-1,:]\n",
    "        \n",
    "        #print(\"ht1\", ht1.shape)\n",
    "        #print(\"ht2\", ht2.shape)\n",
    "        #print(\"ct1\", ct1.shape)\n",
    "        #print(\"ct2\", ct2.shape)\n",
    "        #print(\"lstm_out1\",lstm_out1.shape)\n",
    "        #print(\"lstm_out2\",lstm_out2.shape)\n",
    "        \n",
    "        x = lstm_out1[:,-1,:]\n",
    "        y = lstm_out2[:,-1,:]\n",
    "        \n",
    "        #print(\"x\", x.shape)\n",
    "        #print(\"y\", y.shape)\n",
    "        \n",
    "        # run lstm output through nn to predict outcome\n",
    "        in_lay1 = torch.cat((x, y), 1)\n",
    "        #in_lay1 = torch.cat((ht1, ct1, ht2, ct2), -1)\n",
    "        #print(\"in_lay1\", in_lay1.shape)\n",
    "        out_lay1 = self.l_linear1(in_lay1)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay2 = self.relu(out_lay1)\n",
    "        #print(\"in_lay2\", in_lay2.shape)\n",
    "        out_lay2 = self.l_linear2(in_lay2)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay3 = self.relu(out_lay2)\n",
    "        out_lay3 = self.l_linear3(in_lay3)\n",
    "        #print(\"in_lay3\", in_lay3.shape)\n",
    "        \n",
    "        in_lay4 = self.relu(out_lay3)\n",
    "        out = self.l_linear4(in_lay4)\n",
    "        #print(\"out\", out.shape)\n",
    "        \n",
    "        #print(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d12f610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))[0][0].shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0b6bb19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss accuracy training: 38.72282608695652%\n",
      "Epoch: 0\n",
      "Loss 21.013176426291466 accuracy 19.66873706004141%\n",
      "Epoch: 1\n",
      "Loss 20.596253916621208 accuracy 19.66873706004141%\n",
      "Epoch: 2\n",
      "Loss 20.46710443496704 accuracy 19.66873706004141%\n",
      "Epoch: 3\n",
      "Loss 20.405873775482178 accuracy 19.66873706004141%\n",
      "Epoch: 4\n",
      "Loss 20.362672597169876 accuracy 19.66873706004141%\n",
      "Epoch: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, ((input1, input2), (result1, result2)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     26\u001b[0m     net\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#result = torch.nn.functional.one_hot(result.to(torch.int64), num_classes = 3).to(torch.float32)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#print(f\"pred {pred.dtype}, result {result.dtype}\")\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     result1 \u001b[38;5;241m=\u001b[39m result1[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36mSport_pred_2LSTM.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     43\u001b[0m c02 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_hidden)\u001b[38;5;241m.\u001b[39mto(y\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# run data through lstm and yield output\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m lstm_out1,(ht1, ct1) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml_lstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh01\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc01\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m lstm_out2,(ht2, ct2) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml_lstm2(x,(h02, c02))\n\u001b[1;32m     49\u001b[0m ht1 \u001b[38;5;241m=\u001b[39m ht1\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;66;03m#[-1,:]\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "len_input = next(iter(test_loader))[0][0].shape[2]\n",
    "net = Sport_pred_2LSTM(len_input, len_input, 3)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 1e-5, weight_decay = 0.1)\n",
    "    \n",
    "net.eval()\n",
    "accur = 0\n",
    "for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "    #print(input1.shape)\n",
    "    pred = net(input1, input2)\n",
    "    pred = torch.argmax(pred, dim = 1)\n",
    "    result1 = result1[:,-1,:]\n",
    "    result1 = torch.argmax(result1, dim = 1)\n",
    "    accur += pred.eq(result1).sum().item()\n",
    "\n",
    "print(f\"Loss accuracy training: {100 * accur /((step + 1) * 32)}%\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    losses_val = []\n",
    "    accuracies = []\n",
    "\n",
    "    net.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "        pred = net(input1, input2)\n",
    "        #result = torch.nn.functional.one_hot(result.to(torch.int64), num_classes = 3).to(torch.float32)\n",
    "        #print(f\"pred {pred.dtype}, result {result.dtype}\")\n",
    "        result1 = result1[:,-1,:]\n",
    "        loss = criterion(pred.float(), result1.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    accur = 0\n",
    "    for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "        pred = net(input1, input2)\n",
    "        #print(pred)\n",
    "        #print(pred.shape)\n",
    "        result1 = result1[:,-1,:]\n",
    "        loss += criterion(pred, result1).item()\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        #print(pred)\n",
    "        #print(pred.shape)\n",
    "        result1 = torch.argmax(result1, dim = 1)\n",
    "        accur += pred.eq(result1).sum().item()\n",
    "        \n",
    "    losses_val.append(loss)\n",
    "    accuracy = 100 * (accur /((step + 1) * 63))\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Loss {loss} accuracy {accuracy}%\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99bcbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result1[:,-1,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "        pred = net(input1)\n",
    "        loss += criterion(pred, result1).item()\n",
    "        print(pred.shape)\n",
    "        print(pred)\n",
    "        pred = pred[:,-1,:]\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        print(pred.shape)\n",
    "        print(pred)\n",
    "        result1 = result1[:,-1,:]\n",
    "        result1 = torch.argmax(result1, dim = 1)\n",
    "        accur += pred.eq(result1).sum().item()\n",
    "        print(\"pred\", pred)\n",
    "        print(\"result\", result1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af760828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368be2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred.eq(result2)#.sum().item()#/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.eq(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_test.return_dicts(\"result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, ((input1, input2), (result1, result2)) in enumerate(test_loader):\n",
    "    print(result1[-1,:,:].shape)\n",
    "    print(result1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bb9adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249dc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = preprocess(data)\n",
    "new_data1 = new_data.data_frame#.dataset_team(\"e0652b02\").captain.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330e991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544316df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_data.data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab9a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_df = new_data.data_frame\n",
    "\n",
    "liste = ['schedule_time', 'schedule_round', 'schedule_day', 'result_home', 'gf_home', 'gf_away', 'xg_home',\n",
    "         'xg_away', 'formation_home', 'referee', 'fbref_season', 'fbref_league_id', 'fbref_home_id',\n",
    "         'fbref_away_id', 'fbref_match_id', 'result_away', 'captain_away', 'formation_away', 'captain_home',\n",
    "         'captain_away']\n",
    "\n",
    "cols_to_scale = list(set(list(scale_df.columns)).difference(liste))\n",
    "object = StandardScaler()\n",
    "\n",
    "scale_df.loc[:,cols_to_scale] = object.fit_transform(scale_df.loc[:,cols_to_scale])\n",
    "scale_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b360c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def game_dict(scale_df):\n",
    "    games_train = {}\n",
    "    for seas in scale_df.fbref_season.unique()[:-1]:\n",
    "        for week in sorted(scale_df.schedule_round.unique()):\n",
    "            if week not in games_train.keys():\n",
    "                games_train[week] = []\n",
    "            df = scale_df[(scale_df.fbref_season == seas) & (scale_df.schedule_round == week)]\n",
    "            for game in df.fbref_match_id:\n",
    "                games_train[week].append(game)\n",
    "\n",
    "    games_test = {}\n",
    "    seas = scale_df.fbref_season.unique()[-1]\n",
    "    for week in sorted(scale_df.schedule_round.unique()):\n",
    "        if week not in games_test.keys():\n",
    "            games_test[week] = []\n",
    "        df = scale_df[(scale_df.fbref_season == seas) & (scale_df.schedule_round == week)]\n",
    "        for game in df.fbref_match_id:\n",
    "            games_test[week].append(game)\n",
    "                \n",
    "    return games_train, games_test\n",
    "#games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7edf2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_train, game_valid = game_dict(scale_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa131b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_preprocess(df, where):\n",
    "        colname_dict = {}\n",
    "        result_dict = new_data.return_dicts(\"result\")\n",
    "        for colname in df.loc[:, df.columns.str.endswith(where)]:\n",
    "            colname_dict[colname]  = colname[:-5]\n",
    "    \n",
    "        return df.rename(colname_dict, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_team(data_frame, team_id):\n",
    "        # split dataset \n",
    "        random_team1 = data_frame[data_frame.fbref_home_id == team_id].loc[:, data_frame.columns.str.endswith('home')]\n",
    "        random_team2 = data_frame[data_frame.fbref_away_id == team_id].loc[:, data_frame.columns.str.endswith('away')]\n",
    "        random_team3 = data_frame[(data_frame.fbref_home_id == team_id) |\n",
    "                        (data_frame.fbref_away_id == team_id)].loc[:,['schedule_time', 'attendance',\n",
    "                                                                           'referee', 'fbref_season',\n",
    "                                                                           'fbref_league_id', 'fbref_home_id',\n",
    "                                                                           'fbref_away_id', 'fbref_match_id']]\n",
    "\n",
    "        #random_team2.rename({\"gf_away\": \"gf_home\", \"gf_home\": \"gf_away\"}, axis = 1, inplace = True)\n",
    "        # preprocess chunks\n",
    "        concat_preprocess(random_team1, \"home\")\n",
    "        concat_preprocess(random_team2, \"away\")\n",
    "        random_team = pd.concat([random_team1, random_team2])\n",
    "\n",
    "        # dummy to account for home and away\n",
    "        random_team3[\"home\"] = np.where(random_team3.fbref_home_id == team_id, 1, 0)\n",
    "\n",
    "        # rename columns to change them\n",
    "        random_team3_away = random_team3.loc[random_team3.fbref_away_id == team_id].copy()\n",
    "        random_team3_away.rename({\"fbref_home_id\": \"fbref_away_id\",\n",
    "                                  \"fbref_away_id\": \"fbref_home_id\"}, axis = 1, inplace = True)\n",
    "        random_team3_home = random_team3[random_team3.fbref_home_id == team_id]\n",
    "\n",
    "        # concat after changing column names and rename \n",
    "        random_team3 = pd.concat([random_team3_home, random_team3_away])\n",
    "        random_team3.rename({\"fbref_home_id\": \"fbref_own\", \n",
    "                             \"fbref_away_id\": \"fbref_oppon\",\n",
    "                             \"gf_home\": \"gf_own\", \n",
    "                             \"gf_away\": \"gf_own\"}, axis = 1, inplace = True)\n",
    "\n",
    "        end_team = random_team.merge(random_team3, \"left\", on = [\"schedule_date\"], sort = True).copy()\n",
    "\n",
    "        return end_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f036c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clubs = {}\n",
    "for club in set(list(scale_df.fbref_home_id) + list(scale_df.fbref_away_id)):\n",
    "    clubs[club] = dataset_team(scale_df, club)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383d620",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def inputs(games, clubs):\n",
    "    #lstm_inputs = {}\n",
    "\n",
    "    rearrange_list = ['result', 'gf', 'xg', 'captain', 'formation',\n",
    "     'shooting_standard_gls', 'shooting_standard_sh', 'shooting_standard_sot', 'shooting_standard_sot_perc',\n",
    "     'shooting_standard_g_per_sh', 'shooting_standard_g_per_sot', 'shooting_standard_dist', 'shooting_standard_fk',\n",
    "     'shooting_standard_pk', 'shooting_standard_pkatt', 'shooting_expected_npxg', 'shooting_expected_npxg_per_sh',\n",
    "     'shooting_expected_g_minus_xg', 'shooting_expected_npg_minus_xg', 'keeper_performance_sota', 'keeper_performance_saves',\n",
    "     'keeper_performance_save_perc', 'keeper_performance_cs', 'keeper_performance_psxg', 'keeper_performance_psxg_plus_minus',\n",
    "     'keeper_penaltykicks_pkatt', 'keeper_penaltykicks_pksv', 'keeper_penaltykicks_pkm', 'keeper_launched_cmp',\n",
    "     'keeper_launched_att', 'keeper_launched_cmp_perc', 'keeper_passes_att', 'keeper_passes_thr', 'keeper_passes_launch_perc',\n",
    "     'keeper_passes_avglen', 'keeper_goalkicks_att', 'keeper_goalkicks_launch_perc', 'keeper_goalkicks_avglen',\n",
    "     'keeper_crosses_opp', 'keeper_crosses_stp', 'keeper_crosses_stp_perc', 'keeper_sweeper_number_opa', 'keeper_sweeper_avgdist',\n",
    "     'passing_total_cmp', 'passing_total_att', 'passing_total_cmp_perc', 'passing_total_totdist', 'passing_total_prgdist',\n",
    "     'passing_short_cmp', 'passing_short_att', 'passing_short_cmp_perc', 'passing_medium_cmp', 'passing_medium_att',\n",
    "     'passing_medium_cmp_perc', 'passing_long_cmp', 'passing_long_att', 'passing_long_cmp_perc', 'passing_attacking_ast',\n",
    "     'passing_attacking_xag', 'passing_attacking_xa', 'passing_attacking_kp', 'passing_attacking_1_per_3',\n",
    "     'passing_attacking_ppa', 'passing_attacking_crspa', 'passing_attacking_prgp', 'passing_types_passtypes_live',\n",
    "     'passing_types_passtypes_dead', 'passing_types_passtypes_fk', 'passing_types_passtypes_tb', 'passing_types_passtypes_sw',\n",
    "     'passing_types_passtypes_crs', 'passing_types_passtypes_ti', 'passing_types_passtypes_ck', 'passing_types_cornerkicks_in',\n",
    "     'passing_types_cornerkicks_out', 'passing_types_cornerkicks_str', 'passing_types_outcomes_off', 'passing_types_outcomes_blocks',\n",
    "     'gca_scatypes_sca', 'gca_scatypes_passlive', 'gca_scatypes_passdead', 'gca_scatypes_to', 'gca_scatypes_sh',\n",
    "     'gca_scatypes_fld', 'gca_scatypes_def', 'gca_gcatypes_gca', 'gca_gcatypes_passlive','gca_gcatypes_passdead',\n",
    "     'gca_gcatypes_to', 'gca_gcatypes_sh', 'gca_gcatypes_fld', 'gca_gcatypes_def', 'defense_tackles_tkl', 'defense_tackles_tklw',\n",
    "     'defense_tackles_def3rd', 'defense_tackles_mid3rd', 'defense_tackles_att3rd', 'defense_challenges_tkl',\n",
    "     'defense_challenges_att', 'defense_challenges_tkl_perc', 'defense_challenges_lost', 'defense_blocks_blocks',\n",
    "     'defense_blocks_sh', 'defense_blocks_pass', 'defense_general_int', 'defense_general_tkl_plus_int',\n",
    "     'defense_general_clr', 'defense_general_err', 'possession_general_poss', 'possession_touches_touches',\n",
    "     'possession_touches_defpen', 'possession_touches_def3rd', 'possession_touches_mid3rd', 'possession_touches_att3rd',\n",
    "     'possession_touches_attpen', 'possession_touches_live', 'possession_takeons_att', 'possession_takeons_succ',\n",
    "     'possession_takeons_succ_perc', 'possession_takeons_tkld', 'possession_takeons_tkld_perc', 'possession_carries_carries', \n",
    "     'possession_carries_totdist', 'possession_carries_prgdist', 'possession_carries_prgc', 'possession_carries_1_per_3',\n",
    "     'possession_carries_cpa', 'possession_carries_mis', 'possession_carries_dis', 'possession_receiving_rec', \n",
    "     'possession_receiving_prgr', 'misc_performance_crdy', 'misc_performance_crdr', 'misc_performance_2crdy',\n",
    "     'misc_performance_fls', 'misc_performance_fld', 'misc_performance_off', 'misc_performance_og', 'misc_performance_recov', \n",
    "     'misc_aerialduels_won', 'misc_aerialduels_lost', 'misc_aerialduels_won_perc', 'keeper_penaltykicks_pka', 'attendance',\n",
    "     'schedule_time', 'referee', 'fbref_season', 'fbref_league_id', 'fbref_own', 'fbref_oppon', 'fbref_match_id', 'home']\n",
    "    lstm_inputs = [[], [], [], [], []]\n",
    "    for i in range(7,len(games)): # not plus 1 as last matchday should yield no result, investigate!\n",
    "        #lstm_inputs[i] = [[], [], []]\n",
    "        print(i)\n",
    "        for game in games[i]:\n",
    "            # get ids of participating teams\n",
    "            team1 = int(scale_df[scale_df.fbref_match_id == game].fbref_home_id.values)\n",
    "            team2 = int(scale_df[scale_df.fbref_match_id == game].fbref_away_id.values)\n",
    "\n",
    "            #  get teams df\n",
    "            df_team1 = clubs[team1]#  get teams df\n",
    "            df_team1 = df_team1[rearrange_list]\n",
    "\n",
    "            season = df_team1[df_team1.fbref_match_id == game].fbref_season.values[0]  # to only take history of current season into account\n",
    "            df_team1_past = df_team1.loc[df_team1[df_team1.fbref_season == season].iloc[0].name:df_team1[df_team1.fbref_match_id == game].index.values[0],:]  # df with all predecessing games in this season\n",
    "\n",
    "            df_team2 = clubs[team2]#  get teams df\n",
    "            df_team2 = df_team2[rearrange_list]\n",
    "\n",
    "            df_team2_past = df_team2.loc[df_team2[df_team2.fbref_season == season].iloc[0].name:df_team2[df_team2.fbref_match_id == game].index.values[0],:]  # df with all predecessing games in this season\n",
    "\n",
    "            # create zeros np array to store date\n",
    "            np_team1 = np.zeros([len(games), df_team1_past.loc[:,\"xg\":].shape[1]])\n",
    "            np_team2 = np.zeros([len(games), df_team2_past.loc[:,\"xg\":].shape[1]])\n",
    "\n",
    "            # insert data into dataframe (back to front) such that all input into the lstm has the same length (padding)\n",
    "            np_team1[- len(df_team1_past):, :-8] = df_team1_past.loc[:,\"xg\": \"attendance\"]\n",
    "            np_team2[- len(df_team2_past):, :-8] = df_team2_past.loc[:,\"xg\": \"attendance\"]\n",
    "\n",
    "             # tensor for pytorch lstm \n",
    "            #np_team1 = torch.from_numpy(np_team1)\n",
    "            #np_team2 = torch.from_numpy(np_team2)\n",
    "\n",
    "            # if last game, no do not add as no future results to predict\n",
    "            if df_team1.iloc[-1].name == df_team1_past.iloc[-1].name:\n",
    "                pass\n",
    "\n",
    "            elif df_team2.iloc[-1].name == df_team2_past.iloc[-1].name:\n",
    "                pass\n",
    "\n",
    "            # if not last game, add to input dict together with result\n",
    "            else:\n",
    "                np_team1[- len(df_team1_past):, -8:] = df_team1.loc[df_team1[df_team1.fbref_season == season].iloc[1].name:df_team1.loc[df_team1[df_team1.fbref_match_id == game].index.values[0]:,:].iloc[1].name,\"schedule_time\":]  # df with all predecessing games in this season\n",
    "                np_team2[- len(df_team2_past):, -8:] = df_team2.loc[df_team2[df_team2.fbref_season == season].iloc[1].name:df_team2.loc[df_team2[df_team2.fbref_match_id == game].index.values[0]:,:].iloc[1].name,\"schedule_time\":]  # df with all predecessing games in this season\n",
    "\n",
    "                result = df_team1.loc[df_team1[df_team1.fbref_match_id == game].index.values[0]:,:].iloc[1].result\n",
    "                fut_feature1 = df_team1.loc[df_team1[df_team1.fbref_match_id == game].index.values[0]:,:].iloc[1,2:].values\n",
    "                fut_feature2 = df_team2.loc[df_team2[df_team2.fbref_match_id == game].index.values[0]:,:].iloc[1,2:].values\n",
    "\n",
    "                lstm_inputs[0].append(np_team1)\n",
    "                lstm_inputs[1].append(np_team2)\n",
    "                lstm_inputs[2].append(result)\n",
    "                lstm_inputs[3].append(fut_feature1)\n",
    "                lstm_inputs[4].append(fut_feature2)\n",
    "    return lstm_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e4efe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_inputs = inputs(game_train, clubs)\n",
    "valid_inputs = inputs(game_valid, clubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_to_lstm():\n",
    "\n",
    "    def __init__(self, mylist):\n",
    "        self.output = []\n",
    "        self.team_home = mylist[0]\n",
    "        self.team_away = mylist[1]\n",
    "        self.result = mylist[2]\n",
    "        self.team_home_next = mylist[3]\n",
    "        self.team_away_next = mylist[4]\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.team_home)):\n",
    "            self.output.append(((self.team_home[i], self.team_away[i]),self.result[i], (self.team_home_next[i], self.team_away_next[i])))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.output)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.output[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29616f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_for_loader = data_to_lstm(train_inputs)\n",
    "valid_for_loader = data_to_lstm(valid_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a22890",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_for_loader, batch_size = 32, drop_last = True)\n",
    "test_loader = torch.utils.data.DataLoader(valid_for_loader, batch_size = 32, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902027c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_2LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_2LSTM, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        \n",
    "        # two separate lstms to account for every teams history\n",
    "        self.l_lstm1 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "        \n",
    "        self.l_lstm2 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(4 * self.n_hidden, 4 * self.n_hidden)\n",
    "        self.l_linear2 = torch.nn.Linear(4 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear3 = torch.nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "        self.l_linear4 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        \n",
    "        \n",
    "        self.soft = torch.nn.Softmax()\n",
    "        self.relu = torch.nn.SiLU()\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        y = y.to(torch.float32)\n",
    "        y = torch.nan_to_num(y, nan = 0.0)    \n",
    "        \n",
    "        \n",
    "        # initialize values for two lstms\n",
    "        #h01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #h02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        #c01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #c02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        \n",
    "        \n",
    "        # run data through lstm and yield output\n",
    "        lstm_out1,(ht1, ct1) = self.l_lstm1(x)#,(h01, c01))\n",
    "        lstm_out2,(ht2, ct2) = self.l_lstm2(x)#,(h02, c02))\n",
    "        ht1 = ht1.squeeze()#[-1,:]\n",
    "        ht2 = ht2.squeeze()#[-1,:]\n",
    "        ct1 = ct1.squeeze()#[-1,:]\n",
    "        ct2 = ct2.squeeze()#[-1,:]\n",
    "        \n",
    "        #print(\"ht1\", ht1.shape)\n",
    "        #print(\"ht2\", ht2.shape)\n",
    "        #print(\"ct1\", ct1.shape)\n",
    "        #print(\"ct2\", ct2.shape)\n",
    "        #print(\"lstm_out1\",lstm_out1.shape)\n",
    "        #print(\"lstm_out2\",lstm_out2.shape)\n",
    "        \n",
    "        x = lstm_out1.contiguous().view(-1, self.n_hidden)\n",
    "        y = lstm_out2.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        #print(\"x\", x.shape)\n",
    "        #print(\"y\", y.shape)\n",
    "        \n",
    "        # run lstm output through nn to predict outcome\n",
    "        #in_lay1 = torch.cat((x, y), 1)\n",
    "        in_lay1 = torch.cat((ht1, ct1, ht2, ct2), -1)\n",
    "        #print(\"in_lay1\", in_lay1.shape)\n",
    "        out_lay1 = self.l_linear1(in_lay1)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay2 = self.relu(out_lay1)\n",
    "        #print(\"in_lay2\", in_lay2.shape)\n",
    "        out_lay2 = self.l_linear2(in_lay2)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay3 = self.relu(out_lay2)\n",
    "        out_lay3 = self.l_linear3(in_lay3)\n",
    "        #print(\"in_lay3\", in_lay3.shape)\n",
    "        \n",
    "        in_lay4 = self.relu(out_lay3)\n",
    "        out = self.soft(self.l_linear4(in_lay4))\n",
    "        #print(\"out\", out.shape)\n",
    "        \n",
    "        #print(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7514f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_sport_pred_2LSTM(lstm, train_loader, test_loader, learn_rate, epochs):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(lstm.parameters(), lr = learn_rate, weight_decay = 0.2)\n",
    "    \n",
    "    lstm.eval()\n",
    "    accur = 0\n",
    "    for step, ((input1, input2), result, (next_feat1, next_feat2)) in enumerate(test_loader):\n",
    "        pred = lstm(input1, input2)\n",
    "        #print(pred.shape)\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "        #print(pred.shape)\n",
    "        #result = torch.nn.functional.one_hot(result.to(torch.int64), num_classes = 3)\n",
    "        accur += pred.eq(result).sum().item()\n",
    "    print(f\"Loss accuracy training: {100 * accur /((step + 1) * 32)}%\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses_val = []\n",
    "        accuracies = []\n",
    "        \n",
    "        lstm.train()\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        for step, ((input1, input2), result, (next_feat1, next_feat2)) in enumerate(train_loader):\n",
    "            lstm.zero_grad()\n",
    "            pred = lstm(input1, input2)\n",
    "            result = torch.nn.functional.one_hot(result.to(torch.int64), num_classes = 3).to(torch.float32)\n",
    "            #print(f\"pred {pred.dtype}, result {result.dtype}\")\n",
    "            loss = criterion(pred, result)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        lstm.eval()\n",
    "        loss = 0\n",
    "        accur = 0\n",
    "        for step, ((input1, input2), result, (next_feat1, next_feat2)) in enumerate(test_loader):\n",
    "            pred = lstm(input1, input2)\n",
    "            result_oh = result.to(torch.int64)\n",
    "            #print(result.shape)\n",
    "            result_oh = torch.nn.functional.one_hot(result_oh, num_classes = 3)\n",
    "            #print(result.shape)\n",
    "            #print(pred.shape)\n",
    "            loss += criterion(pred, result_oh).item()\n",
    "            pred = torch.argmax(pred, dim = 1)\n",
    "            #print(pred.shape)\n",
    "            #print(pred.eq(result).sum())\n",
    "            accur += pred.eq(result).sum().item()\n",
    "        losses_val.append(loss)\n",
    "        accuracy = 100 * (accur /((step + 1) * 32))\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Loss {loss} accuracy {accuracy}%\") \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d598ce56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm1 = Sport_pred_2LSTM(150, 150, 3)\n",
    "train_sport_pred_2LSTM(lstm1, train_loader, test_loader, 5e-5, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_2LSTM_var1(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_2LSTM_var1, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        \n",
    "        # two separate lstms to account for every teams history\n",
    "        self.l_lstm1 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "        \n",
    "        self.l_lstm2 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear2 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear3 = torch.nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "        self.l_linear4 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        \n",
    "        \n",
    "        self.soft = torch.nn.Softmax()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        y = y.to(torch.float32)\n",
    "        y = torch.nan_to_num(y, nan = 0.0)    \n",
    "        \n",
    "        \n",
    "        # initialize values for two lstms\n",
    "        #h01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #h02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        #c01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #c02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        \n",
    "        \n",
    "        # run data through lstm and yield output\n",
    "        lstm_out1,(ht1, ct1) = self.l_lstm1(x)#,(h01, c01))\n",
    "        lstm_out2,(ht2, ct2) = self.l_lstm2(x)#,(h02, c02))\n",
    "        #ht1 = ht1.squeeze()#[-1,:]\n",
    "        #ht2 = ht2.squeeze()#[-1,:]\n",
    "        ct1 = ct1.squeeze()#[-1,:]\n",
    "        ct2 = ct2.squeeze()#[-1,:]\n",
    "        \n",
    "        #print(\"ht1\", ht1.shape)\n",
    "        #print(\"ht2\", ht2.shape)\n",
    "        #print(\"ct1\", ct1.shape)\n",
    "        #print(\"ct2\", ct2.shape)\n",
    "        #print(\"lstm_out1\",lstm_out1.shape)\n",
    "        #print(\"lstm_out2\",lstm_out2.shape)\n",
    "        \n",
    "        x = lstm_out1.contiguous().view(-1, self.n_hidden)\n",
    "        y = lstm_out2.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        #print(\"x\", x.shape)\n",
    "        #print(\"y\", y.shape)\n",
    "        \n",
    "        # run lstm output through nn to predict outcome\n",
    "        #in_lay1 = torch.cat((x, y), 1)\n",
    "        in_lay1 = torch.cat((ct1, ct2), -1)\n",
    "        #print(\"in_lay1\", in_lay1.shape)\n",
    "        out_lay1 = self.l_linear1(in_lay1)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay2 = self.relu(out_lay1)\n",
    "        #print(\"in_lay2\", in_lay2.shape)\n",
    "        out_lay2 = self.l_linear2(in_lay2)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay3 = self.relu(out_lay2)\n",
    "        out_lay3 = self.l_linear3(in_lay3)\n",
    "        #print(\"in_lay3\", in_lay3.shape)\n",
    "        \n",
    "        in_lay4 = self.relu(out_lay3)\n",
    "        out = self.soft(self.l_linear4(in_lay4))\n",
    "        #print(\"out\", out.shape)\n",
    "        \n",
    "        #print(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183aa7fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm2 = Sport_pred_2LSTM_var1(150, 150, 3)\n",
    "train_sport_pred_2LSTM(lstm2, train_loader, test_loader, 1e-4, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94439e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_2LSTM_var2(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_2LSTM_var2, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        \n",
    "        # two separate lstms to account for every teams history\n",
    "        self.l_lstm1 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "        \n",
    "        self.l_lstm2 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear2 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear3 = torch.nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "        self.l_linear4 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        \n",
    "        \n",
    "        self.soft = torch.nn.Softmax()\n",
    "        self.relu = torch.nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        y = y.to(torch.float32)\n",
    "        y = torch.nan_to_num(y, nan = 0.0)    \n",
    "        \n",
    "        \n",
    "        # initialize values for two lstms\n",
    "        #h01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #h02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        #c01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #c02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        \n",
    "        \n",
    "        # run data through lstm and yield output\n",
    "        lstm_out1,(ht1, ct1) = self.l_lstm1(x)#,(h01, c01))\n",
    "        lstm_out2,(ht2, ct2) = self.l_lstm2(x)#,(h02, c02))\n",
    "        #ht1 = ht1.squeeze()#[-1,:]\n",
    "        #ht2 = ht2.squeeze()#[-1,:]\n",
    "        ct1 = ct1.squeeze()#[-1,:]\n",
    "        ct2 = ct2.squeeze()#[-1,:]\n",
    "        \n",
    "        #print(\"ht1\", ht1.shape)\n",
    "        #print(\"ht2\", ht2.shape)\n",
    "        #print(\"ct1\", ct1.shape)\n",
    "        #print(\"ct2\", ct2.shape)\n",
    "        #print(\"lstm_out1\",lstm_out1.shape)\n",
    "        #print(\"lstm_out2\",lstm_out2.shape)\n",
    "        \n",
    "        x = lstm_out1.contiguous().view(-1, self.n_hidden)\n",
    "        y = lstm_out2.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        #print(\"x\", x.shape)\n",
    "        #print(\"y\", y.shape)\n",
    "        \n",
    "        # run lstm output through nn to predict outcome\n",
    "        #in_lay1 = torch.cat((x, y), 1)\n",
    "        in_lay1 = torch.cat((ct1, ct2), -1)\n",
    "        #print(\"in_lay1\", in_lay1.shape)\n",
    "        out_lay1 = self.l_linear1(in_lay1)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay2 = self.relu(out_lay1)\n",
    "        #print(\"in_lay2\", in_lay2.shape)\n",
    "        out_lay2 = self.l_linear2(in_lay2)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay3 = self.relu(out_lay2)\n",
    "        out_lay3 = self.l_linear3(in_lay3)\n",
    "        #print(\"in_lay3\", in_lay3.shape)\n",
    "        \n",
    "        in_lay4 = self.relu(out_lay3)\n",
    "        out = self.soft(self.l_linear4(in_lay4))\n",
    "        #print(\"out\", out.shape)\n",
    "        \n",
    "        #print(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm3 = Sport_pred_2LSTM_var2(150, 150, 3)\n",
    "train_sport_pred_2LSTM(lstm3, train_loader, test_loader, 1e-4, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d106ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_2LSTM_var3(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_2LSTM_var3, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "        \n",
    "        # two separate lstms to account for every teams history\n",
    "        self.l_lstm1 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "        \n",
    "        self.l_lstm2 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear2 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear3 = torch.nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "        self.l_linear4 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        \n",
    "        \n",
    "        self.soft = torch.nn.Softmax()\n",
    "        self.relu = torch.nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        y = y.to(torch.float32)\n",
    "        y = torch.nan_to_num(y, nan = 0.0)    \n",
    "        \n",
    "        \n",
    "        # initialize values for two lstms\n",
    "        #h01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #h02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        #c01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #c02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        \n",
    "        \n",
    "        # run data through lstm and yield output\n",
    "        lstm_out1,(ht1, ct1) = self.l_lstm1(x)#,(h01, c01))\n",
    "        lstm_out2,(ht2, ct2) = self.l_lstm2(x)#,(h02, c02))\n",
    "        #ht1 = ht1.squeeze()#[-1,:]\n",
    "        #ht2 = ht2.squeeze()#[-1,:]\n",
    "        ct1 = ct1.squeeze()#[-1,:]\n",
    "        ct2 = ct2.squeeze()#[-1,:]\n",
    "        \n",
    "        #print(\"ht1\", ht1.shape)\n",
    "        #print(\"ht2\", ht2.shape)\n",
    "        #print(\"ct1\", ct1.shape)\n",
    "        #print(\"ct2\", ct2.shape)\n",
    "        #print(\"lstm_out1\",lstm_out1.shape)\n",
    "        #print(\"lstm_out2\",lstm_out2.shape)\n",
    "        \n",
    "        x = lstm_out1[:,-1,:]\n",
    "        y = lstm_out2[:,-1,:]\n",
    "        \n",
    "        #print(\"x\", x.shape)\n",
    "        #print(\"y\", y.shape)\n",
    "        \n",
    "        # run lstm output through nn to predict outcome\n",
    "        in_lay1 = torch.cat((x, y), 1)\n",
    "        #in_lay1 = torch.cat((ct1, ct2), -1)\n",
    "        #print(\"in_lay1\", in_lay1.shape)\n",
    "        out_lay1 = self.l_linear1(in_lay1)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay2 = self.relu(out_lay1)\n",
    "        #print(\"in_lay2\", in_lay2.shape)\n",
    "        out_lay2 = self.l_linear2(in_lay2)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay3 = self.relu(out_lay2)\n",
    "        out_lay3 = self.l_linear3(in_lay3)\n",
    "        #print(\"in_lay3\", in_lay3.shape)\n",
    "        \n",
    "        in_lay4 = self.relu(out_lay3)\n",
    "        out = self.soft(self.l_linear4(in_lay4))\n",
    "        #print(\"out\", out.shape)\n",
    "        \n",
    "        #print(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57590bce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm4 = Sport_pred_2LSTM_var3(150, 150, 3)\n",
    "train_sport_pred_2LSTM(lstm4, train_loader, test_loader, 1e-3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b46f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_2LSTM_var4(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_2LSTM_var4, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 2 # number of LSTM layers (stacked)\n",
    "        \n",
    "        # two separate lstms to account for every teams history\n",
    "        self.l_lstm1 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "        \n",
    "        self.l_lstm2 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear2 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear3 = torch.nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "        self.l_linear4 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        \n",
    "        \n",
    "        self.soft = torch.nn.Softmax()\n",
    "        self.relu = torch.nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        y = y.to(torch.float32)\n",
    "        y = torch.nan_to_num(y, nan = 0.0)    \n",
    "        \n",
    "        \n",
    "        # initialize values for two lstms\n",
    "        #h01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #h02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        #c01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #c02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        \n",
    "        \n",
    "        # run data through lstm and yield output\n",
    "        lstm_out1,(ht1, ct1) = self.l_lstm1(x)#,(h01, c01))\n",
    "        lstm_out2,(ht2, ct2) = self.l_lstm2(x)#,(h02, c02))\n",
    "        #ht1 = ht1.squeeze()#[-1,:]\n",
    "        #ht2 = ht2.squeeze()#[-1,:]\n",
    "        ct1 = ct1.squeeze()#[-1,:]\n",
    "        ct2 = ct2.squeeze()#[-1,:]\n",
    "        \n",
    "        #print(\"ht1\", ht1.shape)\n",
    "        #print(\"ht2\", ht2.shape)\n",
    "        #print(\"ct1\", ct1.shape)\n",
    "        #print(\"ct2\", ct2.shape)\n",
    "        #print(\"lstm_out1\",lstm_out1.shape)\n",
    "        #print(\"lstm_out2\",lstm_out2.shape)\n",
    "        \n",
    "        x = lstm_out1[:,-1,:]\n",
    "        y = lstm_out2[:,-1,:]\n",
    "        \n",
    "        #print(\"x\", x.shape)\n",
    "        #print(\"y\", y.shape)\n",
    "        \n",
    "        # run lstm output through nn to predict outcome\n",
    "        in_lay1 = torch.cat((x, y), 1)\n",
    "        #in_lay1 = torch.cat((ct1, ct2), -1)\n",
    "        #print(\"in_lay1\", in_lay1.shape)\n",
    "        out_lay1 = self.l_linear1(in_lay1)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay2 = self.relu(out_lay1)\n",
    "        #print(\"in_lay2\", in_lay2.shape)\n",
    "        out_lay2 = self.l_linear2(in_lay2)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay3 = self.relu(out_lay2)\n",
    "        out_lay3 = self.l_linear3(in_lay3)\n",
    "        #print(\"in_lay3\", in_lay3.shape)\n",
    "        \n",
    "        in_lay4 = self.relu(out_lay3)\n",
    "        out = self.soft(self.l_linear4(in_lay4))\n",
    "        #print(\"out\", out.shape)\n",
    "        \n",
    "        #print(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12156f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm5 = Sport_pred_2LSTM_var4(150, 300, 3)\n",
    "train_sport_pred_2LSTM(lstm5, train_loader, test_loader, 1e-3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_2LSTM_var4(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_2LSTM_var4, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 2 # number of LSTM layers (stacked)\n",
    "        \n",
    "        # two separate lstms to account for every teams history\n",
    "        self.l_lstm1 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "        \n",
    "        self.l_lstm2 = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers,\n",
    "                             batch_first = True)\n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear2 = torch.nn.Linear(2 * self.n_hidden, 2 * self.n_hidden)\n",
    "        self.l_linear3 = torch.nn.Linear(2 * self.n_hidden, self.n_hidden)\n",
    "        self.l_linear4 = torch.nn.Linear(self.n_hidden, self.num_classes)\n",
    "        \n",
    "        \n",
    "        self.soft = torch.nn.Softmax()\n",
    "        self.relu = torch.nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        y = y.to(torch.float32)\n",
    "        y = torch.nan_to_num(y, nan = 0.0)    \n",
    "        \n",
    "        \n",
    "        # initialize values for two lstms\n",
    "        #h01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #h02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        #c01 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(x.dtype)\n",
    "        #c02 = torch.zeros(self.n_layers*1, 32, self.n_hidden).to(y.dtype)\n",
    "        \n",
    "        \n",
    "        # run data through lstm and yield output\n",
    "        lstm_out1,(ht1, ct1) = self.l_lstm1(x)#,(h01, c01))\n",
    "        lstm_out2,(ht2, ct2) = self.l_lstm2(x)#,(h02, c02))\n",
    "        #ht1 = ht1.squeeze()#[-1,:]\n",
    "        #ht2 = ht2.squeeze()#[-1,:]\n",
    "        ct1 = ct1.squeeze()#[-1,:]\n",
    "        ct2 = ct2.squeeze()#[-1,:]\n",
    "        \n",
    "        #print(\"ht1\", ht1.shape)\n",
    "        #print(\"ht2\", ht2.shape)\n",
    "        #print(\"ct1\", ct1.shape)\n",
    "        #print(\"ct2\", ct2.shape)\n",
    "        #print(\"lstm_out1\",lstm_out1.shape)\n",
    "        #print(\"lstm_out2\",lstm_out2.shape)\n",
    "        \n",
    "        x = lstm_out1[:,-1,:]\n",
    "        y = lstm_out2[:,-1,:]\n",
    "        \n",
    "        #print(\"x\", x.shape)\n",
    "        #print(\"y\", y.shape)\n",
    "        \n",
    "        # run lstm output through nn to predict outcome\n",
    "        in_lay1 = torch.cat((x, y), 1)\n",
    "        #in_lay1 = torch.cat((ct1, ct2), -1)\n",
    "        #print(\"in_lay1\", in_lay1.shape)\n",
    "        out_lay1 = self.l_linear1(in_lay1)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay2 = self.relu(out_lay1)\n",
    "        #print(\"in_lay2\", in_lay2.shape)\n",
    "        out_lay2 = self.l_linear2(in_lay2)\n",
    "        #print(\"out_lay1\", out_lay1.shape)\n",
    "        \n",
    "        in_lay3 = self.relu(out_lay2)\n",
    "        out_lay3 = self.l_linear3(in_lay3)\n",
    "        #print(\"in_lay3\", in_lay3.shape)\n",
    "        \n",
    "        in_lay4 = self.relu(out_lay3)\n",
    "        out = self.soft(self.l_linear4(in_lay4))\n",
    "        #print(\"out\", out.shape)\n",
    "        \n",
    "        #print(out)\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_lstm1 = torch.nn.LSTM(input_size = 150, \n",
    "                             hidden_size = 150,\n",
    "                             num_layers = 1,\n",
    "                             batch_first = True)\n",
    "\n",
    "l_lstm2 = torch.nn.LSTM(input_size = 150, \n",
    "                             hidden_size = 150,\n",
    "                             num_layers = 1,\n",
    "                             batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57ca64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def train_sport_pred_2LSTM(lstm, train_loader, test_loader, learn_rate, epochs):\n",
    "criterion1 = torch.nn.MSELoss()\n",
    "optimizer1 = torch.optim.SGD(lstm.parameters(), lr = 1e-3)\n",
    "\n",
    "criterion2 = torch.nn.MSELoss()\n",
    "optimizer2 = torch.optim.SGD(lstm.parameters(), lr = 1e-3)\n",
    "    \n",
    "l_lstm1.eval()\n",
    "accur = 0\n",
    "#for step, ((input1, input2), result, (next_feat1, next_feat2)) in enumerate(test_loader):\n",
    " #   pred1 = l_lstm1(input1)\n",
    "  #  pred2 = l_lstm2(input2)\n",
    "        #print(pred.shape)\n",
    "    #pred = torch.argmax(pred, dim = 1)\n",
    "        #print(pred.shape)\n",
    "        #result = torch.nn.functional.one_hot(result.to(torch.int64), num_classes = 3)\n",
    "   # accur += pred.eq(next_feat1).sum().item()\n",
    "#print(f\"Loss accuracy training: {100 * accur /((step + 1) * 32)}%\")\n",
    "    \n",
    "for epoch in range(15):\n",
    "    losses_val1 = []\n",
    "    losses_val2 = []\n",
    "    accuracies = []\n",
    "        \n",
    "    l_lstm1.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    for step, ((input1, input2), result, (next_feat1, next_feat2)) in enumerate(train_loader):\n",
    "        lstm.zero_grad()\n",
    "        pred1, (_,_) = l_lstm1(input1.float())\n",
    "        pred1 = pred1[:,-1,:]\n",
    "        pred2, (_,_) = l_lstm2(input2.float())\n",
    "        pred2 = pred2[:,-1,:]\n",
    "        #result = torch.nn.functional.one_hot(result.to(torch.int64), num_classes = 3).to(torch.float32)\n",
    "            #print(f\"pred {pred.dtype}, result {result.dtype}\")\n",
    "        #print(f\"pred1 {type(pred1)}, next feature {type(next_feat1)}\")\n",
    "        loss1 = criterion1(pred1, next_feat1.float())\n",
    "        loss2 = criterion2(pred2, next_feat2.float())\n",
    "        loss1.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        loss2.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "    l_lstm1.eval()\n",
    "    loss1 = 0\n",
    "    loss2 = 0\n",
    "    accur = 0\n",
    "    for step, ((input1, input2), result, (next_feat1, next_feat2)) in enumerate(test_loader):\n",
    "        pred1, (_,_) = l_lstm1(input1.float())\n",
    "        pred1 = pred1[:,-1,:]\n",
    "        \n",
    "        pred2, (_,_) = l_lstm2(input2.float())\n",
    "        pred2 = pred2[:,-1,:]\n",
    "        #result_oh = result.to(torch.int64)\n",
    "            #print(result.shape)\n",
    "        #result_oh = torch.nn.functional.one_hot(result_oh, num_classes = 3)\n",
    "            #print(result.shape)\n",
    "            #print(pred.shape)\n",
    "        loss1 += criterion1(pred1, next_feat1.float()).item()\n",
    "        loss2 += criterion2(pred2, next_feat2.float()).item()\n",
    "        #pred = torch.argmax(pred, dim = 1)\n",
    "            #print(pred.shape)\n",
    "            #print(pred.eq(result).sum())\n",
    "        #accur += pred.eq(next_feat1).sum().item()\n",
    "    losses_val1.append(loss1)\n",
    "    losses_val2.append(loss2)\n",
    "    accuracy = 100 * (accur /((step + 1) * 32))\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Loss1 {loss1} Loss2 {loss2}%\") \n",
    "                \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f26f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "for club in clubs.keys():\n",
    "    a = clubs[club]\n",
    "    #print(type(a))\n",
    "    for _, games in a.iterrows():\n",
    "        res_pos_neg = (int(games.possession_general_poss), int(games.fbref_own), int(games.fbref_oppon))\n",
    "        contexts.append(res_pos_neg)\n",
    "#        if int(games.result) == new_data.return_dicts(\"result\")[\"W\"]:\n",
    " #           res_pos_neg = (int(games.result), int(games.fbref_own), int(games.fbref_oppon))\n",
    "  #          contexts.append(res_pos_neg)\n",
    "   #     elif int(games.result) == new_data.return_dicts(\"result\")[\"L\"]:\n",
    "    #        res_pos_neg = (int(games.result), int(games.fbref_oppon), int(games.fbref_own))\n",
    "     #       contexts.append(res_pos_neg)\n",
    "      #  else:\n",
    "       #     res_pos_neg = (int(games.result), int(games.fbref_own), int(games.fbref_oppon))\n",
    "        #    contexts.append(res_pos_neg)\n",
    "         #   res_pos_neg = (int(games.result), int(games.fbref_oppon), int(games.fbref_own))\n",
    "          #  contexts.append(res_pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(new_data_test.formation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51284281",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_test.data_frame.misc_performance_crdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_for_embed():\n",
    "\n",
    "    def __init__(self, mylist):\n",
    "        self.output = mylist\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.output)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.output[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933036d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dataset = data_for_embed(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64267ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_data(pos_neg):\n",
    "    result = torch.tensor([pos_neg[lab][0] for lab in range(len(pos_neg))], dtype=torch.int64)  \n",
    "    context_positive = torch.tensor([pos_neg[lab][1] for lab in range(len(pos_neg))], dtype=torch.int64) \n",
    "    context_negative = torch.tensor([pos_neg[lab][2] for lab in range(len(pos_neg))], dtype=torch.int64) \n",
    "    return (result, context_positive, context_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fast_word2vec(torch.nn.Module):\n",
    "    def __init__(self, size, dims):\n",
    "        super(fast_word2vec, self).__init__()\n",
    "        self.vocab_size = size\n",
    "        self.embed_dim = dims\n",
    "        \n",
    "        \n",
    "        self.cent_embed = torch.nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.cont_embed = torch.nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        \n",
    "        # Initialization\n",
    "        self.cent_embed.weight.data.uniform_(-1, 1)\n",
    "        self.cont_embed.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward(self, sample):\n",
    "        cent_word = sample[0]\n",
    "        pos_con = sample[1]\n",
    "        neg_con = sample[2]\n",
    "        \n",
    "        emb_cent = self.cent_embed(cent_word)\n",
    "        emb_cont_pos = self.cont_embed(pos_con)\n",
    "        emb_cont_neg = self.cont_embed(neg_con)\n",
    "\n",
    "        pos = torch.clamp(torch.sum(emb_cent*emb_cont_pos, 1),max = 10, min = -10)\n",
    "        neg = torch.clamp(torch.sum(emb_cent*emb_cont_neg, 1),max = 10, min = -10)\n",
    "        \n",
    "        m = torch.nn.Sigmoid()\n",
    "        pos_score = torch.mean(m(pos), 0, True)\n",
    "        neg_score = torch.mean(m(-neg), 0, True)\n",
    "        \n",
    "        #print(pos_score)\n",
    "        #print(neg_score)\n",
    "        return torch.neg(torch.cat((pos_score, neg_score), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac04563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting for training plus dataloader object\n",
    "max_epochs = 300\n",
    "lrn_rate = 0.05\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "club_len = len(set(list(new_data1.fbref_home_id) + list(new_data1.fbref_away_id)))\n",
    "emb_dim = min(50, (club_len+1)/2)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "net = fast_word2vec(club_len, emb_dim).to(device) \n",
    "\n",
    "batch_size = 100\n",
    "train_ldr = torch.utils.data.DataLoader(context_dataset, batch_size = batch_size, shuffle = True, collate_fn = collate_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec6bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training of the network\n",
    "def train(net, ldr, bs, me, lr, loss_fn):\n",
    "  # network, loader, batch_size, max_epochs, lerning_rate\n",
    "    running_loss = []\n",
    "    \n",
    "    #loss_fn = nn.NLLLoss()\n",
    "    \n",
    "    target = torch.tensor([1, 0])\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    loss = net  \n",
    "    print(\"\\nStarting training\")\n",
    "    for itera in range(me):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for bix, cont in enumerate(ldr):\n",
    "            opt.zero_grad()\n",
    "            output = net.forward(cont)  \n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "            opt.step()\n",
    "            \n",
    "        if itera % 2 == 0:\n",
    "            print(\"epoch = %4d   loss = %0.8f\" % (itera, epoch_loss))\n",
    "        running_loss.append(epoch_loss)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b86756",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nbatch_size = %3d \" % batch_size)\n",
    "print(\"max epochs = \" + str(max_epochs))\n",
    "print(\"loss = Sigmoid\")\n",
    "print(\"optimizer = Adam\")\n",
    "print(\"lrn_rate = %0.3f \" % lrn_rate)\n",
    "train(net, train_ldr, batch_size, 50, lrn_rate, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import database_server.db_utilities as dbu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e1d1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = dbu.select_query(query_str= \"SELECT name, fbref_id FROM teams;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70048d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f10594",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"own_id\"] = df.fbref_id.map(new_data.return_dicts(\"teams\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.name == 'Bayern Munich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23069c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a679aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, WT = net.parameters()\n",
    "W = W.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e3754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_complementary_items(word, wordvecs, metric = \"cosine\", k = 5):\n",
    "    # word or embedding, parameters form training, metric to find, num of comp items to find\n",
    "    if type(word) == str:\n",
    "        dist_matrix = distance.squareform(distance.pdist(wordvecs, metric))\n",
    "        #index = vocab_trch[word]\n",
    "        dists = dist_matrix[index]\n",
    "        \n",
    "    elif type(word) == np.ndarray:\n",
    "        word = np.reshape(word, (1, 300), order = \"F\")\n",
    "        wordvecs = np.concatenate((wordvecs,word), axis = 0)\n",
    "        dist_matrix = distance.squareform(distance.pdist(wordvecs, metric))\n",
    "        index = len(wordvecs)\n",
    "        dists = dist_matrix[-1]\n",
    "    ind = np.argpartition(dists, k)[:k+1]\n",
    "    ind = ind[np.argsort(dists[ind])][1:]\n",
    "    out = [(i, translator[vocab_i2t[i]], dists[i]) for i in ind]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4002c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = distance.squareform(distance.pdist(W, \"cosine\"))\n",
    "dists = dist_matrix[73]\n",
    "\n",
    "ind = np.argpartition(dists, 5)[:5+1]\n",
    "ind = ind[np.argsort(dists[ind])][1:]\n",
    "out = [(i, df[df.own_id == i], dists[i]) for i in ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce5b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0619c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraping.MarketValues import MarketValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a7b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f21913",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_club = [x.lower().replace(\" \",\"\") for x in list(df.name)]\n",
    "uniq_club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bl = MarketValues(\"bundesliga\", 6)\n",
    "#pl = MarketValues(\"premier league\", 6)\n",
    "#l1 = MarketValues(\"ligue 1\", 6)\n",
    "ll = MarketValues(\"la liga\", 6)\n",
    "sa = MarketValues(\"serie a\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528def68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bl = bl.get_club_overview()\n",
    "df_pl = pl.get_club_overview()\n",
    "df_l1 = l1.get_club_overview()\n",
    "df_ll = ll.get_club_overview()\n",
    "df_sa = sa.get_club_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59927f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_club = [x.lower().replace(\" \",\"\") for x in list(df.name.unique())]\n",
    "uniq_club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17721e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_club_list = list(df_bl.club.unique()) + list(df_pl.club.unique()) + list(df_l1.club.unique()) + list(df_ll.club.unique()) + list(df_sa.club.unique())\n",
    "unique_club_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def similar(seq1, seq2):\n",
    "    return difflib.SequenceMatcher(a=seq1.lower(), b=seq2.lower()).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fabb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "club_dict = {}\n",
    "for club1 in uniq_club:\n",
    "    previous = 0\n",
    "    for club2 in unique_club_list:\n",
    "        simil = similar(club1,club2)\n",
    "        if simil > previous and club1 in club2:\n",
    "            club_dict[club1] = club2\n",
    "            previous = simil\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "club_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.setdiff1d(uniq_club,list(club_dict.keys())))\n",
    "print(np.setdiff1d(unique_club_list,list(club_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c687f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}       \n",
    "for club3 in np.setdiff1d(uniq_club,list(club_dict.keys())):\n",
    "    previous = 0\n",
    "    for club4 in np.setdiff1d(unique_club_list,list(club_dict.values())):\n",
    "        simil = similar(club3,club4)\n",
    "        if simil > previous:\n",
    "            new_dict[club3] = club4\n",
    "            previous = simil\n",
    "        else:\n",
    "            pass\n",
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict['heidenheim'] = None\n",
    "new_dict['holsteinkiel'] = None\n",
    "new_dict['nice'] = 'ogcnizza'\n",
    "new_dict['napoli'] = \"sscneapel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "club_dict.update(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f749ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name = df.name.apply(lambda x: x.lower().replace(\" \",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd89f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"trans_name\"] = df.name.map(club_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a026a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958cd8fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_dict1 = new_data.return_dicts(\"season\")\n",
    "date_dict1_reverse = {v:k for k,v in date_dict1.items()}\n",
    "\n",
    "date_dict2 = {}\n",
    "i = 0\n",
    "for date in df_bl.season.unique():\n",
    "    date_dict2[date] = date_dict1_reverse[i]\n",
    "    i+= 1\n",
    "date_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee56172",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict1_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa45656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = dbu.select_query(query_str= \"SELECT * FROM matches;\")\n",
    "df_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "for club in clubs.keys():\n",
    "    df = clubs[club]\n",
    "    df.head()\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596bfde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ab2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"market_worth\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409763b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"fbref_id_ds\"] = df.fbref_id.map(new_data.return_dicts(\"teams\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2897db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66ff75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc4262c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e34d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d35ce2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137e282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b868c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09144e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737096e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316e416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8b659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_feat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7437f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_lstm1(input1.float())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ac845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import unique\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "#from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras.layers #--upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9536df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of learned embedding encoding for a neural network\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "def load_dataset(filename):\n",
    "    # load the dataset as a pandas DataFrame\n",
    "    data = read_csv(filename, header=None)\n",
    "    # retrieve numpy array\n",
    "    dataset = data.values\n",
    "    # split into input (X) and output (y) variables\n",
    "    X = dataset[:, :-1]\n",
    "    y = dataset[:,-1]\n",
    "    # format all fields as string\n",
    "    X = X.astype(str)\n",
    "    # reshape target to be a 2d array\n",
    "    y = y.reshape((len(y), 1))\n",
    "    return X, y\n",
    "\n",
    "# prepare input data\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    X_train_enc, X_test_enc = list(), list()\n",
    "    # label encode each column\n",
    "    for i in range(X_train.shape[1]):\n",
    "        le = LabelEncoder()\n",
    "        le.fit(X_train[:, i])\n",
    "        # encode\n",
    "        train_enc = le.transform(X_train[:, i])\n",
    "        test_enc = le.transform(X_test[:, i])\n",
    "        # store\n",
    "        X_train_enc.append(train_enc)\n",
    "        X_test_enc.append(test_enc)\n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "# prepare target\n",
    "def prepare_targets(y_train, y_test):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train_enc = le.transform(y_train)\n",
    "    y_test_enc = le.transform(y_test)\n",
    "    return y_train_enc, y_test_enc\n",
    "\n",
    "# load the dataset\n",
    "X, y = load_dataset('breast-cancer.csv')\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# prepare input data\n",
    "X_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "# make output 3d\n",
    "y_train_enc = y_train_enc.reshape((len(y_train_enc), 1, 1))\n",
    "y_test_enc = y_test_enc.reshape((len(y_test_enc), 1, 1))\n",
    "# prepare each input head\n",
    "in_layers = list()\n",
    "em_layers = list()\n",
    "for i in range(len(X_train_enc)):\n",
    "    # calculate the number of unique inputs\n",
    "    n_labels = len(unique(X_train_enc[i]))\n",
    "    # define input layer\n",
    "    in_layer = Input(shape=(1,))\n",
    "    # define embedding layer\n",
    "    em_layer = Embedding(n_labels, 10)(in_layer)\n",
    "    # store layers\n",
    "    in_layers.append(in_layer)\n",
    "    em_layers.append(em_layer)\n",
    "# concat all embeddings\n",
    "merge = concatenate(em_layers)\n",
    "dense = Dense(10, activation='relu', kernel_initializer='he_normal')(merge)\n",
    "output = Dense(1, activation='sigmoid')(dense)\n",
    "model = Model(inputs=in_layers, outputs=output)\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# plot graph\n",
    "plot_model(model, show_shapes=True, to_file='embeddings.png')\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train_enc, y_train_enc, epochs=20, batch_size=16, verbose=2)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_test_enc, y_test_enc, verbose=0)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = min(50,103)\n",
    "#model = Model()\n",
    "model = keras.models.Sequential()\n",
    "model.add(Embedding(input_dim = 12, output_dim = embedding_size, input_length = 1, name=\"embedding\"))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dense(15, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"custom_metrics\"])\n",
    "model.fit(x = new_data1['referee'].values, y=new_data1['result_home'].values , epochs = 50, batch_size = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff500fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(new_data1['referee'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4156807",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1.referee.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4076a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289dded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a8dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc764f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a460a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce2e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87d0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c0a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69142341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6bb0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.num_classes = num_classes\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 2 # number of LSTM layers (stacked)\n",
    "        \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers)\n",
    "\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden, self.n_hidden)\n",
    "        self.l_linear2 = torch.nn.Linear(self.n_hidden, num_classes)\n",
    "        self.soft = torch.nn.Softmax(dim = 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    #def init_hidden(self, batch_size):\n",
    "     #   hidden_state = torch.randn(self.n_layers,batch_size,self.n_hidden)\n",
    "      #  cell_state = torch.randn(self.n_layers,batch_size,self.n_hidden)\n",
    "       # self.hidden = (hidden_state, cell_state)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        #print(x)\n",
    "        h0 = torch.randn(self.n_layers*1, 38, self.n_hidden).to(x.dtype)\n",
    "        #print(h0.dtype)\n",
    "        c0 = torch.randn(self.n_layers*1, 38, self.n_hidden).to(x.dtype)\n",
    "        #print(c0.dtype)\n",
    "        \n",
    "        lstm_out,(hn, cn) = self.l_lstm(x,(h0, c0))\n",
    "        #print(\"lstm out:\", lstm_out)\n",
    "        #x = lstm_out.contiguous().view(-1, self.n_hidden)\n",
    "        #x = lstm_out[:, -1, :]\n",
    "        print(hn.shape)\n",
    "        x = hn.view(-1, self.n_hidden)\n",
    "        print(x.shape)\n",
    "        x = self.relu(self.l_linear(x))\n",
    "        print(x.shape)\n",
    "        out = self.soft(self.l_linear2(x))\n",
    "        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091250f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee09475",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, (s,p) in enumerate(train_loader):\n",
    "    print(s.shape)\n",
    "    print(p.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.num_classes = num_classes\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "        self.n_layers = 2 # number of LSTM layers (stacked)\n",
    "        \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                             hidden_size = self.n_hidden,\n",
    "                             num_layers = self.n_layers)\n",
    "\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden, num_classes)\n",
    "        self.soft = torch.nn.Softmax(dim = 1)\n",
    "        \n",
    "    #def init_hidden(self, batch_size):\n",
    "     #   hidden_state = torch.randn(self.n_layers,batch_size,self.n_hidden)\n",
    "      #  cell_state = torch.randn(self.n_layers,batch_size,self.n_hidden)\n",
    "       # self.hidden = (hidden_state, cell_state)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        #print(x)\n",
    "        h0 = torch.randn(self.n_layers*1, self.n_hidden).to(x.dtype)\n",
    "        #print(h0.dtype)\n",
    "        c0 = torch.randn(self.n_layers*1, self.n_hidden).to(x.dtype)\n",
    "        #print(c0.dtype)\n",
    "        \n",
    "        lstm_out,_ = self.l_lstm(x,(h0, c0))\n",
    "        #print(\"lstm out:\", lstm_out)\n",
    "        x = lstm_out.contiguous().view(-1, self.n_hidden)\n",
    "        #print(x)\n",
    "        out = self.soft(self.l_linear(x))\n",
    "        #print(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5350704",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(X1[0]) # this is number of parallel inputs\n",
    "\n",
    "# create NN\n",
    "mv_net = Sport_pred_LSTM(n_features, 150, 3)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-4, weight_decay = 1e-3)\n",
    "\n",
    "train_episodes = 500\n",
    "batch_size = 30\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f94b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mv_net.train()\n",
    "\n",
    "for t in range(train_episodes):\n",
    "\n",
    "    mv_net.train()\n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "        inpt = data.to(torch.float32)\n",
    "        label = target\n",
    "        #print(data.dtype)\n",
    "\n",
    "        output = mv_net.forward(data) \n",
    "        print(output)\n",
    "        loss = criterion(output, label.to(torch.float32)) \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad() \n",
    "        break\n",
    "    if t%50 == 0:\n",
    "        print('step : ' , t , 'loss : ' , loss.item())\n",
    "        \n",
    "    #mv_net.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2080e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da888cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797ec9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717bd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fbref_match_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sequencizer(X, y, seq_len):\n",
    "    x_t = []\n",
    "    y_t = []\n",
    "    for i in range(len(X)):\n",
    "        if i + seq_len + 1 == len(X):\n",
    "            break\n",
    "        x_train = X[i:i + seq_len]\n",
    "        y_train = y[i + seq_len + 1]\n",
    "        \n",
    "        x_t.append(x_train)\n",
    "        y_t.append(y_train)\n",
    "        \n",
    "    return np.array(x_t), np.array(y_t)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c2017",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281351e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "empty_df = pd.DataFrame(columns = list(new_data.dataset_team(0)), index = [0])\n",
    "\n",
    "for team in set(list(new_data1.fbref_home_id.unique()) + list(new_data1.fbref_away_id.unique())):\n",
    "    empty_df = pd.concat([new_data.dataset_team(team), empty_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14bd65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts2.loc[\"2023-04-25\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c116e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dts1#.loc[\"2022-08-13\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = new_data.data_frame\n",
    "X = []\n",
    "y = []\n",
    "for team in set(list(empty_df.fbref_own.unique()) + list(empty_df.fbref_oppon.unique())):\n",
    "    for seas in empty_df.fbref_season.unique():\n",
    "        dts1 = empty_df[(empty_df.fbref_own == team) & (empty_df.fbref_season == seas)]\n",
    "        for i in range(len(dts1)):\n",
    "            i += 11\n",
    "            date11 = dts1.index[i - 11]\n",
    "            date12 = dts1.index[i - 1]\n",
    "            \n",
    "            dts2 = empty_df[(empty_df.fbref_own == dts1.iloc[i].fbref_oppon) & (empty_df.fbref_season == seas)]\n",
    "            date21 = dts2.index[i - 11]\n",
    "            date22 = dts2.index[i - 1]\n",
    "            \n",
    "            #if dts1.index[i] != dts2.index[i]:\n",
    "             #   print(dts1.iloc[i])\n",
    "              #  print(dts2.iloc[i])\n",
    "            if dts1.iloc[i].fbref_match_id != dts2.iloc[i].fbref_match_id:\n",
    "                print(\"Alarm\")\n",
    "            x = [np.array(dts1.loc[date11:date12]), np.array(dts2.loc[date21:date22])]\n",
    "            X.append(x)\n",
    "            y.append(dts1.iloc[i].result) \n",
    "            break\n",
    "            if i == len(dts1)-1:\n",
    "                break\n",
    "    print(team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2910fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_df[(empty_df.fbref_own == team) & (empty_df.fbref_season == seas)].index[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9afb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [torch.from_numpy(x.astype(float)) for z in X for x in z]\n",
    "y = [torch.from_numpy(Y.astype(float)) for Y in y]\n",
    "y = [torch.nn.functional.one_hot(x.long(), num_classes = 3) for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_to_2lstm():\n",
    "\n",
    "    def __init__(self, mylist1, mylist2):\n",
    "        self.output =[]\n",
    "        \n",
    "        for i in range(len(mylist1)):\n",
    "            self.output.append((mylist1[i][0], mylist1[i][1], mylist2[i]))\n",
    "        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.output1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.output[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2lstm = data_to_2lstm(X,y)\n",
    "dataloader = DataLoader(dt2lstm, batch_size = 30,\n",
    "                        shuffle = False)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2lstm.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74585bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x1, x2, y) in enumerate(dataloader):\n",
    "    print(i, x1, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b895d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X = {}\n",
    "y = []\n",
    "\n",
    "#set(list(df.fbref_away_id.unique()) + list(df.fbref_home_id.unique()))\n",
    "for team in set(list(df.fbref_away_id.unique()) + list(df.fbref_home_id.unique())):\n",
    "    data_team = new_data.dataset_team(team)\n",
    "    for seas in data_team.fbref_season.unique():\n",
    "        dts = data_team[data_team.fbref_season == seas]\n",
    "        for i in range(len(dts)):\n",
    "            string = str(team) + \"_\" + str(seas) + \"_\"\n",
    "            X.append(dts.iloc[i:i + 10])\n",
    "            y.append(dts.iloc[i + 10 + 1].result)\n",
    "            #print(dts.iloc[i:i+10].result)\n",
    "            if i + 10 + 1 == len(dts)-1:\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27269c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b94735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dts.iloc[0:4]\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(dts)):\n",
    "    X.append(dts.iloc[i:i + 10])\n",
    "    y.append(dts.iloc[i + 10 + 1])\n",
    "    #print(dts.iloc[i:i+10].result)\n",
    "    if i + 10 + 1 == len(dts)-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bc7b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "empty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c6c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X[0])\n",
    "#X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_standard_nn(torch.nn.Module):\n",
    "    def __init__(self,n_features, hidden, num_classes):\n",
    "        super(Sport_pred_LSTM, self).__init__()\n",
    "        self.n_features = n_features \n",
    "        self.num_classes = num_classes # number of classes (win, draw, lose)\n",
    "        self.n_hidden = hidden # number of hidden states\n",
    "\n",
    "        # classic neural net to process outcomes\n",
    "        self.l_linear1 = torch.nn.Linear(self.n_features * 2, self.n_hidden * 2)\n",
    "        self.l_linear2 = torch.nn.Linear(self.n_hidden * 2, self.n_hidden * 0.5)\n",
    "        self.l_linear3 = torch.nn.Linear(self.n_hidden * 0.5, num_classes)\n",
    "        \n",
    "        self.soft = torch.nn.Softmax(dim = 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # convert input to fit the model\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.nan_to_num(x, nan = 0.0)\n",
    "        y = y.to(torch.float32)\n",
    "        y = torch.nan_to_num(y, nan = 0.0)    \n",
    "        \n",
    "        # run lstm output through nn to predict outcome\n",
    "        in_lay1 = torch.cat((x, y), 1)\n",
    "        out_lay1 = self.l_linear1(in_lay1)\n",
    "        in_lay2 = self.relu(out_lay1)\n",
    "        out_lay2 = self.l_linear2(in_lay2)\n",
    "        in_lay3 = self.relu(out_lay3)\n",
    "        out = self.soft(self.l_linear3(in_lay3))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dropout: float = 0.1, max_seq_len: int = 5000, d_model: int = 512, batch_first: bool = False):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "                     (Vaswani et al, 2017)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.x_dim = 1 if batch_first else 0\n",
    "\n",
    "        # copy pasted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        \n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or \n",
    "               [enc_seq_len, batch_size, dim_val]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(self.x_dim)]\n",
    "\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd93b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sport_pred_Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, dec_seq_len: int, batch_first: bool, out_seq_len: int = 58, \n",
    "                 dim_val: int = 512, n_encoder_layers: int = 4, n_decoder_layers: int = 4, n_heads: int = 8,\n",
    "                 dropout_encoder: float = 0.2, dropout_decoder: float = 0.2, dropout_pos_enc: float = 0.1,\n",
    "                 dim_feedforward_encoder: int = 2048, dim_feedforward_decoder: int = 2048, \n",
    "                 num_predicted_features: int = 1): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce \n",
    "                     outputs of dimension dim_val\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "            dropout_decoder: float, the dropout rate of the decoder\n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer \n",
    "                                     of the encoder\n",
    "            dim_feedforward_decoder: int, number of neurons in the linear layer \n",
    "                                     of the decoder\n",
    "            num_predicted_features: int, the number of features you want to predict.\n",
    "                                    Most of the time, this will be 1 because we're\n",
    "                                    only forecasting FCR-N prices in DK2, but in\n",
    "                                    we wanted to also predict FCR-D with the same\n",
    "                                    model, num_predicted_features should be 2.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        #print(\"input_size is: {}\".format(input_size))\n",
    "        #print(\"dim_val is: {}\".format(dim_val))\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features = input_size, \n",
    "            out_features = dim_val \n",
    "            )\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features = num_predicted_features,\n",
    "            out_features = dim_val\n",
    "            )  \n",
    "        \n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features = dim_val, \n",
    "            out_features = num_predicted_features\n",
    "            )\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = pe.PositionalEncoder(\n",
    "            d_model = dim_val,\n",
    "            dropout = dropout_pos_enc\n",
    "            )\n",
    "\n",
    "        # The encoder layer used in the paper is identical to the one used by\n",
    "        # Vaswani et al (2017) on which the PyTorch module is based.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = dim_val, \n",
    "            nhead = n_heads,\n",
    "            dim_feedforward = dim_feedforward_encoder,\n",
    "            dropout = dropout_encoder,\n",
    "            batch_first = batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the encoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerEncoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer = encoder_layer,\n",
    "            num_layers = n_encoder_layers, \n",
    "            norm = None\n",
    "            )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model = dim_val,\n",
    "            nhead = n_heads,\n",
    "            dim_feedforward = dim_feedforward_decoder,\n",
    "            dropout = dropout_decoder,\n",
    "            batch_first = batch_first\n",
    "            )\n",
    "\n",
    "        # Stack the decoder layers in nn.TransformerDecoder\n",
    "        # It seems the option of passing a normalization instance is redundant\n",
    "        # in my case, because nn.TransformerDecoderLayer per default normalizes\n",
    "        # after each sub-layer\n",
    "        # (https://github.com/pytorch/pytorch/issues/24930).\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer = decoder_layer,\n",
    "            num_layers = n_decoder_layers, \n",
    "            norm = None\n",
    "            )\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor = None, \n",
    "                tgt_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \n",
    "        Args:\n",
    "            src: the encoder's output sequence. Shape: (S,E) for unbatched input, \n",
    "                 (S, N, E) if batch_first=False or (N, S, E) if \n",
    "                 batch_first=True, where S is the source sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            tgt: the sequence to the decoder. Shape: (T,E) for unbatched input, \n",
    "                 (T, N, E)(T,N,E) if batch_first=False or (N, T, E) if \n",
    "                 batch_first=True, where T is the target sequence length, \n",
    "                 N is the batch size, and E is the number of features (1 if univariate)\n",
    "            src_mask: the mask for the src sequence to prevent the model from \n",
    "                      using data points from the target sequence\n",
    "            tgt_mask: the mask for the tgt sequence to prevent the model from\n",
    "                      using data points from the target sequence\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"From model.forward(): Size of src as given to forward(): {}\".format(src.size()))\n",
    "        #print(\"From model.forward(): tgt size = {}\".format(tgt.size()))\n",
    "\n",
    "        # Pass throguh the input layer right before the encoder\n",
    "        src = self.encoder_input_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after input layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        src = self.positional_encoding_layer(src) # src shape: [batch_size, src length, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of src after pos_enc layer: {}\".format(src.size()))\n",
    "\n",
    "        # Pass through all the stacked encoder layers in the encoder\n",
    "        # Masking is only needed in the encoder if input sequences are padded\n",
    "        # which they are not in this time series use case, because all my\n",
    "        # input sequences are naturally of the same length. \n",
    "        # (https://github.com/huggingface/transformers/issues/4083)\n",
    "        src = self.encoder( # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "            src=src\n",
    "            )\n",
    "        #print(\"From model.forward(): Size of src after encoder: {}\".format(src.size()))\n",
    "\n",
    "        # Pass decoder input through decoder input layer\n",
    "        decoder_output = self.decoder_input_layer(tgt) # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
    "        #print(\"From model.forward(): Size of decoder_output after linear decoder layer: {}\".format(decoder_output.size()))\n",
    "\n",
    "        #if src_mask is not None:\n",
    "            #print(\"From model.forward(): Size of src_mask: {}\".format(src_mask.size()))\n",
    "        #if tgt_mask is not None:\n",
    "            #print(\"From model.forward(): Size of tgt_mask: {}\".format(tgt_mask.size()))\n",
    "\n",
    "        # Pass throguh decoder - output shape: [batch_size, target seq len, dim_val]\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=decoder_output,\n",
    "            memory=src,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "            )\n",
    "\n",
    "        #print(\"From model.forward(): decoder_output shape after decoder: {}\".format(decoder_output.shape))\n",
    "\n",
    "        # Pass through linear mapping\n",
    "        decoder_output = self.linear_mapping(decoder_output) # shape [batch_size, target seq len]\n",
    "        #print(\"From model.forward(): decoder_output size after linear_mapping = {}\".format(decoder_output.size()))\n",
    "\n",
    "        return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model parameters\n",
    "dim_val = 512 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 8 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_decoder_layers = 4 # Number of times the decoder layer is stacked in the decoder\n",
    "n_encoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 1 # The number of input variables. 1 if univariate forecasting.\n",
    "dec_seq_len = 92 # length of input given to decoder. Can have any integer value.\n",
    "enc_seq_len = 153 # length of input given to encoder. Can have any integer value.\n",
    "output_sequence_length = 58 # Length of the target sequence, i.e. how many time steps should your forecast cover\n",
    "max_seq_len = enc_seq_len # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "\n",
    "model = tst.TimeSeriesTransformer(\n",
    "    dim_val=dim_val,\n",
    "    input_size=input_size, \n",
    "    dec_seq_len=dec_seq_len,\n",
    "    max_seq_len=max_seq_len,\n",
    "    out_seq_len=output_sequence_length, \n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_heads=n_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_src_trg(\n",
    "        self,\n",
    "        sequence: torch.Tensor, \n",
    "        enc_seq_len: int, \n",
    "        target_seq_len: int\n",
    "        ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        Generate the src (encoder input), trg (decoder input) and trg_y (the target)\n",
    "        sequences from a sequence. \n",
    "        Args:\n",
    "            sequence: tensor, a 1D tensor of length n where \n",
    "                    n = encoder input length + target sequence length  \n",
    "            enc_seq_len: int, the desired length of the input to the transformer encoder\n",
    "            target_seq_len: int, the desired length of the target sequence (the \n",
    "                            one against which the model output is compared)\n",
    "        Return: \n",
    "            src: tensor, 1D, used as input to the transformer model\n",
    "            trg: tensor, 1D, used as input to the transformer model\n",
    "            trg_y: tensor, 1D, the target sequence against which the model output\n",
    "                is compared when computing loss. \n",
    "        \n",
    "        \"\"\"\n",
    "        #print(\"Called dataset.TransformerDataset.get_src_trg\")\n",
    "        assert len(sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "        \n",
    "        #print(\"From data.TransformerDataset.get_src_trg: sequence shape: {}\".format(sequence.shape))\n",
    "\n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len] \n",
    "        \n",
    "        # decoder input. As per the paper, it must have the same dimension as the \n",
    "        # target sequence, and it must contain the last value of src, and all\n",
    "        # values of trg_y except the last (i.e. it must be shifted right by 1)\n",
    "        trg = sequence[enc_seq_len-1:len(sequence)-1]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg shape before slice: {}\".format(trg.shape))\n",
    "\n",
    "        trg = trg[:, 0]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg shape after slice: {}\".format(trg.shape))\n",
    "\n",
    "        if len(trg.shape) == 1:\n",
    "\n",
    "            trg = trg.unsqueeze(-1)\n",
    "\n",
    "            #print(\"From data.TransformerDataset.get_src_trg: trg shape after unsqueeze: {}\".format(trg.shape))\n",
    "\n",
    "        \n",
    "        assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "        # The target sequence against which the model output will be compared to compute loss\n",
    "        trg_y = sequence[-target_seq_len:]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg_y shape before slice: {}\".format(trg_y.shape))\n",
    "\n",
    "        # We only want trg_y to consist of the target variable not any potential exogenous variables\n",
    "        trg_y = trg_y[:, 0]\n",
    "\n",
    "        #print(\"From data.TransformerDataset.get_src_trg: trg_y shape after slice: {}\".format(trg_y.shape))\n",
    "\n",
    "        assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "        return src, trg, trg_y.squeeze(-1) # change size from [batch_size, target_seq_len, num_features] to [batch_size, target_seq_len] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_square_subsequent_mask(dim1: int, dim2: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Generates an upper-triangular matrix of -inf, with zeros on diag.\n",
    "    Source:\n",
    "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    Args:\n",
    "        dim1: int, for both src and tgt masking, this must be target sequence\n",
    "              length\n",
    "        dim2: int, for src masking this must be encoder sequence length (i.e. \n",
    "              the length of the input sequence to the model), \n",
    "              and for tgt masking, this must be target sequence length \n",
    "    Return:\n",
    "        A Tensor of shape [dim1, dim2]\n",
    "    \"\"\"\n",
    "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)\n",
    "\n",
    "# Input length\n",
    "enc_seq_len = 100\n",
    "\n",
    "# Output length\n",
    "output_sequence_length = 58\n",
    "\n",
    "# Make src mask for decoder with size:\n",
    "tgt_mask = utils.generate_square_subsequent_mask(\n",
    "    dim1=output_sequence_length,\n",
    "    dim2=output_sequence_length\n",
    "   )\n",
    "\n",
    "src_mask = utils.generate_square_subsequent_mask(\n",
    "    dim1=output_sequence_length,\n",
    "    dim2=enc_seq_len\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8af39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = model(\n",
    "    src=src, \n",
    "    tgt=trg,\n",
    "    src_mask=src_mask,\n",
    "    tgt_mask=tgt_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733694f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955f061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751bce4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c85b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b1124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f6947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a16a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14df31e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e8ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de472784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b37151",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for input_,target in dataloader:\n",
    "    print(input_.shape)\n",
    "    net = torch.nn.LSTM(288,10,1)\n",
    "    h0 = torch.randn(1, 10)\n",
    "    c0 = torch.randn(1, 10)\n",
    "    output, (hn, cn) = net(input_, (h0, c0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893f3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9782fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes # output size\n",
    "        self.num_layers = num_layers # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size # input size\n",
    "        self.hidden_size = hidden_size # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2) # lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) # fully connected \n",
    "        self.fc_2 = nn.Linear(128, num_classes) # fully connected last layer\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # cell state\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        # propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (input, hidden, and internal state)\n",
    "        hn = hn.view(-1, self.hidden_size) # reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) # first dense\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc_2(out) # final output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e267fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def data_to_input(data_frame):\n",
    "    club_np = data_frame.to_numpy()\n",
    "    X = []\n",
    "    y = []\n",
    "    for cl in range(window_size, len(club_np)-1):\n",
    "        x = [[a] for a in club_np[cl-window_size:cl,3:]]\n",
    "        label = club_np[cl+1,0]\n",
    "        X.append(x)\n",
    "        y.append(label)\n",
    "    return X,y\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
