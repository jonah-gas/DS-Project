{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "root_path = os.path.abspath(os.path.join('../..')) # <- adjust such that root_path always points at the root project dir (i.e. if current file is two folders deep, use '../..'). \n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import pickle as pkl\n",
    "\n",
    "import time\n",
    "\n",
    "# feature generation, model training, prediction & evaluation modules\n",
    "from models.trad_ml.feature_generation import FeatureGen\n",
    "from models.trad_ml.training_prediction_evaluation import ModelTrainer\n",
    "from models.trad_ml.training_prediction_evaluation import ModelPrediction\n",
    "from models.trad_ml.training_prediction_evaluation import ModelEvaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "We try to find optimal parameters for two steps of the modelling process:\n",
    "1. Feature generation\n",
    "    - Parameters are defined in the ``params`` dictionary of the ``FeatureGen`` class. \n",
    "    - Includes parameters for the moving average computations, data preparation steps (scaling, encoding, pca, NA value treatment) and more.\n",
    "2. Model training\n",
    "    - Parameters are defined in the ``params`` dictionary of the ``Training`` class.\n",
    "    - Includes model parameters for the specific model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Models\n",
    "\n",
    "We include a range of different model types (which can accept our feature dataframe as input) in the optimization process. \n",
    "\n",
    "#### Linear Model\n",
    "- Logistic Regression\n",
    "\n",
    "#### Tree-based Models\n",
    "- xgboost\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Tasks\n",
    "\n",
    "We train (and tune) models for the following prediction tasks:\n",
    "- Number of goals scored by each team\n",
    "- Win/Loss/Draw (can be derived from number of goals, or goal difference)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "We use two different metrics to evaluate the models:\n",
    "- Logarithmic loss function\n",
    "- Accuracy based on \"argmax\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Process: Description\n",
    "\n",
    "Optimization is separated by model type and prediction task. For each combination of model type & prediction task, we perform a grid search over a range of parameter value combinations ('sweep'). Promising parameter combinations might be fine-tuned further in a second round.\n",
    "\n",
    "Results for each iteration ('run') over the parameter combinations are saved in dictionaries in a subfolder for the current sweep. (Saving each run individually ensures we don't lose intermediate results if the optimization process is interrupted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Process: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Random Forest Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1) feature generation search space\n",
    "\n",
    "# parameters to be varied\n",
    "# example values\n",
    "fg_varied = {\n",
    "    'ma_alpha': [0.1], # the higher alpha, the more weight is put on recent observations vs. older observations\n",
    "    'ma_min_periods': [1], # note: we would strongly prefer low / no min_periods here (to enable predictions also for teams with only few matches in our database)\n",
    "    'ma_restart_each_season': [False], # we would prefer False here, see above\n",
    "\n",
    "    'h2h_feature_cols': ['result_score'], # list of columns of which h2h features should be generated\n",
    "    'h2h_alpha': [0.5], # head2head feature EWMA alpha\n",
    "    'pca_n_components': [0.525] , # only relevant when fitting new pca (note: n_components can be a fraction between zero and one, in which case the number of components is determined via the explained variance threshold)\n",
    "\n",
    "    \n",
    "}\n",
    "# fixed parameters\n",
    "fg_fixed = {\n",
    "    \n",
    "    'h2h_alpha': 0.5, # head2head feature EWMA alpha\n",
    "\n",
    "    'min_non_na_share': 0.9,\n",
    "    \n",
    "    'merge_type': 'wide', # how should feature rows of two teams be combined? -> one of ['wide', 'diff_or_ratio']\n",
    "\n",
    "    'apply_ohe': False, # True -> one-hot encode selected features, False -> drop all categorical features\n",
    "    'ohe_name': None, # load fitted ohe from file <- must not be None when generating prediction features!\n",
    "\n",
    "    'tt_split_cutoff_date': None, # cutoff date is the most recent date to be included in training set. Format: pd.to_datetime('yyyy-mm-dd').date()\n",
    "    'tt_split_test_season': '2022-2023', # season in format yyyy-yyyy\n",
    "\n",
    "    'apply_scaler': True,\n",
    "    'scaler_name': None, # load fitted scaler from file <- must not be None when generating prediction features!\n",
    "    'apply_pca': True,\n",
    "    \n",
    "    'pca_name': None, # load fitted pca from file (provide filename without .pkl suffix) <- must not be None when generating prediction features!\n",
    "\n",
    "    'targets': ['gf', 'ga'], # one of [['gf', 'ga'], ['xg', 'xga']] or list of any single stat column.\n",
    "    'target_as_diff': False # if True (and two target columns were specified), target is provided as difference between the two columns\n",
    "}\n",
    "\n",
    "### 2) model training search space: xgb params\n",
    "\n",
    "# xgboost params to be varied\n",
    "# example values \n",
    "model_varied = {'dif':[False], # False -> separately predicting home and away goals, True ->predicting difference in goals\n",
    "                'n_estimators':[200], # number of trees in the forest\n",
    "                'max_depth':[10], # maximum depth of the tree\n",
    "                'min_samples_split':[10] # minimum number of samples required to split an internal node\n",
    "                \n",
    "}\n",
    "# xgboost params to be fixed\n",
    "model_fixed = {\n",
    "                \n",
    "}\n",
    "\n",
    "### instantiate feature gen and model training/predicting/evaluation objects\n",
    "\n",
    "fg = FeatureGen(params_dict={**fg_fixed, **fg_varied}) # note: loads full data set from db during first feature gen run\n",
    "\n",
    "modeller = ModelTrainer()\n",
    "\n",
    "predictor = ModelPrediction()\n",
    "\n",
    "evaluator = ModelEvaluation() \n",
    "\n",
    "\n",
    "# optimization sweep name\n",
    "sweep_name =  'rf_example' # CHANGE FOR EVERY NEW SWEEP (will create directory)\n",
    "# should models be saved during the sweep (or just the result dicts)?\n",
    "save_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sweep 'rf_example' with 1 runs total.\n",
      "************************************************************\n",
      "Starting training feature generation (run_name: pydatz).\n",
      " - df shape after feature additions: (21708, 161)\n",
      " - number of h2h_ cols: 1\n",
      " - df shape after ma computation: (21708, 163)\n",
      " - df shape after encoding and dropping non-encoded categoricals: (21708, 156)\n",
      " - df shape after merge: (10854, 302)\n",
      " - n rows with any na after merge: 1918\n",
      " - df shape after dropping na rows over na threshold: (10854, 302)\n",
      " - X shape after feature/target split: (10854, 300)\n",
      " - X_train, X_test, y_train, y_test shapes after train/test split: (9027, 300), (1827, 300), (9027, 2), (1827, 2)\n",
      " - X_train, X_test, y_train, y_test shapes after final NA row drop: ((8632, 300), (1819, 300), (8632, 2), (1819, 2))\n",
      " - X_train, X_test shapes post scaling: ((8632, 300), (1819, 300))\n",
      " - X_train, X_test shapes post pca: ((8632, 16), (1819, 16))\n",
      "Feature generation complete (run: pydatz)\n",
      "Run pydatz used {'h2h_alpha': 0.5, 'min_non_na_share': 0.9, 'merge_type': 'wide', 'apply_ohe': False, 'ohe_name': None, 'tt_split_cutoff_date': None, 'tt_split_test_season': '2022-2023', 'apply_scaler': True, 'scaler_name': None, 'apply_pca': True, 'pca_name': None, 'targets': ['gf', 'ga'], 'target_as_diff': False, 'ma_alpha': 0.1, 'ma_min_periods': 1, 'ma_restart_each_season': False, 'h2h_feature_cols': 'result_score', 'pca_n_components': 0.525} and {'dif': False, 'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 10}\n",
      "Run pydatz (1/1) finished in 30.02819037437439 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model save path (same for all sweeps/runs)\n",
    "model_save_path = os.path.join(root_path, 'models', 'trad_ml', 'saved_models')\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "# define path for sweep results (new folder for each sweep)\n",
    "results_save_path = os.path.join(root_path, 'models', 'trad_ml', 'sweep_results', sweep_name)\n",
    "if not os.path.exists(results_save_path):\n",
    "    os.makedirs(results_save_path)\n",
    "\n",
    "### sweep procedure\n",
    "\n",
    "# get number of runs to be executed for this sweep\n",
    "n_runs = len(list(itertools.product(*fg_varied.values()))) * len(list(itertools.product(*model_varied.values()))) # (note: still works if one of the varied param dicts is empty, since itertools returns an empty tuple (which counts as a list element))\n",
    "counter = 0 \n",
    "print(f\"Starting sweep '{sweep_name}' with {n_runs} runs total.\")\n",
    "  \n",
    "# iterate over all combinations of fg_space_varied using itertools.product\n",
    "for fg_params in itertools.product(*fg_varied.values()): # yields fg_varied value combinations\n",
    "    for model_params in itertools.product(*model_varied.values()): # yields model_varied value combinations\n",
    "        run_start_time = time.time()\n",
    "        counter += 1\n",
    "        # create new run name (random 6-character string)\n",
    "        run_name = ''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=6, replace=True))\n",
    "\n",
    "        # assemble feature & model params dicts for current run\n",
    "        fg_config = fg_fixed.copy() # start with fixed params\n",
    "        fg_config.update(dict(zip(fg_varied.keys(), fg_params))) # add varied params\n",
    "        model_config = model_fixed.copy() \n",
    "        model_config.update(dict(zip(model_varied.keys(), model_params))) \n",
    "\n",
    "        ### generate features\n",
    "        # update feature gen object with new params\n",
    "        fg.set_params(new_params_dict=fg_config, run_name=run_name)\n",
    "        # generate features (& labels)\n",
    "        X_train, X_test, y_train, y_test = fg.generate_features(incl_non_feature_cols=False, print_logs=True) # logs false?\n",
    "\n",
    "        ### create and train model\n",
    "        model = modeller.train_rf(X_train, y_train, **model_config) \n",
    "\n",
    "        ### evaluate model (i.e. predict test set and compute metric(s))\n",
    "        preds = predictor.predict_prob(X_test, model, dif= model_config['dif'])\n",
    "        accuracy = evaluator.accuracy(y_test, preds)\n",
    "        lnloss = evaluator.lnloss(y_test,preds)\n",
    "        \n",
    "\n",
    "        ### save results\n",
    "        # create results dict, containing all relevant info for current run (except the model itself)\n",
    "        results_dict = {\n",
    "            'run_name': run_name, # <- to be able to identify corresponding model and data prep objects later (if saved during the process)\n",
    "            'fg_config': fg_config,\n",
    "            'model_config': model_config,\n",
    "            'train_test_split': {'train_size': len(X_train), 'test_size': len(X_test)}, # should be considered since different fg configs can yield differently sized train and test sets\n",
    "            'task': f\"targets: {fg_config['targets']}, predicting diff: {model_config['dif']}\",\n",
    "            'metrics': {'accuracy': accuracy,\n",
    "                       'lnloss': lnloss} # dict of multiple metrics of performance on test set (accuracy, logloss)\n",
    "        }\n",
    "        # save\n",
    "        with open(os.path.join(results_save_path, f\"{sweep_name}_{counter}_{run_name}.pkl\"), 'wb') as f:\n",
    "            pkl.dump(results_dict, f)\n",
    "\n",
    "        ### save model (separately from results dict)\n",
    "        if save_models:\n",
    "            with open(os.path.join(model_save_path, f\"{model.__class__.__name__}_{run_name}.pkl\"), 'wb') as f:\n",
    "                pkl.dump(model, f)\n",
    "        \n",
    "        # print paramters of iteration\n",
    "        print(f\"Run {run_name} used {fg_config} and {model_config}\")\n",
    "\n",
    "        # print progress\n",
    "        print(f\"Run {run_name} ({counter}/{n_runs}) finished in {time.time() - run_start_time} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name  ma_alpha  ma_min_periods  ma_restart_each_season h2h_feature_cols  \\\n",
      "1  pydatz       0.1               1                   False     result_score   \n",
      "0  dvjyno       0.1               1                   False     result_score   \n",
      "\n",
      "   accuracy    lnloss  n_estimators  max_depth    dif  h2h_alpha  \\\n",
      "1  0.523914  0.992225           200         10  False        0.5   \n",
      "0  0.523364  0.992581           200         10  False        0.5   \n",
      "\n",
      "   pca_components  min_samples_split  \n",
      "1           0.525                 10  \n",
      "0           0.525                 10  \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "# Get a list of all .pkl files in the directory\n",
    "pkl_files = glob.glob(os.path.join(results_save_path, \"*.pkl\"))\n",
    "\n",
    "# Initialize an empty list to store the data from each file\n",
    "data = []\n",
    "\n",
    "# Load and extract the data from each pickle file\n",
    "for file in pkl_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        obj = pkl.load(f)\n",
    "        data.append({\n",
    "            'name': obj['run_name'],\n",
    "            'ma_alpha': obj['fg_config']['ma_alpha'],\n",
    "            'ma_min_periods': obj['fg_config']['ma_min_periods'],\n",
    "            'ma_restart_each_season': obj['fg_config']['ma_restart_each_season'],\n",
    "            'h2h_feature_cols':obj['fg_config']['h2h_feature_cols'],\n",
    "            'accuracy': obj['metrics']['accuracy'],\n",
    "            'lnloss': obj['metrics']['lnloss'],\n",
    "            'n_estimators': obj['model_config']['n_estimators'],\n",
    "            'max_depth' : obj['model_config']['max_depth'],\n",
    "            'dif': obj['model_config']['dif'],\n",
    "            'h2h_alpha': obj['fg_config'][\"h2h_alpha\"],\n",
    "            'pca_components': obj['fg_config']['pca_n_components'],\n",
    "            'h2h_feature_cols': obj['fg_config']['h2h_feature_cols'],\n",
    "            'min_samples_split': obj['model_config']['min_samples_split']\n",
    "            \n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sort_values(['lnloss'])\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                            pydatz\n",
      "ma_alpha                           0.1\n",
      "ma_min_periods                       1\n",
      "ma_restart_each_season           False\n",
      "h2h_feature_cols          result_score\n",
      "accuracy                      0.523914\n",
      "lnloss                        0.992225\n",
      "n_estimators                       200\n",
      "max_depth                           10\n",
      "dif                              False\n",
      "h2h_alpha                          0.5\n",
      "pca_components                   0.525\n",
      "min_samples_split                   10\n",
      "Name: 1, dtype: object\n",
      "name                            pydatz\n",
      "ma_alpha                           0.1\n",
      "ma_min_periods                       1\n",
      "ma_restart_each_season           False\n",
      "h2h_feature_cols          result_score\n",
      "accuracy                      0.523914\n",
      "lnloss                        0.992225\n",
      "n_estimators                       200\n",
      "max_depth                           10\n",
      "dif                              False\n",
      "h2h_alpha                          0.5\n",
      "pca_components                   0.525\n",
      "min_samples_split                   10\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Find the row with the highest accuracy respecitvly lowest lnloss\n",
    "max_accuracy_row = df.loc[df['accuracy'].idxmax()]\n",
    "min_lnloss_row = df.loc[df['lnloss'].idxmin()]\n",
    "print(max_accuracy_row)\n",
    "print(min_lnloss_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by parameter of choice and print mean of lnloss or accuracy\n",
    "# example: grouped_df_pca_components = df.groupby('pca_components')['lnloss'].mean()\n",
    "#          print(grouped_df_pca_components)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
