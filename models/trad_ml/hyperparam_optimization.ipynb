{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "root_path = os.path.abspath(os.path.join('../..')) # <- adjust such that root_path always points at the root project dir (i.e. if current file is two folders deep, use '../..'). \n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import pickle as pkl\n",
    "\n",
    "import time\n",
    "\n",
    "# feature generation, model training, prediction & evaluation modules\n",
    "from models.trad_ml.feature_generation import FeatureGen\n",
    "# from models.trad_ml.??? import ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "We try to find optimal parameters for two steps of the modelling process:\n",
    "1. Feature generation\n",
    "    - Parameters are defined in the ``params`` dictionary of the ``FeatureGen`` class. \n",
    "    - Includes parameters for the moving average computations, data preparation steps (scaling, encoding, pca, NA value treatment) and more.\n",
    "2. Model training\n",
    "    - Parameters are defined in the ``params`` dictionary of the ``Training`` class.\n",
    "    - Includes model parameters for the specific model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Models\n",
    "\n",
    "We include a range of different model types (which can accept our feature dataframe as input) in the optimization process. \n",
    "\n",
    "#### Linear Models\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- Ridge Regression\n",
    "#### Tree-based Models\n",
    "- xgboost\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Tasks\n",
    "\n",
    "We train (and tune) models for the following prediction tasks:\n",
    "- Number of goals scored by each team\n",
    "- Win/Loss/Draw (can be derived from number of goals, or predicted directly)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Process: Description\n",
    "\n",
    "Optimization is separated by model type and prediction task. For each combination of model type & prediction task, we perform a grid search over a range of parameter value combinations ('sweep'). Promising parameter combinations might be fine-tuned further in a second round.\n",
    "\n",
    "Results for each iteration ('run') over the parameter combinations are saved in dictionaries in a subfolder for the current sweep. (Saving each run individually ensures we don't lose intermediate results if the optimization process is interrupted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Process: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1) feature generation search space\n",
    "\n",
    "# parameters to be varied\n",
    "fg_varied = {\n",
    "    'ma_alpha': [0.25, 0.5, 0.75], # the higher alpha, the more weight is put on recent observations vs. older observations\n",
    "    'ma_min_periods': [0, 5], # note: we would strongly prefer low / no min_periods here (to enable predictions also for teams with only few matches in our database)\n",
    "    'ma_restart_each_season': [True, False], # we would prefer False here, see above\n",
    "\n",
    "    'h2h_feature_cols': [['result_score'], ['result_score', 'xg', 'xga']], # list of columns of which h2h features should be generated\n",
    "    'h2h_alpha': [0.35, 0.75], # head2head feature EWMA alpha\n",
    "\n",
    "    'pca_n_components': [0.95, 0.99], # only relevant when fitting new pca (note: n_components can be a fraction between zero and one, in which case the number of components is determined via the explained variance threshold)\n",
    "}\n",
    "\n",
    "# fixed parameters\n",
    "fg_fixed = {\n",
    "    'min_non_na_share': 0.9,\n",
    "\n",
    "    'merge_type': 'wide', # how should feature rows of two teams be combined? -> one of ['wide', 'diff_or_ratio']\n",
    "\n",
    "    'apply_ohe': False, # True -> one-hot encode selected features, False -> drop all categorical features\n",
    "    'ohe_name': None, # load fitted ohe from file <- must not be None when generating prediction features!\n",
    "\n",
    "    'tt_split_cutoff_date': None, # cutoff date is the most recent date to be included in training set\n",
    "    'tt_split_test_season': '2022-2023',\n",
    "\n",
    "    'apply_scaler': True,\n",
    "    'scaler_name': None, # load fitted scaler from file <- must not be None when generating prediction features!\n",
    "    'apply_pca': True,\n",
    "    'pca_name': None, # load fitted pca from file (provide filename without .pkl suffix) <- must not be None when generating prediction features!\n",
    "\n",
    "    'targets': ['gf', 'ga'], # one of [['gf', 'ga'], ['xg', 'xga']] or list of any single stat column.\n",
    "    'target_as_diff': False # if True (and two target columns were specified), target is provided as difference between the two columns\n",
    "}\n",
    "\n",
    "### 2) model training search space: xgb params\n",
    "\n",
    "# xgboost params to be varied\n",
    "model_varied = {\n",
    "    ### varied params\n",
    "}\n",
    "model_fixed = {\n",
    "    ### fixed params\n",
    "}\n",
    "\n",
    "### instantiate feature gen and model training objects\n",
    "\n",
    "fg = FeatureGen(params_dict={**fg_fixed, **fg_varied}) # note: loads full data set from db during first feature gen run\n",
    "\n",
    "# modeller = ... # <- instantiate modeller object here \n",
    "\n",
    "# predictor = ... # <- instantiate predictor object here\n",
    "\n",
    "# evaluator = ... # <- instantiate evaluator object here (if implemented as separate class)\n",
    "\n",
    "\n",
    "# optimization sweep name\n",
    "sweep_name = 'xgb_coarse_grid_search' ### <- CHANGE FOR EVERY NEW SWEEP (will create directory)\n",
    "# should models be saved during the sweep (or just the result dicts)?\n",
    "save_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sweep 'xgb_coarse_grid_search_1' with 96 runs total.\n",
      "************************************************************\n",
      "Starting training feature generation (run_name: qkrove).\n",
      " - training data set loaded from db, shape: (21708, 159)\n",
      " - df shape after feature additions: (21708, 161)\n",
      " - number of h2h_ cols: 1\n",
      " - df shape after ma computation: (21708, 163)\n",
      " - df shape after encoding and dropping non-encoded categoricals: (21708, 156)\n",
      " - df shape after merge: (10854, 302)\n",
      " - n rows with any na after merge: 2948\n",
      " - df shape after dropping na rows over na threshold: (10854, 302)\n",
      " - X shape after feature/target split: (10854, 300)\n",
      " - X_train, X_test, y_train, y_test shapes after train/test split: (9027, 300), (1827, 300), (9027, 2), (1827, 2)\n",
      " - X_train, X_test, y_train, y_test shapes after final NA row drop: ((7641, 300), (1765, 300), (7641, 2), (1765, 2))\n",
      " - X_train, X_test shapes post scaling: ((7641, 300), (1765, 300))\n",
      " - X_train, X_test shapes post pca: ((7641, 128), (1765, 128))\n",
      "Feature generation complete (run: qkrove)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jan Jacobsen\\DS_Tue_local\\DS-Project\\models\\trad_ml\\hyperparam_optimization.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jan%20Jacobsen/DS_Tue_local/DS-Project/models/trad_ml/hyperparam_optimization.ipynb#X13sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     pkl\u001b[39m.\u001b[39mdump(results_dict, f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jan%20Jacobsen/DS_Tue_local/DS-Project/models/trad_ml/hyperparam_optimization.ipynb#X13sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m### save model (separately from results dict)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Jan%20Jacobsen/DS_Tue_local/DS-Project/models/trad_ml/hyperparam_optimization.ipynb#X13sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_save_path, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mrun_name\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jan%20Jacobsen/DS_Tue_local/DS-Project/models/trad_ml/hyperparam_optimization.ipynb#X13sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     pkl\u001b[39m.\u001b[39mdump(results_dict, f)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Jan%20Jacobsen/DS_Tue_local/DS-Project/models/trad_ml/hyperparam_optimization.ipynb#X13sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# print progress\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# model save path (same for all sweeps/runs)\n",
    "model_save_path = os.path.join(root_path, 'models', 'trad_ml', 'saved_models')\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "# define path for sweep results (new folder for each sweep)\n",
    "results_save_path = os.path.join(root_path, 'models', 'trad_ml', 'sweep_results', sweep_name)\n",
    "if not os.path.exists(results_save_path):\n",
    "    os.makedirs(results_save_path)\n",
    "\n",
    "### sweep procedure\n",
    "\n",
    "# get number of runs to be executed for this sweep\n",
    "n_runs = len(list(itertools.product(*fg_varied.values()))) * len(list(itertools.product(*model_varied.values()))) # (note: still works if one of the varied param dicts is empty, since itertools returns an empty tuple (which counts as a list element))\n",
    "counter = 0 \n",
    "print(f\"Starting sweep '{sweep_name}' with {n_runs} runs total.\")\n",
    "  \n",
    "# iterate over all combinations of fg_space_varied using itertools.product\n",
    "for fg_params in itertools.product(*fg_varied.values()): # yields fg_varied value combinations\n",
    "    for model_params in itertools.product(*model_varied.values()): # yields model_varied value combinations\n",
    "        run_start_time = time.time()\n",
    "        counter += 1\n",
    "        # create new run name (random 6-character string)\n",
    "        run_name = ''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=6, replace=True))\n",
    "\n",
    "        # assemble feature & model params dicts for current run\n",
    "        fg_config = fg_fixed.copy() # start with fixed params\n",
    "        fg_config.update(dict(zip(fg_varied.keys(), fg_params))) # add varied params\n",
    "        model_config = model_fixed.copy() \n",
    "        model_config.update(dict(zip(model_varied.keys(), model_params))) \n",
    "\n",
    "        ### generate features\n",
    "        # update feature gen object with new params\n",
    "        fg.set_params(new_params_dict=fg_config, run_name=run_name)\n",
    "        # generate features (& labels)\n",
    "        X_train, X_test, y_train, y_test = fg.generate_features(incl_non_feature_cols=False, print_logs=True) # logs false?\n",
    "\n",
    "        ### create and train model\n",
    "        # ...\n",
    "        # model = modeller.train_xgb(X_train, X_test, params_dict=model_config) # <- sth like this\n",
    "\n",
    "        ### evaluate model (i.e. predict test set and compute metric(s))\n",
    "        # ...\n",
    "        # preds = predictor.predict_proba(...)\n",
    "        # ... evaluate (maybe third class and rename to train_pred_eval.py? idk...)\n",
    "\n",
    "        ### save results\n",
    "        # create results dict, containing all relevant info for current run (except the model itself)\n",
    "        results_dict = {\n",
    "            'run_name': run_name, # <- to be able to identify corresponding model and data prep objects later (if saved during the process)\n",
    "            'fg_config': fg_config,\n",
    "            'model_config': model_config,\n",
    "            'train_test_split': {'train_size': len(X_train), 'test_size': len(X_test)}, # should be considered since different fg configs can yield differently sized train and test sets\n",
    "            'task': None, # <- maybe description / type of task (classification/regression, etc.)\n",
    "            'metrics': None # <- maybe dict of multiple metrics of performance on test set (accuracy, logloss, etc.)\n",
    "        }\n",
    "        # save\n",
    "        with open(os.path.join(results_save_path, f\"{sweep_name}_{counter}_{run_name}.pkl\"), 'wb') as f:\n",
    "            pkl.dump(results_dict, f)\n",
    "\n",
    "        ### save model (separately from results dict)\n",
    "        with open(os.path.join(model_save_path, f\"{model.__class__.__name__}_{run_name}.pkl\"), 'wb') as f:\n",
    "            pkl.dump(results_dict, f)\n",
    "        \n",
    "        # print progress\n",
    "        print(f\"Run {run_name} ({counter}/{n_runs}) finished in {time.time() - run_start_time} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
