{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d700baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "root_path = os.path.abspath(os.path.join('../..')) # <- adjust such that root_path always points at the root project dir (i.e. if current file is two folders deep, use '../..'). \n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import itertools\n",
    "import pickle as pkl\n",
    "import time\n",
    "\n",
    "# feature generation, model training, prediction & evaluation modules\n",
    "import database_server.db_utilities as dbu\n",
    "from models.trad_ml.feature_generation import FeatureGen\n",
    "from models.trad_ml.training_prediction_evaluation import ModelTrainer\n",
    "from models.trad_ml.training_prediction_evaluation import ModelPrediction\n",
    "from models.trad_ml.training_prediction_evaluation import ModelEvaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdc344",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "We try to find optimal parameters for two steps of the modelling process:\n",
    "1. Feature generation\n",
    "    - Parameters are defined in the ``params`` dictionary of the ``FeatureGen`` class. \n",
    "    - Includes parameters for the moving average computations, data preparation steps (scaling, encoding, pca, NA value treatment) and more.\n",
    "2. Model training\n",
    "    - Parameters are defined in the ``params`` dictionary of the ``Training`` class.\n",
    "    - Includes model parameters for the specific model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c78f97",
   "metadata": {},
   "source": [
    "### Types of Models\n",
    "\n",
    "We include a range of different model types (which can accept our feature dataframe as input) in the optimization process. \n",
    "\n",
    "#### Linear Model\n",
    "- Logistic Regression\n",
    "\n",
    "#### Tree-based Models\n",
    "- xgboost\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4854c2",
   "metadata": {},
   "source": [
    "### Prediction Tasks\n",
    "\n",
    "We train (and tune) models for the following prediction tasks:\n",
    "- Number of goals scored by each team\n",
    "- Win/Loss/Draw (can be derived from number of goals, or goal difference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20ddf7",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "We use two different metrics to evaluate the models:\n",
    "- Logarithmic loss function\n",
    "- Accuracy based on \"argmax\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d929b",
   "metadata": {},
   "source": [
    "### Optimization Process: Description\n",
    "\n",
    "Optimization is separated by model type and prediction task. For each combination of model type & prediction task, we perform a grid search over a range of parameter value combinations ('sweep'). Promising parameter combinations might be fine-tuned further in a second round.\n",
    "\n",
    "Results for each iteration ('run') over the parameter combinations are saved in dictionaries in a subfolder for the current sweep. (Saving each run individually ensures we don't lose intermediate results if the optimization process is interrupted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81a5a6",
   "metadata": {},
   "source": [
    "### Optimization Process: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e033e",
   "metadata": {},
   "source": [
    "#### 1. Logreg Model Predection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2354bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1) feature generation search space\n",
    "\n",
    "# parameters to be varied\n",
    "# example values\n",
    "fg_varied = {\n",
    "    'ma_alpha': [0.05], # the higher alpha, the more weight is put on recent observations vs. older observations\n",
    "    'ma_min_periods': [10], # note: we would strongly prefer low / no min_periods here (to enable predictions also for teams with only few matches in our database)\n",
    "    'ma_restart_each_season': [False], # we would prefer False here, see above\n",
    "\n",
    "    'h2h_feature_cols': ['result_score'], # list of columns of which h2h features should be generated\n",
    "    'h2h_alpha': [0.1], # head2head feature EWMA alpha\n",
    "    'pca_n_components':[0.925 ]  # only relevant when fitting new pca (note: n_components can be a fraction between zero and one, in which case the number of components is determined via the explained variance threshold)\n",
    "\n",
    "}\n",
    "# fixed parameters\n",
    "# example values\n",
    "fg_fixed = {\n",
    "    \n",
    "    'min_non_na_share': 0.9,\n",
    "    \n",
    "    'merge_type': 'wide', # how should feature rows of two teams be combined? -> one of ['wide', 'diff_or_ratio']\n",
    "\n",
    "    'apply_ohe': False, # True -> one-hot encode selected features, False -> drop all categorical features\n",
    "    'ohe_name': None, # load fitted ohe from file <- must not be None when generating prediction features!\n",
    "\n",
    "    'tt_split_cutoff_date': None, # cutoff date is the most recent date to be included in training set. Format: pd.to_datetime('yyyy-mm-dd').date()\n",
    "    'tt_split_test_season': '2022-2023', # season in format yyyy-yyyy\n",
    "\n",
    "    'apply_scaler': True,\n",
    "    'scaler_name': None, # load fitted scaler from file <- must not be None when generating prediction features!\n",
    "    'apply_pca': True,\n",
    "    \n",
    "    'pca_name': None, # load fitted pca from file (provide filename without .pkl suffix) <- must not be None when generating prediction features!\n",
    "\n",
    "    'targets': ['gf', 'ga'], # one of [['gf', 'ga'], ['xg', 'xga']] or list of any single stat column.\n",
    "    'target_as_diff': False # if True (and two target columns were specified), target is provided as difference between the two columns\n",
    "}\n",
    "\n",
    "### 2) model training search space: logreg params\n",
    "\n",
    "# logreg params to be varied\n",
    "model_varied = {'max_iter': [20],\n",
    "                'C':[0.001],\n",
    "                'dif': [False],\n",
    "                'class_weight' : [None]\n",
    "\n",
    "}\n",
    "# logreg params to be fixed\n",
    "model_fixed = {}\n",
    "\n",
    "### instantiate feature gen and model training/predicting/evaluation objects\n",
    "\n",
    "fg = FeatureGen(params_dict={**fg_fixed, **fg_varied}) # note: loads full data set from db during first feature gen run\n",
    "\n",
    "modeller = ModelTrainer()\n",
    "\n",
    "predictor = ModelPrediction()\n",
    "\n",
    "evaluator = ModelEvaluation() \n",
    "\n",
    "\n",
    "# optimization sweep name\n",
    "sweep_name = 'logreg_example' ### <- CHANGE FOR EVERY NEW SWEEP (will create directory)\n",
    "# should models be saved during the sweep (or just the result dicts)?\n",
    "save_models = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73192eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sweep 'logreg_example' with 1 runs total.\n",
      "************************************************************\n",
      "Starting training feature generation (run_name: mbywna).\n",
      " - df shape after feature additions: (21708, 161)\n",
      " - number of h2h_ cols: 1\n",
      " - df shape after ma computation: (21708, 163)\n",
      " - df shape after encoding and dropping non-encoded categoricals: (21708, 156)\n",
      " - df shape after merge: (10854, 302)\n",
      " - n rows with any na after merge: 2028\n",
      " - df shape after dropping na rows over na threshold: (10854, 302)\n",
      " - X shape after feature/target split: (10854, 300)\n",
      " - X_train, X_test, y_train, y_test shapes after train/test split: (9027, 300), (1827, 300), (9027, 2), (1827, 2)\n",
      " - X_train, X_test, y_train, y_test shapes after final NA row drop: ((7701, 300), (1759, 300), (7701, 2), (1759, 2))\n",
      " - X_train, X_test shapes post scaling: ((7701, 300), (1759, 300))\n",
      " - X_train, X_test shapes post pca: ((7701, 84), (1759, 84))\n",
      "Feature generation complete (run: mbywna)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan_nii2lon\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run mbywna used {'min_non_na_share': 0.9, 'merge_type': 'wide', 'apply_ohe': False, 'ohe_name': None, 'tt_split_cutoff_date': None, 'tt_split_test_season': '2022-2023', 'apply_scaler': True, 'scaler_name': None, 'apply_pca': True, 'pca_name': None, 'targets': ['gf', 'ga'], 'target_as_diff': False, 'ma_alpha': 0.05, 'ma_min_periods': 10, 'ma_restart_each_season': False, 'h2h_feature_cols': 'result_score', 'h2h_alpha': 0.1, 'pca_n_components': 0.925} and {'max_iter': 20, 'C': 0.001, 'dif': False, 'class_weight': None}\n",
      "Run mbywna (1/1) finished in 11.695378541946411 seconds.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan_nii2lon\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# model save path (same for all sweeps/runs)\n",
    "model_save_path = os.path.join(root_path, 'models', 'trad_ml', 'saved_models')\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "# define path for sweep results (new folder for each sweep)\n",
    "results_save_path = os.path.join(root_path, 'models', 'trad_ml', 'sweep_results', sweep_name)\n",
    "if not os.path.exists(results_save_path):\n",
    "    os.makedirs(results_save_path)\n",
    "\n",
    "### sweep procedure\n",
    "\n",
    "# get number of runs to be executed for this sweep\n",
    "n_runs = len(list(itertools.product(*fg_varied.values()))) * len(list(itertools.product(*model_varied.values()))) # (note: still works if one of the varied param dicts is empty, since itertools returns an empty tuple (which counts as a list element))\n",
    "counter = 0 \n",
    "print(f\"Starting sweep '{sweep_name}' with {n_runs} runs total.\")\n",
    "  \n",
    "# iterate over all combinations of fg_space_varied using itertools.product\n",
    "for fg_params in itertools.product(*fg_varied.values()): # yields fg_varied value combinations\n",
    "    for model_params in itertools.product(*model_varied.values()): # yields model_varied value combinations\n",
    "        run_start_time = time.time()\n",
    "        counter += 1\n",
    "        # create new run name (random 6-character string)\n",
    "        run_name = ''.join(np.random.choice(list('abcdefghijklmnopqrstuvwxyz'), size=6, replace=True))\n",
    "\n",
    "        # assemble feature & model params dicts for current run\n",
    "        fg_config = fg_fixed.copy() # start with fixed params\n",
    "        fg_config.update(dict(zip(fg_varied.keys(), fg_params))) # add varied params\n",
    "        model_config = model_fixed.copy() \n",
    "        model_config.update(dict(zip(model_varied.keys(), model_params))) \n",
    "\n",
    "        ### generate features\n",
    "        # update feature gen object with new params\n",
    "        fg.set_params(new_params_dict=fg_config, run_name=run_name)\n",
    "        # generate features (& labels)\n",
    "        X_train, X_test, y_train, y_test = fg.generate_features(incl_non_feature_cols=False, print_logs=True) # logs false?\n",
    "\n",
    "        ### create and train model\n",
    "        model = modeller.train_logreg(X_train, y_train, **model_config) \n",
    "\n",
    "        ### evaluate model \n",
    "        preds = predictor.predict_prob(X_test, model, dif = model_config['dif'] )\n",
    "        accuracy = evaluator.accuracy(y_test, preds)\n",
    "        lnloss = evaluator.lnloss(y_test,preds)\n",
    "        \n",
    "        ### save results\n",
    "        # create results dict, containing all relevant info for current run (except the model itself)\n",
    "        results_dict = {\n",
    "            'run_name': run_name, # <- to be able to identify corresponding model and data prep objects later (if saved during the process)\n",
    "            'fg_config': fg_config,\n",
    "            'model_config': model_config,\n",
    "            'train_test_split': {'train_size': len(X_train), 'test_size': len(X_test)}, # should be considered since different fg configs can yield differently sized train and test sets\n",
    "            'task': f\"targets: {fg_config['targets']}, predicting diff: {model_config['dif']}\",\n",
    "            'metrics': {'accuracy': accuracy,\n",
    "                      'lnloss': lnloss} # dict of multiple metrics of performance on test set (accuracy, logloss)\n",
    "        }\n",
    "        # save\n",
    "        with open(os.path.join(results_save_path, f\"{sweep_name}_{counter}_{run_name}.pkl\"), 'wb') as f:\n",
    "            pkl.dump(results_dict, f)\n",
    "\n",
    "        ### save model (separately from results dict)\n",
    "        if save_models:\n",
    "            with open(os.path.join(model_save_path, f\"{model.__class__.__name__}_{run_name}.pkl\"), 'wb') as f:\n",
    "                pkl.dump(model, f)\n",
    "        \n",
    "        # print paramters of iteration\n",
    "        print(f\"Run {run_name} used {fg_config} and {model_config}\")\n",
    "\n",
    "        # print progress\n",
    "        print(f\"Run {run_name} ({counter}/{n_runs}) finished in {time.time() - run_start_time} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5014909a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name  ma_alpha  h2h_alpha  ma_min_periods  ma_restart_each_season  \\\n",
      "0  mbywna      0.05        0.1              10                   False   \n",
      "\n",
      "  h2h_feature_cols  max_iter      C  accuracy    lnloss    dif  \\\n",
      "0     result_score        20  0.001  0.528709  0.989105  False   \n",
      "\n",
      "   pca_n_components class_weight  \n",
      "0             0.925         None  \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "# Get a list of all .pkl files in the directory\n",
    "pkl_files = glob.glob(os.path.join(results_save_path, \"*.pkl\"))\n",
    "\n",
    "# Initialize an empty list to store the data from each file\n",
    "data = []\n",
    "\n",
    "# Load and extract the data from each pickle file\n",
    "for file in pkl_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        obj = pkl.load(f)\n",
    "        data.append({\n",
    "            'name': obj['run_name'],\n",
    "            'ma_alpha': obj['fg_config']['ma_alpha'],\n",
    "            'h2h_alpha': obj['fg_config']['h2h_alpha'],\n",
    "            'ma_min_periods': obj['fg_config']['ma_min_periods'],\n",
    "            'ma_restart_each_season': obj['fg_config']['ma_restart_each_season'],\n",
    "            'h2h_feature_cols':obj['fg_config']['h2h_feature_cols'],\n",
    "            'max_iter':obj['model_config']['max_iter'],\n",
    "            'C':obj['model_config']['C'],\n",
    "            'accuracy': obj['metrics']['accuracy'],\n",
    "            'lnloss': obj['metrics']['lnloss'],\n",
    "            'dif': obj['model_config']['dif'],\n",
    "            'pca_n_components': obj['fg_config']['pca_n_components'],\n",
    "            'class_weight': obj['model_config']['class_weight']\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54ad1e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name                            mbywna\n",
      "ma_alpha                          0.05\n",
      "h2h_alpha                          0.1\n",
      "ma_min_periods                      10\n",
      "ma_restart_each_season           False\n",
      "h2h_feature_cols          result_score\n",
      "max_iter                            20\n",
      "C                                0.001\n",
      "accuracy                      0.528709\n",
      "lnloss                        0.989105\n",
      "dif                              False\n",
      "pca_n_components                 0.925\n",
      "class_weight                      None\n",
      "Name: 0, dtype: object\n",
      "name                            mbywna\n",
      "ma_alpha                          0.05\n",
      "h2h_alpha                          0.1\n",
      "ma_min_periods                      10\n",
      "ma_restart_each_season           False\n",
      "h2h_feature_cols          result_score\n",
      "max_iter                            20\n",
      "C                                0.001\n",
      "accuracy                      0.528709\n",
      "lnloss                        0.989105\n",
      "dif                              False\n",
      "pca_n_components                 0.925\n",
      "class_weight                      None\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Find the row with the highest accuracy respectivly lowest accuracy\n",
    "max_accuracy_row = df.loc[df['accuracy'].idxmax()]\n",
    "min_lnloss_row = df.loc[df['lnloss'].idxmin()]\n",
    "print(max_accuracy_row)\n",
    "print(min_lnloss_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "210d677f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# group by parameter of choice and print mean of lnloss or accuracy\n",
    "# example: grouped_df_pca_components = df.groupby('pca_components')['lnloss'].mean()\n",
    "#          print(grouped_df_pca_components)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
